{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b96bd09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ngzhi\\Documents\\Year 4 Sem 1\\DSA4213\\DSA4213-SEC-Fillings-Chatbot\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import uuid\n",
    "import time\n",
    "import requests\n",
    "import gc\n",
    "import json  # For saving results\n",
    "from typing import List, Dict, Tuple\n",
    "from collections import defaultdict\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "\n",
    "# External libraries\n",
    "from dotenv import load_dotenv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient, models\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
    "from openai import OpenAI\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Imports for NER\n",
    "import spacy\n",
    "from transformers import pipeline\n",
    "\n",
    "# Suppress warnings (especially from transformers)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb1f3dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# PART 1: CONFIGURATION & SETUP\n",
    "# ===========================================================================\n",
    "# (From base_RAG.ipynb)\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Configuration class for API keys and model settings\"\"\"\n",
    "    QDRANT_URL = os.getenv(\"QDRANT_URL\")\n",
    "    QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")\n",
    "    OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "    SEC_HEADERS = {'User-Agent': 'EnhancedRAG-Project research@example.com'}\n",
    "    CIK_MAP_URL = 'https://www.sec.gov/files/company_tickers.json'\n",
    "\n",
    "    # Model Configuration\n",
    "    EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"  # 384 dimensions\n",
    "    LLM_MODEL = \"gpt-4o\"\n",
    "    NER_MODEL = \"dslim/bert-base-NER\"\n",
    "    SPACY_MODEL = \"en_core_web_lg\"\n",
    "\n",
    "    COLLECTION_NAME = \"sec_filings_10q_enhanced\" # New collection name\n",
    "    VECTOR_SIZE = 384\n",
    "    CHUNK_SIZE = 800\n",
    "    CHUNK_OVERLAP = 200\n",
    "    TOP_K = 5\n",
    "    TICKERS = ['NVDA', 'AAPL', 'MSFT', 'AMZN', 'META', 'GOOGL', 'TSLA', 'ORCL', 'JPM', 'AMD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "779fd8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# PART 2: DOCUMENT LOADING\n",
    "# ===========================================================================\n",
    "# (From base_RAG.ipynb)\n",
    "\n",
    "class SECDocumentLoader:\n",
    "    \"\"\"Handles fetching and parsing of SEC 10-Q filings\"\"\"\n",
    "    @staticmethod\n",
    "    def get_recent_10q_metadata(ticker: str, num_filings: int = 4) -> List[Dict[str, str]]:\n",
    "        print(f\"  → Fetching CIK for ticker: {ticker}...\")\n",
    "        response = requests.get(Config.CIK_MAP_URL, headers=Config.SEC_HEADERS)\n",
    "        response.raise_for_status()\n",
    "        company_data = response.json()\n",
    "\n",
    "        cik = None\n",
    "        company_name = None\n",
    "        for company in company_data.values():\n",
    "            if company['ticker'] == ticker.upper():\n",
    "                cik = str(company['cik_str']).zfill(10)\n",
    "                company_name = company['title']\n",
    "                break\n",
    "        if not cik:\n",
    "            raise ValueError(f\"Ticker '{ticker}' not found in SEC CIK mapping.\")\n",
    "        print(f\"  → Found CIK: {cik} ({company_name})\")\n",
    "\n",
    "        submissions_url = f\"https://data.sec.gov/submissions/CIK{cik}.json\"\n",
    "        time.sleep(0.1)\n",
    "        response = requests.get(submissions_url, headers=Config.SEC_HEADERS)\n",
    "        response.raise_for_status()\n",
    "        submissions = response.json()\n",
    "\n",
    "        filings_metadata = []\n",
    "        for i, form in enumerate(submissions['filings']['recent']['form']):\n",
    "            if form == '10-Q':\n",
    "                accession_number = submissions['filings']['recent']['accessionNumber'][i]\n",
    "                primary_document = submissions['filings']['recent']['primaryDocument'][i]\n",
    "                filing_date = submissions['filings']['recent']['filingDate'][i]\n",
    "                accession_number_clean = accession_number.replace('-', '')\n",
    "                filing_url = (\n",
    "                    f\"https://www.sec.gov/Archives/edgar/data/{cik}/\"\n",
    "                    f\"{accession_number_clean}/{primary_document}\"\n",
    "                )\n",
    "                metadata = {\n",
    "                    'ticker': ticker.upper(),\n",
    "                    'company_name': company_name,\n",
    "                    'filing_date': filing_date,\n",
    "                    'cik': cik,\n",
    "                    'filing_url': filing_url\n",
    "                }\n",
    "                filings_metadata.append(metadata)\n",
    "                if len(filings_metadata) >= num_filings:\n",
    "                    break\n",
    "        if not filings_metadata:\n",
    "            raise ValueError(f\"No recent 10-Q filings found for ticker '{ticker}'.\")\n",
    "        print(f\"  → Found {len(filings_metadata)} recent 10-Q filing metadata entries.\")\n",
    "        return filings_metadata\n",
    "\n",
    "    @staticmethod\n",
    "    def get_filing_html(filing_url: str) -> str:\n",
    "        time.sleep(0.1)\n",
    "        response = requests.get(filing_url, headers=Config.SEC_HEADERS)\n",
    "        response.raise_for_status()\n",
    "        return response.text\n",
    "\n",
    "    @staticmethod\n",
    "    def _normalize_header_text(text: str) -> str:\n",
    "        text = text.strip().upper()\n",
    "        part_match = re.search(r'^\\s*(PART\\s+I{1,2})', text)\n",
    "        if part_match:\n",
    "            return re.sub(r'\\s+', ' ', part_match.group(1))\n",
    "        item_match = re.search(r'^\\s*(ITEM\\s+\\d[A-Z]?)', text)\n",
    "        if item_match:\n",
    "            return re.sub(r'\\s+', ' ', item_match.group(1))\n",
    "        return None\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_html_table(table_tag) -> str:\n",
    "        markdown_rows = []\n",
    "        for tr in table_tag.find_all('tr'):\n",
    "            cells = [\" \".join(cell.get_text(strip=True).split())\n",
    "                    for cell in tr.find_all(['td', 'th'])]\n",
    "            if any(cells):\n",
    "                markdown_rows.append(cells)\n",
    "        if not markdown_rows: return \"\"\n",
    "        md_output = []\n",
    "        header = markdown_rows[0]\n",
    "        md_output.append(\"| \" + \" | \".join(header) + \" |\")\n",
    "        md_output.append(\"| \" + \" | \".join(['---'] * len(header)) + \" |\")\n",
    "        for row in markdown_rows[1:]:\n",
    "            while len(row) < len(header): row.append(\"\")\n",
    "            row = row[:len(header)]\n",
    "            md_output.append(\"| \" + \" | \".join(row) + \" |\")\n",
    "        return \"\\n\" + \"\\n\".join(md_output) + \"\\n\"\n",
    "\n",
    "    @classmethod\n",
    "    def parse_10q(cls, html_content: str) -> Dict:\n",
    "        # Use 'lxml' for speed and robustness\n",
    "        soup = BeautifulSoup(html_content, 'lxml')\n",
    "        potential_headers = soup.find_all(['p', 'b', 'strong', 'div'])\n",
    "        doc_headers = []\n",
    "        for header in potential_headers:\n",
    "            text = header.get_text(strip=True)\n",
    "            if len(text) > 100: continue\n",
    "            normalized_key = cls._normalize_header_text(text)\n",
    "            if normalized_key and not header.find_parent('a'):\n",
    "                doc_headers.append({'tag': header, 'key': normalized_key})\n",
    "        if not doc_headers: return {}\n",
    "        parsed_data = defaultdict(lambda: defaultdict(str))\n",
    "        current_part_key = None\n",
    "        for i, header_info in enumerate(doc_headers):\n",
    "            current_key = header_info['key']\n",
    "            if 'PART' in current_key:\n",
    "                current_part_key = current_key\n",
    "                continue\n",
    "            if 'ITEM' in current_key:\n",
    "                if not current_part_key: current_part_key = \"PART I\"\n",
    "                start_node = header_info['tag']\n",
    "                end_node = doc_headers[i + 1]['tag'] if i + 1 < len(doc_headers) else None\n",
    "                content_parts = []\n",
    "                element = start_node.next_element\n",
    "                while element and element != end_node:\n",
    "                    if isinstance(element, NavigableString):\n",
    "                        if not element.find_parent('table'):\n",
    "                            text = element.strip()\n",
    "                            if text: content_parts.append(text)\n",
    "                    elif element.name == 'table':\n",
    "                        if not element.find_parent('table'):\n",
    "                            table_markdown = cls._parse_html_table(element)\n",
    "                            if table_markdown: content_parts.append(table_markdown)\n",
    "                    element = element.next_element\n",
    "                full_content = \"\\n\".join(content_parts)\n",
    "                clean_content = re.sub(r'\\n{3,}', '\\n\\n', full_content).strip()\n",
    "                parsed_data[current_part_key][current_key] = clean_content\n",
    "        return {part: dict(items) for part, items in parsed_data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "831b48b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# PART 3: TEXT CHUNKING & EMBEDDING\n",
    "# ===========================================================================\n",
    "# (From base_RAG.ipynb)\n",
    "\n",
    "class DocumentProcessor:\n",
    "    def __init__(self, embedding_model_name: str = Config.EMBEDDING_MODEL):\n",
    "        print(f\"\\n Loading embedding model: {embedding_model_name}\")\n",
    "        self.model = SentenceTransformer(embedding_model_name)\n",
    "        print(f\"   ✓ Model loaded (dimension: {self.model.get_sentence_embedding_dimension()})\")\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=Config.CHUNK_SIZE,\n",
    "            chunk_overlap=Config.CHUNK_OVERLAP,\n",
    "            length_function=len,\n",
    "            add_start_index=False,\n",
    "        )\n",
    "        print(f\"   ✓ Initialized RecursiveCharacterTextSplitter\")\n",
    "\n",
    "    def generate_document_chunks(self, parsed_data: Dict, metadata: Dict,\n",
    "                                 embed_batch_size: int = 1024):\n",
    "        all_docs = []\n",
    "        for part, items in parsed_data.items():\n",
    "            for item, content in items.items():\n",
    "                if not content: continue\n",
    "                doc_metadata = {\n",
    "                    'ticker': metadata['ticker'],\n",
    "                    'company_name': metadata['company_name'],\n",
    "                    'filing_date': metadata['filing_date'],\n",
    "                    'filing_url': metadata['filing_url'],\n",
    "                    'part': part,\n",
    "                    'item': item\n",
    "                }\n",
    "                doc = Document(page_content=content, metadata=doc_metadata)\n",
    "                all_docs.append(doc)\n",
    "        if not all_docs: return\n",
    "        print(f\"     → Splitting {len(all_docs)} high-level 'Items' into smaller chunks...\")\n",
    "        chunked_docs = self.text_splitter.split_documents(all_docs)\n",
    "        print(f\"     → Generated {len(chunked_docs)} chunks\")\n",
    "        text_batch, metadata_batch = [], []\n",
    "        for chunk in chunked_docs:\n",
    "            text_batch.append(chunk.page_content)\n",
    "            metadata_batch.append(chunk.metadata)\n",
    "            if len(text_batch) >= embed_batch_size:\n",
    "                embeddings = self.model.encode(text_batch, show_progress_bar=False)\n",
    "                for txt, emb, meta in zip(text_batch, embeddings, metadata_batch):\n",
    "                    payload = {'text': txt, **meta}\n",
    "                    yield PointStruct(id=str(uuid.uuid4()), vector=emb.tolist(), payload=payload)\n",
    "                text_batch, metadata_batch = [], []\n",
    "        if text_batch:\n",
    "            embeddings = self.model.encode(text_batch, show_progress_bar=False)\n",
    "            for txt, emb, meta in zip(text_batch, embeddings, metadata_batch):\n",
    "                payload = {'text': txt, **meta}\n",
    "                yield PointStruct(id=str(uuid.uuid4()), vector=emb.tolist(), payload=payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7ca28ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# PART 4: QDRANT VECTOR DATABASE (MODIFIED)\n",
    "# ===========================================================================\n",
    "# (From base_RAG.ipynb, with modified search)\n",
    "\n",
    "class QdrantManager:\n",
    "    def __init__(self):\n",
    "        print(f\"\\nConnecting to Qdrant Cloud...\")\n",
    "        self.client = QdrantClient(url=Config.QDRANT_URL, api_key=Config.QDRANT_API_KEY)\n",
    "        print(f\"   ✓ Connected to Qdrant\")\n",
    "\n",
    "    def create_collection(self, collection_name: str = Config.COLLECTION_NAME,\n",
    "                         vector_size: int = Config.VECTOR_SIZE):\n",
    "        print(f\"\\n Setting up collection: {collection_name}\")\n",
    "        collections = self.client.get_collections().collections\n",
    "        exists = any(col.name == collection_name for col in collections)\n",
    "        if exists:\n",
    "            print(f\"   ⚠ Collection exists, recreating...\")\n",
    "            self.client.delete_collection(collection_name)\n",
    "        self.client.create_collection(\n",
    "            collection_name=collection_name,\n",
    "            vectors_config=models.VectorParams(size=vector_size, distance=models.Distance.COSINE)\n",
    "        )\n",
    "        print(f\"   ✓ Collection created\")\n",
    "        print(f\"   → Creating payload index for 'ticker'...\")\n",
    "        self.client.create_payload_index(\n",
    "            collection_name=collection_name,\n",
    "            field_name=\"ticker\",\n",
    "            field_schema=models.PayloadSchemaType.KEYWORD\n",
    "        )\n",
    "        print(f\"   → Creating payload index for 'item'...\")\n",
    "        self.client.create_payload_index(\n",
    "            collection_name=collection_name,\n",
    "            field_name=\"item\",\n",
    "            field_schema=models.PayloadSchemaType.KEYWORD\n",
    "        )\n",
    "        print(f\"   ✓ Payload indexes created.\")\n",
    "\n",
    "    def upsert_documents(self, points_generator,\n",
    "                        collection_name: str = Config.COLLECTION_NAME,\n",
    "                        batch_size: int = 2048) -> int:\n",
    "        print(f\" Uploading chunks to Qdrant in batches of {batch_size}...\")\n",
    "        batch, count = [], 0\n",
    "        for point in points_generator:\n",
    "            batch.append(point)\n",
    "            if len(batch) >= batch_size:\n",
    "                self.client.upsert(collection_name=collection_name, points=batch, wait=False)\n",
    "                count += len(batch)\n",
    "                print(f\"     → Uploaded {count} chunks so far...\")\n",
    "                batch = []\n",
    "        if batch:\n",
    "            self.client.upsert(collection_name=collection_name, points=batch, wait=False)\n",
    "            count += len(batch)\n",
    "        print(f\"  ✓ All chunks uploaded for this document. Total: {count}\")\n",
    "        return count\n",
    "\n",
    "    def search(self, query_vector: List[float],\n",
    "              collection_name: str = Config.COLLECTION_NAME,\n",
    "              limit: int = Config.TOP_K,\n",
    "              ticker_list: List[str] = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        --- MODIFIED SEARCH ---\n",
    "        Search for similar documents.\n",
    "        Now accepts a list of tickers to create an 'OR' filter.\n",
    "        \"\"\"\n",
    "        qdrant_filter = None\n",
    "        if ticker_list:\n",
    "            if len(ticker_list) == 1:\n",
    "                # 'must' is efficient for a single item\n",
    "                qdrant_filter = models.Filter(must=[\n",
    "                    models.FieldCondition(key=\"ticker\", match=models.MatchValue(value=ticker_list[0]))\n",
    "                ])\n",
    "            else:\n",
    "                # 'should' acts as an 'OR' filter for multiple tickers\n",
    "                qdrant_filter = models.Filter(should=[\n",
    "                    models.FieldCondition(key=\"ticker\", match=models.MatchValue(value=ticker))\n",
    "                    for ticker in ticker_list\n",
    "                ])\n",
    "        \n",
    "        results = self.client.search(\n",
    "            collection_name=collection_name,\n",
    "            query_vector=query_vector,\n",
    "            limit=limit,\n",
    "            query_filter=qdrant_filter,\n",
    "            with_payload=True\n",
    "        )\n",
    "        return [{'score': result.score, 'payload': result.payload} for result in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f86ff98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# PART 5: NER PIPELINE COMPONENTS\n",
    "# ===========================================================================\n",
    "# (From 02_ner.ipynb, now integrated)\n",
    "\n",
    "class FinancialKnowledgeBase:\n",
    "    \"\"\"Comprehensive knowledge base for financial entity normalization\"\"\"\n",
    "    def __init__(self):\n",
    "        print(\"   → Initializing Financial Knowledge Base...\")\n",
    "        self.company_to_ticker = self._build_company_ticker_map()\n",
    "        self.financial_metrics = {\n",
    "            \"revenue\": \"REVENUE\", \"revenues\": \"REVENUE\", \"total revenue\": \"REVENUE\",\n",
    "            \"net income\": \"NET_INCOME\", \"net earnings\": \"NET_INCOME\", \"profit\": \"NET_INCOME\",\n",
    "            \"operating income\": \"OPERATING_INCOME\", \"ebit\": \"EBIT\", \"ebitda\": \"EBITDA\",\n",
    "            \"cash flow\": \"CASH_FLOW\", \"operating cash flow\": \"OPERATING_CASH_FLOW\",\n",
    "            \"free cash flow\": \"FREE_CASH_FLOW\", \"fcf\": \"FREE_CASH_FLOW\",\n",
    "            \"total assets\": \"TOTAL_ASSETS\", \"total liabilities\": \"TOTAL_LIABILITIES\",\n",
    "            \"equity\": \"SHAREHOLDERS_EQUITY\", \"shareholders' equity\": \"SHAREHOLDERS_EQUITY\",\n",
    "            \"earnings per share\": \"EPS\", \"eps\": \"EPS\",\n",
    "        }\n",
    "        self.risk_types = {\n",
    "            \"market risk\": \"MARKET_RISK\", \"credit risk\": \"CREDIT_RISK\",\n",
    "            \"operational risk\": \"OPERATIONAL_RISK\", \"liquidity risk\": \"LIQUIDITY_RISK\",\n",
    "            \"interest rate risk\": \"INTEREST_RATE_RISK\", \"currency risk\": \"CURRENCY_RISK\",\n",
    "            \"foreign exchange risk\": \"CURRENCY_RISK\", \"regulatory risk\": \"REGULATORY_RISK\",\n",
    "            \"compliance risk\": \"COMPLIANCE_RISK\", \"legal risk\": \"LEGAL_RISK\",\n",
    "            \"reputational risk\": \"REPUTATIONAL_RISK\", \"strategic risk\": \"STRATEGIC_RISK\",\n",
    "            \"cybersecurity risk\": \"CYBERSECURITY_RISK\", \"technology risk\": \"TECHNOLOGY_RISK\",\n",
    "            \"political risk\": \"POLITICAL_RISK\", \"climate risk\": \"CLIMATE_RISK\",\n",
    "        }\n",
    "\n",
    "    def _build_company_ticker_map(self):\n",
    "        try:\n",
    "            response = requests.get(Config.CIK_MAP_URL, headers=Config.SEC_HEADERS)\n",
    "            response.raise_for_status()\n",
    "            company_data = response.json()\n",
    "            mapping = {}\n",
    "            for company in company_data.values():\n",
    "                ticker = company['ticker']\n",
    "                title = company['title']\n",
    "                mapping[title] = ticker\n",
    "                mapping[title.upper()] = ticker\n",
    "                base_name = re.sub(r'\\s+(Inc\\.?|Corp\\.?|Corporation|Company|Co\\.?|Ltd\\.?|LLC|L\\.P\\.)\\s*,?', '', title, flags=re.IGNORECASE).strip()\n",
    "                mapping[base_name] = ticker\n",
    "                mapping[base_name.upper()] = ticker\n",
    "                mapping[ticker] = ticker\n",
    "                mapping[ticker.upper()] = ticker\n",
    "            print(f\"     ✓ Built company map with {len(mapping)} entries\")\n",
    "            return mapping\n",
    "        except Exception as e:\n",
    "            print(f\"     ⚠ Warning: Could not build company map from SEC. Using fallback. Error: {e}\")\n",
    "            return {\n",
    "                \"Alphabet\": \"GOOGL\", \"Alphabet Inc.\": \"GOOGL\", \"Google\": \"GOOGL\",\n",
    "                \"Microsoft\": \"MSFT\", \"Microsoft Corporation\": \"MSFT\",\n",
    "                \"Tesla\": \"TSLA\", \"Tesla, Inc.\": \"TSLA\",\n",
    "                \"Apple\": \"AAPL\", \"Apple Inc.\": \"AAPL\",\n",
    "                \"Amazon\": \"AMZN\", \"Amazon.com\": \"AMZN\", \"AMZN\": \"AMZN\",\n",
    "                \"Meta\": \"META\", \"Meta Platforms\": \"META\", \"Facebook\": \"META\",\n",
    "                \"NVIDIA\": \"NVDA\", \"Nvidia\": \"NVDA\", \"NVDA\": \"NVDA\",\n",
    "                \"AMD\": \"AMD\", \"Advanced Micro Devices\": \"AMD\",\n",
    "                \"Oracle\": \"ORCL\", \"JPMorgan\": \"JPM\",\n",
    "            }\n",
    "\n",
    "    def normalize_company(self, text):\n",
    "        text_upper = text.upper()\n",
    "        if text_upper in self.company_to_ticker:\n",
    "            return self.company_to_ticker[text_upper]\n",
    "        clean_text = re.sub(r'\\s+(Inc\\.?|Corp\\.?|Corporation|Company|Co\\.?|Ltd\\.?|LLC)\\s*,?', '', text, flags=re.IGNORECASE).strip().upper()\n",
    "        if clean_text in self.company_to_ticker:\n",
    "            return self.company_to_ticker[clean_text]\n",
    "        return None\n",
    "\n",
    "    def normalize_metric(self, text):\n",
    "        return self.financial_metrics.get(text.lower().strip())\n",
    "\n",
    "    def normalize_risk(self, text):\n",
    "        return self.risk_types.get(text.lower().strip())\n",
    "\n",
    "class FinancialEntityExtractor:\n",
    "    \"\"\"Enhanced entity extractor with financial domain knowledge.\"\"\"\n",
    "    def __init__(self, knowledge_base: FinancialKnowledgeBase):\n",
    "        self.kb = knowledge_base\n",
    "        print(\"   → Loading NER models...\")\n",
    "        try:\n",
    "            self.spacy_nlp = spacy.load(Config.SPACY_MODEL)\n",
    "            print(f\"     ✓ Loaded spaCy model: {Config.SPACY_MODEL}\")\n",
    "        except Exception as e:\n",
    "            print(f\"     ✗ ERROR: Failed to load spaCy model '{Config.SPACY_MODEL}'.\")\n",
    "            print(\"     Please run: python -m spacy download en_core_web_lg\")\n",
    "            raise e\n",
    "\n",
    "        try:\n",
    "            self.transformer_ner = pipeline(\n",
    "                \"token-classification\",\n",
    "                model=Config.NER_MODEL,\n",
    "                aggregation_strategy=\"simple\",\n",
    "                device=-1 # Use CPU (-1) for broader compatibility\n",
    "            )\n",
    "            print(f\"     ✓ Loaded Transformer model: {Config.NER_MODEL}\")\n",
    "        except Exception as e:\n",
    "            print(f\"     ⚠ Warning: Could not load transformer model: {e}\")\n",
    "            self.transformer_ner = None\n",
    "\n",
    "    def extract_entities_spacy(self, text):\n",
    "        doc = self.spacy_nlp(text)\n",
    "        return [{'text': ent.text, 'label': ent.label_, 'start': ent.start_char, 'end': ent.end_char, 'source': 'spacy'} for ent in doc.ents]\n",
    "\n",
    "    def extract_entities_transformer(self, text):\n",
    "        if not self.transformer_ner: return []\n",
    "        try:\n",
    "            # Chunking for long texts (max 512 tokens)\n",
    "            max_length = 500 # Keep a buffer\n",
    "            chunks = [text[i:i+max_length] for i in range(0, len(text), max_length)]\n",
    "            all_entities = []\n",
    "            offset = 0\n",
    "            for chunk in chunks:\n",
    "                entities = self.transformer_ner(chunk)\n",
    "                for ent in entities:\n",
    "                    all_entities.append({'text': ent['word'], 'label': ent['entity_group'], 'start': offset + ent['start'], 'end': offset + ent['end'], 'score': ent['score'], 'source': 'transformer'})\n",
    "                offset += len(chunk)\n",
    "            return all_entities\n",
    "        except Exception as e:\n",
    "            print(f\"     ⚠ Transformer NER error: {e}\")\n",
    "            return []\n",
    "\n",
    "    def extract_financial_patterns(self, text):\n",
    "        entities = []\n",
    "        # Risk patterns (dynamic from KB)\n",
    "        risk_keys = '|'.join(re.escape(k) for k in self.kb.risk_types.keys())\n",
    "        risk_pattern = rf'({risk_keys})'\n",
    "        for match in re.finditer(risk_pattern, text, re.IGNORECASE):\n",
    "            entities.append({'text': match.group(1), 'label': 'RISK_MENTION', 'start': match.start(), 'end': match.end(), 'source': 'pattern'})\n",
    "        # Monetary patterns\n",
    "        monetary_pattern = r'(\\$\\s*\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?|\\d{1,3}(?:,\\d{3})+(?!\\d))'\n",
    "        for match in re.finditer(monetary_pattern, text):\n",
    "            if ',' in match.group(1) or '$' in match.group(1):\n",
    "                entities.append({'text': match.group(1), 'label': 'MONETARY_VALUE', 'start': match.start(), 'end': match.end(), 'source': 'pattern'})\n",
    "        return entities\n",
    "\n",
    "    def normalize_and_link(self, entities):\n",
    "        normalized = []\n",
    "        seen_spans = set()\n",
    "        for ent in sorted(entities, key=lambda x: x.get('score', 1.0), reverse=True):\n",
    "            span = (ent['start'], ent['end'])\n",
    "            if span in seen_spans: continue\n",
    "            seen_spans.add(span)\n",
    "            \n",
    "            ent_text, ent_label = ent['text'].strip(), ent['label']\n",
    "            result = {'original_text': ent_text, 'normalized_text': None, 'entity_type': ent_label, 'linked_id': None, 'confidence': ent.get('score', 1.0), 'source': ent.get('source', 'unknown'), 'span': span}\n",
    "\n",
    "            if ent_label in ['ORG', 'ORGANIZATION']:\n",
    "                ticker = self.kb.normalize_company(ent_text)\n",
    "                if ticker:\n",
    "                    result['normalized_text'] = ticker\n",
    "                    result['entity_type'] = 'COMPANY'\n",
    "                    result['linked_id'] = f\"TICKER:{ticker}\"\n",
    "            elif ent_label == 'CARDINAL' and ',' in ent_text:\n",
    "                result['entity_type'] = 'MONETARY_VALUE'\n",
    "                result['normalized_text'] = ent_text.replace(',', '')\n",
    "            elif ent_label in ['MONEY', 'MONETARY_VALUE']:\n",
    "                result['entity_type'] = 'MONETARY_VALUE'\n",
    "                numbers = re.findall(r'\\d+(?:,\\d{3})*(?:\\.\\d+)?', ent_text.replace('$', ''))\n",
    "                if numbers: result['normalized_text'] = numbers[0].replace(',', '')\n",
    "            elif ent_label == 'RISK_MENTION':\n",
    "                risk_type = self.kb.normalize_risk(ent_text)\n",
    "                if risk_type:\n",
    "                    result['normalized_text'] = risk_type\n",
    "                    result['entity_type'] = 'RISK_TYPE'\n",
    "                    result['linked_id'] = f\"RISK:{risk_type}\"\n",
    "            \n",
    "            metric = self.kb.normalize_metric(ent_text)\n",
    "            if metric:\n",
    "                result['entity_type'] = 'FINANCIAL_METRIC'\n",
    "                result['normalized_text'] = metric\n",
    "                result['linked_id'] = f\"METRIC:{metric}\"\n",
    "            \n",
    "            normalized.append(result)\n",
    "        return normalized\n",
    "\n",
    "    def extract_and_normalize(self, text):\n",
    "        spacy_entities = self.extract_entities_spacy(text)\n",
    "        transformer_entities = self.extract_entities_transformer(text)\n",
    "        pattern_entities = self.extract_financial_patterns(text)\n",
    "        all_entities = spacy_entities + transformer_entities + pattern_entities\n",
    "        normalized = self.normalize_and_link(all_entities)\n",
    "        return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bd6fa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# PART 6: ENHANCED RAG QUERY ENGINE\n",
    "# ===========================================================================\n",
    "# (Replaces ManualRAGEngine)\n",
    "\n",
    "class EnhancedRAGEngine:\n",
    "    \"\"\"\n",
    "    This is the new ENHANCED RAG query engine.\n",
    "    It uses the NER components to automatically detect tickers\n",
    "    in the question and filter the search.\n",
    "    \"\"\"\n",
    "    def __init__(self, document_processor: DocumentProcessor, qdrant_manager: QdrantManager):\n",
    "        print(\"\\n Initializing Enhanced RAG Query Engine...\")\n",
    "\n",
    "        # 1. Get components from main pipeline\n",
    "        self.embedding_model = document_processor.model\n",
    "        print(\"   ✓ Using existing embedding model\")\n",
    "        self.qdrant_manager = qdrant_manager\n",
    "        print(\"   ✓ Using existing QdrantManager for search\")\n",
    "\n",
    "        # 2. Build NER components\n",
    "        self.kb = FinancialKnowledgeBase()\n",
    "        self.extractor = FinancialEntityExtractor(self.kb)\n",
    "        \n",
    "        # 3. Initialize LLM and Prompt\n",
    "        self.llm = ChatOpenAI(model=Config.LLM_MODEL, api_key=Config.OPENAI_API_KEY, temperature=0)\n",
    "        template = \"\"\"You are a helpful financial analyst assistant. Your role is to answer questions about SEC 10-Q filings based ONLY on the provided context.\n",
    "- Base your answer strictly on the provided context from SEC filings\n",
    "- Cite specific sections (e.g., \"According to Item 1A...\") when referencing information\n",
    "- If the answer is not in the context, clearly state that\n",
    "\n",
    "Context:\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        self.prompt = ChatPromptTemplate.from_template(template)\n",
    "        print(\"   ✓ Enhanced RAG Engine ready.\")\n",
    "\n",
    "    def _format_context(self, search_results: List[Dict]) -> str:\n",
    "        context_str = \"\"\n",
    "        for i, result in enumerate(search_results, 1):\n",
    "            payload = result.get('payload', {})\n",
    "            text = payload.get('text', 'No text found')\n",
    "            item = payload.get('item', 'N/A')\n",
    "            ticker = payload.get('ticker', 'N/A')\n",
    "            context_str += f\"Source {i} ({ticker} - {item}):\\n\\\"{text}\\\"\\n\\n\"\n",
    "        return context_str.strip()\n",
    "\n",
    "    def query(self, question: str, ticker_filter: str = None):\n",
    "        print(f\"\\n Processing query with Enhanced Engine: '{question}'\")\n",
    "\n",
    "        # 1. Manually embed the query (same as before)\n",
    "        print(\"   → Manually embedding query...\")\n",
    "        query_vector = self.embedding_model.encode(question).tolist()\n",
    "\n",
    "        # 2. --- NEW NER STEP ---\n",
    "        #    Use NER to find tickers, unless user provides a manual override.\n",
    "        tickers_to_filter = []\n",
    "        if ticker_filter:\n",
    "            print(f\"   → Using manual ticker override: {ticker_filter}\")\n",
    "            tickers_to_filter = [ticker_filter.upper()]\n",
    "        else:\n",
    "            print(\"   → Running NER on query to find companies...\")\n",
    "            normalized_entities = self.extractor.extract_and_normalize(question)\n",
    "            tickers_from_ner = sorted(list(set(\n",
    "                ent['normalized_text'] \n",
    "                for ent in normalized_entities \n",
    "                if ent['entity_type'] == 'COMPANY' and ent['normalized_text']\n",
    "            )))\n",
    "            if tickers_from_ner:\n",
    "                print(f\"   → NER detected and will filter for: {tickers_from_ner}\")\n",
    "                tickers_to_filter = tickers_from_ner\n",
    "            else:\n",
    "                print(\"   → NER found no companies, searching all documents.\")\n",
    "        \n",
    "        # 3. Manually search Qdrant (using the new multi-ticker list)\n",
    "        print(\"   → Manually searching Qdrant...\")\n",
    "        search_results = self.qdrant_manager.search(\n",
    "            query_vector=query_vector,\n",
    "            limit=Config.TOP_K,\n",
    "            ticker_list=tickers_to_filter\n",
    "        )\n",
    "\n",
    "        if not search_results:\n",
    "            return {'answer': 'No relevant context was found in the documents to answer this question.', 'sources': []}\n",
    "\n",
    "        # 4. Manually format the prompt\n",
    "        print(\"   → Formatting context and building prompt...\")\n",
    "        formatted_context = self._format_context(search_results)\n",
    "        final_prompt_message = self.prompt.format_messages(context=formatted_context, input=question)\n",
    "\n",
    "        # 5. Manually invoke the LLM\n",
    "        print(\"   → Sending prompt to LLM...\")\n",
    "        llm_response = self.llm.invoke(final_prompt_message)\n",
    "        answer = llm_response.content\n",
    "\n",
    "        # 6. Format sources\n",
    "        sources = [{'ticker': r['payload'].get('ticker'), 'company': r['payload'].get('company_name'), 'item': r['payload'].get('item'), 'part': r['payload'].get('part'), 'filing_date': r['payload'].get('filing_date'), 'score': r['score']} for r in search_results]\n",
    "\n",
    "        return {'answer': answer, 'sources': sources}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8ae4082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# PART 7: MAIN PIPELINE ORCHESTRATOR\n",
    "# ===========================================================================\n",
    "# (Modified to use the EnhancedRAGEngine)\n",
    "\n",
    "class SECFilingRAGPipeline:\n",
    "    \"\"\"Main pipeline orchestrator\"\"\"\n",
    "    def __init__(self):\n",
    "        print(\"=\" * 70)\n",
    "        print(\"ENHANCED SEC 10-Q FILING RAG SYSTEM (RAG + NER)\")\n",
    "        print(\"=\" * 70)\n",
    "        self.loader = SECDocumentLoader()\n",
    "        self.processor = DocumentProcessor()\n",
    "        self.qdrant_manager = QdrantManager()\n",
    "\n",
    "        # Initialize the new query engine immediately\n",
    "        self.query_engine = EnhancedRAGEngine(\n",
    "            document_processor=self.processor,\n",
    "            qdrant_manager=self.qdrant_manager\n",
    "        )\n",
    "\n",
    "    def load_and_index_filings(self, tickers: List[str] = Config.TICKERS, num_filings_per_ticker: int = 1):\n",
    "        print(f\"\\n{'=' * 70}\")\n",
    "        print(f\"LOADING & INDEXING PHASE\")\n",
    "        print(f\"{'=' * 70}\")\n",
    "        print(f\"\\nProcessing {len(tickers)} companies: {', '.join(tickers)}\")\n",
    "        print(f\"(Fetching {num_filings_per_ticker} filings per company)\\n\")\n",
    "\n",
    "        self.qdrant_manager.create_collection()\n",
    "        successful_tickers, failed_tickers = [], []\n",
    "        total_chunks_indexed = 0\n",
    "\n",
    "        for idx, ticker in enumerate(tickers, 1):\n",
    "            print(f\"\\n[{idx}/{len(tickers)}] Processing {ticker}\")\n",
    "            print(\"-\" * 70)\n",
    "            ticker_chunks_count, num_filings_processed = 0, 0\n",
    "            try:\n",
    "                filings_metadata_list = self.loader.get_recent_10q_metadata(ticker, num_filings=num_filings_per_ticker)\n",
    "                for filing_metadata in filings_metadata_list:\n",
    "                    try:\n",
    "                        filing_date, filing_url = filing_metadata['filing_date'], filing_metadata['filing_url']\n",
    "                        print(f\"  → Downloading filing from: {filing_date}...\")\n",
    "                        html_content = self.loader.get_filing_html(filing_url)\n",
    "                        print(f\"  → Parsing 10-Q structure...\")\n",
    "                        parsed_data = self.loader.parse_10q(html_content)\n",
    "                        if not parsed_data:\n",
    "                            print(f\"  ⚠ Warning: No structured data parsed for {ticker} on {filing_date}\")\n",
    "                            del html_content; gc.collect(); continue\n",
    "                        print(f\"  → Creating chunks and embeddings generator...\")\n",
    "                        chunks_generator = self.processor.generate_document_chunks(parsed_data, filing_metadata)\n",
    "                        num_uploaded = self.qdrant_manager.upsert_documents(chunks_generator)\n",
    "                        if num_uploaded > 0:\n",
    "                            ticker_chunks_count += num_uploaded\n",
    "                            total_chunks_indexed += num_uploaded\n",
    "                            num_filings_processed += 1\n",
    "                        print(f\"  → Cleaning up memory...\"); del html_content; del parsed_data; del chunks_generator; gc.collect(); print(f\"  ✓ Memory cleaned.\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"  ✗ Error processing filing {filing_metadata.get('filing_date', 'unknown')} for {ticker}: {str(e)}\"); gc.collect()\n",
    "                if ticker_chunks_count > 0:\n",
    "                    successful_tickers.append(ticker); print(f\"  ✓ Finished {ticker}. Total chunks: {ticker_chunks_count} across {num_filings_processed} filings\")\n",
    "                else:\n",
    "                    failed_tickers.append(ticker); print(f\"  ⚠ No chunks created for {ticker}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ✗ Error processing {ticker} (failed to get metadata): {str(e)}\"); failed_tickers.append(ticker)\n",
    "            if idx < len(tickers): time.sleep(0.2)\n",
    "        \n",
    "        print(f\"\\n{'=' * 70}\\nINDEXING COMPLETE\\n{'=' * 70}\")\n",
    "        print(f\"✓ Successfully processed: {len(successful_tickers)} companies ({', '.join(successful_tickers)})\")\n",
    "        if failed_tickers: print(f\"✗ Failed: {len(failed_tickers)} companies ({', '.join(failed_tickers)})\")\n",
    "        print(f\"\\n Total chunks indexed: {total_chunks_indexed}\\n{'=' * 70}\\n\")\n",
    "\n",
    "    def query(self, question: str, ticker_filter: str = None):\n",
    "        \"\"\"Query the indexed filings using the ENHANCED engine\"\"\"\n",
    "        # The query engine is already initialized, so just use it\n",
    "        result = self.query_engine.query(question, ticker_filter)\n",
    "\n",
    "        # Print results\n",
    "        print(f\"\\n{'=' * 70}\\nANSWER\\n{'=' * 70}\")\n",
    "        print(f\"\\n{result['answer']}\\n\")\n",
    "        print(f\"{'=' * 70}\\nSOURCES ({len(result['sources'])} chunks)\\n{'=' * 70}\")\n",
    "        for i, source in enumerate(result['sources'], 1):\n",
    "            print(f\"\\n{i}. {source['company']} ({source['ticker']}) - {source['item']}\")\n",
    "            print(f\"   Filing Date: {source['filing_date']}\")\n",
    "            print(f\"   Relevance Score: {source['score']:.4f}\")\n",
    "        print(f\"\\n{'=' * 70}\\n\")\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4aad4aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ENHANCED SEC 10-Q FILING RAG SYSTEM (RAG + NER)\n",
      "======================================================================\n",
      "\n",
      " Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "   ✓ Model loaded (dimension: 384)\n",
      "   ✓ Initialized RecursiveCharacterTextSplitter\n",
      "\n",
      "Connecting to Qdrant Cloud...\n",
      "   ✓ Connected to Qdrant\n",
      "\n",
      " Initializing Enhanced RAG Query Engine...\n",
      "   ✓ Using existing embedding model\n",
      "   ✓ Using existing QdrantManager for search\n",
      "   → Initializing Financial Knowledge Base...\n",
      "     ✓ Built company map with 34397 entries\n",
      "   → Loading NER models...\n",
      "     ✓ Loaded spaCy model: en_core_web_lg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ✓ Loaded Transformer model: dslim/bert-base-NER\n",
      "   ✓ Enhanced RAG Engine ready.\n",
      "Starting the indexing process... This may take a long time.\n",
      "\n",
      "======================================================================\n",
      "LOADING & INDEXING PHASE\n",
      "======================================================================\n",
      "\n",
      "Processing 10 companies: NVDA, AAPL, MSFT, AMZN, META, GOOGL, TSLA, ORCL, JPM, AMD\n",
      "(Fetching 4 filings per company)\n",
      "\n",
      "\n",
      " Setting up collection: sec_filings_10q_enhanced\n",
      "   ✓ Collection created\n",
      "   → Creating payload index for 'ticker'...\n",
      "   → Creating payload index for 'item'...\n",
      "   ✓ Payload indexes created.\n",
      "\n",
      "[1/10] Processing NVDA\n",
      "----------------------------------------------------------------------\n",
      "  → Fetching CIK for ticker: NVDA...\n",
      "  → Found CIK: 0001045810 (NVIDIA CORP)\n",
      "  → Found 4 recent 10-Q filing metadata entries.\n",
      "  → Downloading filing from: 2025-08-27...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
      "     → Generated 245 chunks\n",
      "  ✓ All chunks uploaded for this document. Total: 245\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  → Downloading filing from: 2025-05-28...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
      "     → Generated 245 chunks\n",
      "  ✓ All chunks uploaded for this document. Total: 245\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  → Downloading filing from: 2024-11-20...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
      "     → Generated 257 chunks\n",
      "  ✓ All chunks uploaded for this document. Total: 257\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  → Downloading filing from: 2024-08-28...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
      "     → Generated 271 chunks\n",
      "  ✓ All chunks uploaded for this document. Total: 271\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  ✓ Finished NVDA. Total chunks: 1018 across 4 filings\n",
      "\n",
      "[2/10] Processing AAPL\n",
      "----------------------------------------------------------------------\n",
      "  → Fetching CIK for ticker: AAPL...\n",
      "  → Found CIK: 0000320193 (Apple Inc.)\n",
      "  → Found 4 recent 10-Q filing metadata entries.\n",
      "  → Downloading filing from: 2025-08-01...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 11 high-level 'Items' into smaller chunks...\n",
      "     → Generated 116 chunks\n",
      "  ✓ All chunks uploaded for this document. Total: 116\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  → Downloading filing from: 2025-05-02...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 11 high-level 'Items' into smaller chunks...\n",
      "     → Generated 136 chunks\n",
      "  ✓ All chunks uploaded for this document. Total: 136\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  → Downloading filing from: 2025-01-31...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 11 high-level 'Items' into smaller chunks...\n",
      "     → Generated 123 chunks\n",
      "  ✓ All chunks uploaded for this document. Total: 123\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  → Downloading filing from: 2024-08-02...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 11 high-level 'Items' into smaller chunks...\n",
      "     → Generated 111 chunks\n",
      "  ✓ All chunks uploaded for this document. Total: 111\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  ✓ Finished AAPL. Total chunks: 486 across 4 filings\n",
      "\n",
      "[3/10] Processing MSFT\n",
      "----------------------------------------------------------------------\n",
      "  → Fetching CIK for ticker: MSFT...\n",
      "  → Found CIK: 0000789019 (MICROSOFT CORP)\n",
      "  → Found 4 recent 10-Q filing metadata entries.\n",
      "  → Downloading filing from: 2025-10-29...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
      "     → Generated 31 chunks\n",
      "  ✓ All chunks uploaded for this document. Total: 31\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  → Downloading filing from: 2025-04-30...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
      "     → Generated 42 chunks\n",
      "  ✓ All chunks uploaded for this document. Total: 42\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  → Downloading filing from: 2025-01-29...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
      "     → Generated 36 chunks\n",
      "  ✓ All chunks uploaded for this document. Total: 36\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  → Downloading filing from: 2024-10-30...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
      "     → Generated 37 chunks\n",
      "  ✓ All chunks uploaded for this document. Total: 37\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  ✓ Finished MSFT. Total chunks: 146 across 4 filings\n",
      "\n",
      "[4/10] Processing AMZN\n",
      "----------------------------------------------------------------------\n",
      "  → Fetching CIK for ticker: AMZN...\n",
      "  → Found CIK: 0001018724 (AMAZON COM INC)\n",
      "  → Found 4 recent 10-Q filing metadata entries.\n",
      "  → Downloading filing from: 2025-10-31...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 11 high-level 'Items' into smaller chunks...\n",
      "     → Generated 352 chunks\n",
      "  ✓ All chunks uploaded for this document. Total: 352\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  → Downloading filing from: 2025-08-01...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 11 high-level 'Items' into smaller chunks...\n",
      "     → Generated 341 chunks\n",
      "  ✓ All chunks uploaded for this document. Total: 341\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  → Downloading filing from: 2025-05-02...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 11 high-level 'Items' into smaller chunks...\n",
      "     → Generated 324 chunks\n",
      "  ✓ All chunks uploaded for this document. Total: 324\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  → Downloading filing from: 2024-11-01...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 11 high-level 'Items' into smaller chunks...\n",
      "     → Generated 357 chunks\n",
      "  ✓ All chunks uploaded for this document. Total: 357\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  ✓ Finished AMZN. Total chunks: 1374 across 4 filings\n",
      "\n",
      "[5/10] Processing META\n",
      "----------------------------------------------------------------------\n",
      "  → Fetching CIK for ticker: META...\n",
      "  → Found CIK: 0001326801 (Meta Platforms, Inc.)\n",
      "  → Found 4 recent 10-Q filing metadata entries.\n",
      "  → Downloading filing from: 2025-10-30...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
      "     → Generated 704 chunks\n",
      "  ✓ All chunks uploaded for this document. Total: 704\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  → Downloading filing from: 2025-07-31...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
      "     → Generated 675 chunks\n",
      "  ✓ All chunks uploaded for this document. Total: 675\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  → Downloading filing from: 2025-05-01...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
      "     → Generated 646 chunks\n",
      "  ✓ All chunks uploaded for this document. Total: 646\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  → Downloading filing from: 2024-10-31...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
      "     → Generated 663 chunks\n",
      "  ✓ All chunks uploaded for this document. Total: 663\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  ✓ Finished META. Total chunks: 2688 across 4 filings\n",
      "\n",
      "[6/10] Processing GOOGL\n",
      "----------------------------------------------------------------------\n",
      "  → Fetching CIK for ticker: GOOGL...\n",
      "  → Found CIK: 0001652044 (Alphabet Inc.)\n",
      "  → Found 4 recent 10-Q filing metadata entries.\n",
      "  → Downloading filing from: 2025-10-30...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
      "     → Generated 277 chunks\n",
      "  ✓ All chunks uploaded for this document. Total: 277\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  → Downloading filing from: 2025-07-24...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
      "     → Generated 274 chunks\n",
      "  ✓ All chunks uploaded for this document. Total: 274\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  → Downloading filing from: 2025-04-25...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
      "     → Generated 244 chunks\n",
      "  ✓ All chunks uploaded for this document. Total: 244\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  → Downloading filing from: 2024-10-30...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
      "     → Generated 274 chunks\n",
      "  ✓ All chunks uploaded for this document. Total: 274\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  ✓ Finished GOOGL. Total chunks: 1069 across 4 filings\n",
      "\n",
      "[7/10] Processing TSLA\n",
      "----------------------------------------------------------------------\n",
      "  → Fetching CIK for ticker: TSLA...\n",
      "  → Found CIK: 0001318605 (Tesla, Inc.)\n",
      "  → Found 4 recent 10-Q filing metadata entries.\n",
      "  → Downloading filing from: 2025-10-23...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 11 high-level 'Items' into smaller chunks...\n",
      "     → Generated 247 chunks\n",
      "  ✓ All chunks uploaded for this document. Total: 247\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  → Downloading filing from: 2025-07-24...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 11 high-level 'Items' into smaller chunks...\n",
      "     → Generated 226 chunks\n",
      "  ✓ All chunks uploaded for this document. Total: 226\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  → Downloading filing from: 2025-04-23...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 11 high-level 'Items' into smaller chunks...\n",
      "     → Generated 210 chunks\n",
      "  ✓ All chunks uploaded for this document. Total: 210\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  → Downloading filing from: 2024-10-24...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 11 high-level 'Items' into smaller chunks...\n",
      "     → Generated 225 chunks\n",
      "  ✓ All chunks uploaded for this document. Total: 225\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  ✓ Finished TSLA. Total chunks: 908 across 4 filings\n",
      "\n",
      "[8/10] Processing ORCL\n",
      "----------------------------------------------------------------------\n",
      "  → Fetching CIK for ticker: ORCL...\n",
      "  → Found CIK: 0001341439 (ORACLE CORP)\n",
      "  → Found 4 recent 10-Q filing metadata entries.\n",
      "  → Downloading filing from: 2025-09-10...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
      "     → Generated 260 chunks\n",
      "  ✓ All chunks uploaded for this document. Total: 260\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  → Downloading filing from: 2025-03-11...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
      "     → Generated 280 chunks\n",
      "  ✓ All chunks uploaded for this document. Total: 280\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  → Downloading filing from: 2024-12-10...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
      "     → Generated 284 chunks\n",
      "  ✓ All chunks uploaded for this document. Total: 284\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  → Downloading filing from: 2024-09-10...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 8 high-level 'Items' into smaller chunks...\n",
      "     → Generated 258 chunks\n",
      "  ✓ All chunks uploaded for this document. Total: 258\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  ✓ Finished ORCL. Total chunks: 1082 across 4 filings\n",
      "\n",
      "[9/10] Processing JPM\n",
      "----------------------------------------------------------------------\n",
      "  → Fetching CIK for ticker: JPM...\n",
      "  → Found CIK: 0000019617 (JPMORGAN CHASE & CO)\n",
      "  → Found 3 recent 10-Q filing metadata entries.\n",
      "  → Downloading filing from: 2025-11-04...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 8 high-level 'Items' into smaller chunks...\n",
      "     → Generated 31 chunks\n",
      "  ✓ All chunks uploaded for this document. Total: 31\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  → Downloading filing from: 2025-08-05...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 8 high-level 'Items' into smaller chunks...\n",
      "     → Generated 23 chunks\n",
      "  ✓ All chunks uploaded for this document. Total: 23\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  → Downloading filing from: 2025-05-01...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 8 high-level 'Items' into smaller chunks...\n",
      "     → Generated 24 chunks\n",
      "  ✓ All chunks uploaded for this document. Total: 24\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  ✓ Finished JPM. Total chunks: 78 across 3 filings\n",
      "\n",
      "[10/10] Processing AMD\n",
      "----------------------------------------------------------------------\n",
      "  → Fetching CIK for ticker: AMD...\n",
      "  → Found CIK: 0000002488 (ADVANCED MICRO DEVICES INC)\n",
      "  → Found 4 recent 10-Q filing metadata entries.\n",
      "  → Downloading filing from: 2025-11-05...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
      "     → Generated 430 chunks\n",
      "  ✓ All chunks uploaded for this document. Total: 430\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  → Downloading filing from: 2025-08-06...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
      "     → Generated 422 chunks\n",
      "  ✓ All chunks uploaded for this document. Total: 422\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  → Downloading filing from: 2025-05-07...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
      "     → Generated 390 chunks\n",
      "  ✓ All chunks uploaded for this document. Total: 390\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  → Downloading filing from: 2024-10-30...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
      "     → Generated 392 chunks\n",
      "  ✓ All chunks uploaded for this document. Total: 392\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  ✓ Finished AMD. Total chunks: 1634 across 4 filings\n",
      "\n",
      "======================================================================\n",
      "INDEXING COMPLETE\n",
      "======================================================================\n",
      "✓ Successfully processed: 10 companies (NVDA, AAPL, MSFT, AMZN, META, GOOGL, TSLA, ORCL, JPM, AMD)\n",
      "\n",
      " Total chunks indexed: 10483\n",
      "======================================================================\n",
      "\n",
      "Indexing complete. Proceeding to queries.\n"
     ]
    }
   ],
   "source": [
    "# ===========================================================================\n",
    "# PART 8: USAGE EXAMPLE (Saving all results to one file)\n",
    "# ===========================================================================\n",
    "# (This is the exact block you provided, to save all results)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # =======================================================================\n",
    "    # 1. INITIALIZE PIPELINE\n",
    "    # =======================================================================\n",
    "    pipeline = SECFilingRAGPipeline()\n",
    "\n",
    "    # =======================================================================\n",
    "    # 2. LOAD AND INDEX FILINGS\n",
    "    # =======================================================================\n",
    "    # WARNING: This step can take a long time (10-20 minutes).\n",
    "    # If you have already run this successfully, you can comment out\n",
    "    # the line below to skip straight to querying.\n",
    "\n",
    "    print(\"Starting the indexing process... This may take a long time.\")\n",
    "    pipeline.load_and_index_filings(num_filings_per_ticker=4)\n",
    "    print(\"Indexing complete. Proceeding to queries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e5e7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running 5 queries...\n",
      "\n",
      "--- Running Query: What are the main risk factors mentioned by tech companies? ---\n",
      "\n",
      " Processing query with Enhanced Engine: 'What are the main risk factors mentioned by tech companies?'\n",
      "   → Manually embedding query...\n",
      "   → Running NER on query to find companies...\n",
      "   → NER found no companies, searching all documents.\n",
      "   → Manually searching Qdrant...\n",
      "   → Formatting context and building prompt...\n",
      "   → Sending prompt to LLM...\n",
      "\n",
      "======================================================================\n",
      "ANSWER\n",
      "======================================================================\n",
      "\n",
      "Based on the provided context from AMD's SEC 10-Q filings, the main risk factors mentioned include:\n",
      "\n",
      "1. **Economic and Strategic Risks**:\n",
      "   - Intel Corporation’s dominance of the microprocessor market and its aggressive business practices may limit AMD's ability to compete effectively on a level playing field (Source 1, 2, 5).\n",
      "   - The markets in which AMD's products are sold are highly competitive and rapidly evolving (Source 3, 4).\n",
      "   - The semiconductor industry is highly cyclical and has experienced severe downturns (Source 3, 4).\n",
      "\n",
      "2. **General Risks**:\n",
      "   - AMD's worldwide operations are subject to political, legal, and economic risks and natural disasters (Source 5).\n",
      "   - AMD may incur future impairments of its technology license purchases (Source 5).\n",
      "   - The inability to continue to attract and retain qualified personnel may hinder AMD's business (Source 5).\n",
      "   - AMD's stock price is subject to volatility (Source 5).\n",
      "\n",
      "These risk factors highlight the challenges AMD faces in terms of market competition, industry cycles, operational risks, and financial volatility.\n",
      "\n",
      "======================================================================\n",
      "SOURCES (5 chunks)\n",
      "======================================================================\n",
      "\n",
      "1. ADVANCED MICRO DEVICES INC (AMD) - ITEM 1A\n",
      "   Filing Date: 2025-05-07\n",
      "   Relevance Score: 0.7126\n",
      "\n",
      "2. ADVANCED MICRO DEVICES INC (AMD) - ITEM 1A\n",
      "   Filing Date: 2024-10-30\n",
      "   Relevance Score: 0.7126\n",
      "\n",
      "3. ADVANCED MICRO DEVICES INC (AMD) - ITEM 1A\n",
      "   Filing Date: 2025-11-05\n",
      "   Relevance Score: 0.7040\n",
      "\n",
      "4. ADVANCED MICRO DEVICES INC (AMD) - ITEM 1A\n",
      "   Filing Date: 2025-08-06\n",
      "   Relevance Score: 0.7040\n",
      "\n",
      "5. ADVANCED MICRO DEVICES INC (AMD) - ITEM 1A\n",
      "   Filing Date: 2024-10-30\n",
      "   Relevance Score: 0.6687\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "--- Running Query: What risks did Apple disclose in their latest 10-Q? ---\n",
      "\n",
      " Processing query with Enhanced Engine: 'What risks did Apple disclose in their latest 10-Q?'\n",
      "   → Manually embedding query...\n",
      "   → Running NER on query to find companies...\n",
      "   → NER detected and will filter for: ['AAPL']\n",
      "   → Manually searching Qdrant...\n",
      "   → Formatting context and building prompt...\n",
      "   → Sending prompt to LLM...\n",
      "\n",
      "======================================================================\n",
      "ANSWER\n",
      "======================================================================\n",
      "\n",
      "According to the provided context, Apple did not disclose any new material changes to their risk factors in their latest 10-Q. Specifically, \"there have been no material changes to the Company’s risk factors since the 2023 Form 10-K\" (Source 1, AAPL - ITEM 1A). Therefore, the risks disclosed remain consistent with those previously reported in their 2023 Form 10-K.\n",
      "\n",
      "======================================================================\n",
      "SOURCES (5 chunks)\n",
      "======================================================================\n",
      "\n",
      "1. Apple Inc. (AAPL) - ITEM 1A\n",
      "   Filing Date: 2024-08-02\n",
      "   Relevance Score: 0.7313\n",
      "\n",
      "2. Apple Inc. (AAPL) - ITEM 2\n",
      "   Filing Date: 2025-05-02\n",
      "   Relevance Score: 0.6400\n",
      "\n",
      "3. Apple Inc. (AAPL) - ITEM 2\n",
      "   Filing Date: 2025-05-02\n",
      "   Relevance Score: 0.6190\n",
      "\n",
      "4. Apple Inc. (AAPL) - ITEM 1A\n",
      "   Filing Date: 2025-05-02\n",
      "   Relevance Score: 0.6020\n",
      "\n",
      "5. Apple Inc. (AAPL) - ITEM 1\n",
      "   Filing Date: 2024-08-02\n",
      "   Relevance Score: 0.5975\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "--- Running Query: Compare the revenue trends of NVIDIA and AMD ---\n",
      "\n",
      " Processing query with Enhanced Engine: 'Compare the revenue trends of NVIDIA and AMD'\n",
      "   → Manually embedding query...\n",
      "   → Running NER on query to find companies...\n",
      "   → NER detected and will filter for: ['AMD', 'NVDA']\n",
      "   → Manually searching Qdrant...\n",
      "   → Formatting context and building prompt...\n",
      "   → Sending prompt to LLM...\n",
      "\n",
      "======================================================================\n",
      "ANSWER\n",
      "======================================================================\n",
      "\n",
      "Based on the provided context, NVIDIA and AMD both experienced revenue growth, but the details and drivers of this growth differ between the two companies.\n",
      "\n",
      "For NVIDIA:\n",
      "- Sales to one direct customer represented 12% of total revenue for the third quarter of fiscal year 2024, and sales to a second direct customer represented 11% of total revenue for the first nine months of fiscal year 2024, both attributable to the Compute & Networking segment (Source 1).\n",
      "\n",
      "For AMD:\n",
      "- Net revenue for the three months ended March 29, 2025, was $7.4 billion, a 36% increase compared to the prior year period. This increase was driven by growth in the Data Center segment, particularly from AMD EPYC™ CPU and AMD Instinct™ GPU sales, and in the Client and Gaming segment due to strong demand for “Zen 5” AMD Ryzen™ processors (Source 4).\n",
      "- Net revenue for the three months ended September 27, 2025, was $9.2 billion, also a 36% increase compared to the prior year period. This growth was driven by strong demand for AMD Ryzen™ processors, semi-custom game console SoCs, Radeon™ gaming GPUs, and 5th generation AMD EPYC™ processors and AMD Instinct™ MI350 Series GPUs (Source 5).\n",
      "\n",
      "In summary, both companies experienced revenue growth, with NVIDIA's growth being significantly influenced by sales to major customers in the Compute & Networking segment, while AMD's growth was driven by strong demand across its Data Center and Client and Gaming segments.\n",
      "\n",
      "======================================================================\n",
      "SOURCES (5 chunks)\n",
      "======================================================================\n",
      "\n",
      "1. NVIDIA CORP (NVDA) - ITEM 1\n",
      "   Filing Date: 2024-11-20\n",
      "   Relevance Score: 0.6546\n",
      "\n",
      "2. NVIDIA CORP (NVDA) - ITEM 1\n",
      "   Filing Date: 2024-11-20\n",
      "   Relevance Score: 0.6414\n",
      "\n",
      "3. NVIDIA CORP (NVDA) - ITEM 1\n",
      "   Filing Date: 2024-08-28\n",
      "   Relevance Score: 0.6392\n",
      "\n",
      "4. ADVANCED MICRO DEVICES INC (AMD) - ITEM 2\n",
      "   Filing Date: 2025-05-07\n",
      "   Relevance Score: 0.6338\n",
      "\n",
      "5. ADVANCED MICRO DEVICES INC (AMD) - ITEM 2\n",
      "   Filing Date: 2025-11-05\n",
      "   Relevance Score: 0.6258\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "--- Running Query: What was Tesla's R&D spending in the latest quarter? ---\n",
      "\n",
      " Processing query with Enhanced Engine: 'What was Tesla's R&D spending in the latest quarter?'\n",
      "   → Manually embedding query...\n",
      "   → Running NER on query to find companies...\n",
      "   → NER found no companies, searching all documents.\n",
      "   → Manually searching Qdrant...\n",
      "   → Formatting context and building prompt...\n",
      "   → Sending prompt to LLM...\n",
      "\n",
      "======================================================================\n",
      "ANSWER\n",
      "======================================================================\n",
      "\n",
      "The specific amount of Tesla's R&D spending in the latest quarter is not provided in the context. The context only provides information about the percentage increase and the factors contributing to the change in R&D expenses.\n",
      "\n",
      "======================================================================\n",
      "SOURCES (5 chunks)\n",
      "======================================================================\n",
      "\n",
      "1. Tesla, Inc. (TSLA) - ITEM 2\n",
      "   Filing Date: 2024-10-24\n",
      "   Relevance Score: 0.5818\n",
      "\n",
      "2. Tesla, Inc. (TSLA) - ITEM 2\n",
      "   Filing Date: 2025-10-23\n",
      "   Relevance Score: 0.5687\n",
      "\n",
      "3. Tesla, Inc. (TSLA) - ITEM 2\n",
      "   Filing Date: 2025-07-24\n",
      "   Relevance Score: 0.5555\n",
      "\n",
      "4. Tesla, Inc. (TSLA) - ITEM 2\n",
      "   Filing Date: 2024-10-24\n",
      "   Relevance Score: 0.5433\n",
      "\n",
      "5. Alphabet Inc. (GOOGL) - ITEM 2\n",
      "   Filing Date: 2025-10-30\n",
      "   Relevance Score: 0.5300\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "--- Running Query: What risks did Apple disclose in their latest 10-Q? ---\n",
      "\n",
      " Processing query with Enhanced Engine: 'What risks did Apple disclose in their latest 10-Q?'\n",
      "   → Manually embedding query...\n",
      "   → Using manual ticker override: MSFT\n",
      "   → Manually searching Qdrant...\n",
      "   → Formatting context and building prompt...\n",
      "   → Sending prompt to LLM...\n",
      "\n",
      "======================================================================\n",
      "ANSWER\n",
      "======================================================================\n",
      "\n",
      "The provided context does not contain information about Apple's 10-Q filing or the risks disclosed by Apple. The context only includes information from Microsoft's 10-Q filing.\n",
      "\n",
      "======================================================================\n",
      "SOURCES (5 chunks)\n",
      "======================================================================\n",
      "\n",
      "1. MICROSOFT CORP (MSFT) - ITEM 1A\n",
      "   Filing Date: 2025-04-30\n",
      "   Relevance Score: 0.3837\n",
      "\n",
      "2. MICROSOFT CORP (MSFT) - ITEM 3\n",
      "   Filing Date: 2025-10-29\n",
      "   Relevance Score: 0.3547\n",
      "\n",
      "3. MICROSOFT CORP (MSFT) - ITEM 3\n",
      "   Filing Date: 2025-04-30\n",
      "   Relevance Score: 0.3547\n",
      "\n",
      "4. MICROSOFT CORP (MSFT) - ITEM 3\n",
      "   Filing Date: 2025-01-29\n",
      "   Relevance Score: 0.3547\n",
      "\n",
      "5. MICROSOFT CORP (MSFT) - ITEM 3\n",
      "   Filing Date: 2024-10-30\n",
      "   Relevance Score: 0.3547\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "--- All queries complete. Saving all results to: data\\base_RAG_NER_results.json ---\n",
      "✓ Successfully saved all answers.\n",
      "\n",
      "Pipeline run finished.\n"
     ]
    }
   ],
   "source": [
    "# =======================================================================\n",
    "# 3. DEFINE QUESTIONS AND SAVE ALL ANSWERS TO ONE FILE\n",
    "# =======================================================================\n",
    "\n",
    "# Define the output folder\n",
    "DATA_FOLDER = \"data\"\n",
    "if not os.path.exists(DATA_FOLDER):\n",
    "    os.makedirs(DATA_FOLDER)\n",
    "    print(f\"Created data folder: {DATA_FOLDER}\")\n",
    "\n",
    "# Define the list of questions to run\n",
    "queries_to_run = [\n",
    "    {\n",
    "        \"question\": \"What are the main risk factors mentioned by tech companies?\",\n",
    "        \"ticker_filter\": None, # NER will search all companies\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What risks did Apple disclose in their latest 10-Q?\",\n",
    "        \"ticker_filter\": None, # NER will find \"Apple\" -> \"AAPL\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Compare the revenue trends of NVIDIA and AMD\",\n",
    "        \"ticker_filter\": None, # NER will find \"NVIDIA\" -> \"NVDA\" and \"AMD\" -> \"AMD\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What was Tesla's R&D spending in the latest quarter?\",\n",
    "        \"ticker_filter\": None, # NER will find \"Tesla\" -> \"TSLA\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What risks did Apple disclose in their latest 10-Q?\",\n",
    "        \"ticker_filter\": \"MSFT\", # MANUAL OVERRIDE: This will search for MSFT risks\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"\\nRunning {len(queries_to_run)} queries...\")\n",
    "\n",
    "# This list will hold all the results\n",
    "all_results = []\n",
    "\n",
    "for query in queries_to_run:\n",
    "    print(f\"\\n--- Running Query: {query['question']} ---\")\n",
    "    \n",
    "    # Run the query and get the result\n",
    "    result = pipeline.query(\n",
    "        question=query[\"question\"],\n",
    "        ticker_filter=query[\"ticker_filter\"]\n",
    "    )\n",
    "    \n",
    "    # Create a dictionary for this query and its result\n",
    "    query_output = {\n",
    "        \"question\": query[\"question\"],\n",
    "        \"ticker_filter_manual\": query[\"ticker_filter\"],\n",
    "        \"response\": result  # The 'result' dict contains 'answer' and 'sources'\n",
    "    }\n",
    "    \n",
    "    # Add the output to our main list\n",
    "    all_results.append(query_output)\n",
    "\n",
    "# After all queries are done, save the entire list to one file\n",
    "save_path = os.path.join(DATA_FOLDER, \"RAG_NER_results.json\")\n",
    "print(f\"\\n--- All queries complete. Saving all results to: {save_path} ---\")\n",
    "\n",
    "try:\n",
    "    with open(save_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_results, f, indent=4)\n",
    "    print(f\"✓ Successfully saved all answers.\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Failed to save results: {e}\")\n",
    "\n",
    "print(\"\\nPipeline run finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSA4213-SEC-Fillings-Chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
