{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7OfAFp4yHjyI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OfAFp4yHjyI",
        "outputId": "a87e391b-c5d6-4360-d32f-c6f2e83ddc31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/Colab Notebooks/DSA4213\n",
            "total 290\n",
            "-rw------- 1 root root 27866 Nov  9 12:02 01_data_loading_exploration.ipynb\n",
            "-rw------- 1 root root 70110 Nov  9 12:02 02_ner.ipynb\n",
            "-rw------- 1 root root  3528 Nov  9 12:02 baseline_no_RAG.ipynb\n",
            "-rw------- 1 root root 49243 Nov  9 15:27 base_RAG.ipynb\n",
            "-rw------- 1 root root   395 Nov  9 12:29 .env\n",
            "drwx------ 2 root root  4096 Nov  9 14:58 __pycache__\n",
            "-rw------- 1 root root  8708 Nov  9 14:32 query_processor.py\n",
            "-rw------- 1 root root 32273 Nov  9 14:55 query_processors.ipynb\n",
            "-rw------- 1 root root 96703 Nov  9 12:02 RAG_post_retrieval.ipynb\n",
            "-rw------- 1 root root  1082 Nov  9 15:05 requirements.txt\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# %cd /content/drive/MyDrive/Colab Notebooks/DSA4213\n",
        "# !ls -la"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "7d3f8234",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d3f8234",
        "outputId": "bfbaaf7d-5920-4e9b-e726-e57db065b756"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting requests==2.32.5 (from -r requirements.txt (line 2))\n",
            "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting beautifulsoup4==4.12.3 (from -r requirements.txt (line 3))\n",
            "  Using cached beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting lxml==5.3.0 (from -r requirements.txt (line 4))\n",
            "  Using cached lxml-5.3.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting sentence_transformers==5.0.0 (from -r requirements.txt (line 7))\n",
            "  Using cached sentence_transformers-5.0.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: qdrant_client==1.15.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 10)) (1.15.1)\n",
            "Collecting openai==2.6.0 (from -r requirements.txt (line 13))\n",
            "  Using cached openai-2.6.0-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting langchain==1.0.5 (from -r requirements.txt (line 14))\n",
            "  Using cached langchain-1.0.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: langchain_core==1.0.4 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 15)) (1.0.4)\n",
            "Requirement already satisfied: langchain_openai==1.0.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 16)) (1.0.2)\n",
            "Collecting langchain_qdrant==1.1.0 (from -r requirements.txt (line 17))\n",
            "  Using cached langchain_qdrant-1.1.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting langchain_community==0.4.1 (from -r requirements.txt (line 18))\n",
            "  Using cached langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langchain_huggingface==1.0.1 (from -r requirements.txt (line 19))\n",
            "  Using cached langchain_huggingface-1.0.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting langchain_text_splitters==1.0.0 (from -r requirements.txt (line 20))\n",
            "  Using cached langchain_text_splitters-1.0.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting transformers==4.44.2 (from -r requirements.txt (line 25))\n",
            "  Using cached transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
            "Collecting accelerate==0.34.2 (from -r requirements.txt (line 26))\n",
            "  Using cached accelerate-0.34.2-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting datasets==2.21.0 (from -r requirements.txt (line 27))\n",
            "  Using cached datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting evaluate==0.4.2 (from -r requirements.txt (line 28))\n",
            "  Using cached evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting tiktoken==0.7.0 (from -r requirements.txt (line 29))\n",
            "  Using cached tiktoken-0.7.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting scikit-learn==1.5.2 (from -r requirements.txt (line 34))\n",
            "  Using cached scikit_learn-1.5.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting pandas==2.2.3 (from -r requirements.txt (line 35))\n",
            "  Using cached pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "Collecting numpy==1.26.4 (from -r requirements.txt (line 36))\n",
            "  Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting matplotlib==3.9.2 (from -r requirements.txt (line 37))\n",
            "  Using cached matplotlib-3.9.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting spacy==3.7.5 (from -r requirements.txt (line 42))\n",
            "  Using cached spacy-3.7.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
            "\u001b[31mERROR: Ignored the following versions that require a different python version: 1.21.2 Requires-Python >=3.7,<3.11; 1.21.3 Requires-Python >=3.7,<3.11; 1.21.4 Requires-Python >=3.7,<3.11; 1.21.5 Requires-Python >=3.7,<3.11; 1.21.6 Requires-Python >=3.7,<3.11\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement en-core-web-sm==3.7.1 (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for en-core-web-sm==3.7.1\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d0b67a3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d0b67a3",
        "outputId": "7c720b74-1f86-47d7-d0ec-47b889cb6fe4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded via spec_from_file_location ✓\n",
            "QueryProcessor OK\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "SEC 10-Q Filing RAG System\n",
        "A complete pipeline for loading, processing, embedding, and querying SEC 10-Q filings\n",
        "using Qdrant vector database and OpenAI GPT-4o.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import re\n",
        "import uuid\n",
        "import time\n",
        "import requests\n",
        "import gc\n",
        "from typing import List, Dict, Tuple\n",
        "from collections import defaultdict\n",
        "from bs4 import BeautifulSoup, NavigableString\n",
        "\n",
        "# External libraries (install via pip)\n",
        "from langchain_core.documents import Document # Will be needed for document aggregation\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
        "from openai import OpenAI\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import importlib.util, types, pathlib\n",
        "\n",
        "module_path = pathlib.Path.cwd() /\"query_processor.py\"\n",
        "spec = importlib.util.spec_from_file_location(\"query_processor\", str(module_path))\n",
        "qp_mod = importlib.util.module_from_spec(spec)\n",
        "spec.loader.exec_module(qp_mod)\n",
        "\n",
        "QueryProcessor = qp_mod.QueryProcessor\n",
        "qp = QueryProcessor()\n",
        "print(\"Loaded via spec_from_file_location ✓\")\n",
        "\n",
        "print(\"QueryProcessor OK\")\n",
        "\n",
        "# from google.colab import userdata\n",
        "\n",
        "# # Load secrets from Colab and set them as environment variables\n",
        "# os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "# os.environ['QDRANT_URL'] = userdata.get('QDRANT_URL')\n",
        "# os.environ['QDRANT_API_KEY'] = userdata.get('QDRANT_API_KEY')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "f3255ddf",
      "metadata": {
        "id": "f3255ddf"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PART 1: CONFIGURATION & SETUP\n",
        "# ============================================================================\n",
        "\n",
        "class Config:\n",
        "    \"\"\"Configuration class for API keys and model settings\"\"\"\n",
        "\n",
        "    # API Keys - SET THESE BEFORE RUNNING\n",
        "    QDRANT_URL = os.getenv(\"QDRANT_URL\")\n",
        "    QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")\n",
        "    OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "    # SEC EDGAR Configuration\n",
        "    SEC_HEADERS = {'User-Agent': 'SEC10Q-RAG-System research@example.com'}\n",
        "    CIK_MAP_URL = 'https://www.sec.gov/files/company_tickers.json'\n",
        "\n",
        "    # Model Configuration\n",
        "    EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"  # 384 dimensions\n",
        "    LLM_MODEL = \"gpt-4o\"\n",
        "\n",
        "    # Collection Configuration\n",
        "    COLLECTION_NAME = \"sec_filings_10q\"\n",
        "    VECTOR_SIZE = 384  # Dimension for all-MiniLM-L6-v2\n",
        "\n",
        "    # Chunking Configuration\n",
        "    CHUNK_SIZE = 800  # Characters per chunk\n",
        "    CHUNK_OVERLAP = 200  # Overlap between chunks\n",
        "\n",
        "    # Retrieval Configuration\n",
        "    TOP_K = 5  # Number of chunks to retrieve\n",
        "\n",
        "    # Company Tickers\n",
        "    TICKERS = ['NVDA', 'AAPL', 'MSFT', 'AMZN', 'META', 'GOOGL', 'TSLA', 'ORCL', 'JPM', 'AMD']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "bfb2dfc0",
      "metadata": {
        "id": "bfb2dfc0"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PART 2: DOCUMENT LOADING\n",
        "# ============================================================================\n",
        "\n",
        "class SECDocumentLoader:\n",
        "    \"\"\"Handles fetching and parsing of SEC 10-Q filings\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def get_recent_10q_metadata(ticker: str, num_filings: int = 4) -> List[Dict[str, str]]:\n",
        "        \"\"\"\n",
        "        Fetches the metadata (links, dates, etc.) for the latest N 10-Q filings.\n",
        "        Does NOT fetch the actual HTML content.\n",
        "\n",
        "        Args:\n",
        "            ticker: The company ticker (e.g., 'AAPL')\n",
        "            num_filings: The number of recent 10-Q filings to fetch\n",
        "\n",
        "        Returns:\n",
        "            List of metadata dictionaries\n",
        "        \"\"\"\n",
        "        print(f\"  → Fetching CIK for ticker: {ticker}...\")\n",
        "\n",
        "        # Get CIK mapping\n",
        "        response = requests.get(Config.CIK_MAP_URL, headers=Config.SEC_HEADERS)\n",
        "        response.raise_for_status()\n",
        "        company_data = response.json()\n",
        "\n",
        "        # Find CIK\n",
        "        cik = None\n",
        "        company_name = None\n",
        "        for company in company_data.values():\n",
        "            if company['ticker'] == ticker.upper():\n",
        "                cik = str(company['cik_str']).zfill(10)\n",
        "                company_name = company['title']\n",
        "                break\n",
        "\n",
        "        if not cik:\n",
        "            raise ValueError(f\"Ticker '{ticker}' not found in SEC CIK mapping.\")\n",
        "\n",
        "        print(f\"  → Found CIK: {cik} ({company_name})\")\n",
        "\n",
        "        # Fetch submission history\n",
        "        submissions_url = f\"https://data.sec.gov/submissions/CIK{cik}.json\"\n",
        "        time.sleep(0.1)  # Rate limiting\n",
        "        response = requests.get(submissions_url, headers=Config.SEC_HEADERS)\n",
        "        response.raise_for_status()\n",
        "        submissions = response.json()\n",
        "\n",
        "        # Find latest N 10-Q filings metadata\n",
        "        filings_metadata = []\n",
        "        for i, form in enumerate(submissions['filings']['recent']['form']):\n",
        "            if form == '10-Q':\n",
        "                accession_number = submissions['filings']['recent']['accessionNumber'][i]\n",
        "                primary_document = submissions['filings']['recent']['primaryDocument'][i]\n",
        "                filing_date = submissions['filings']['recent']['filingDate'][i]\n",
        "\n",
        "                accession_number_clean = accession_number.replace('-', '')\n",
        "\n",
        "                # Construct the filing URL\n",
        "                filing_url = (\n",
        "                    f\"https://www.sec.gov/Archives/edgar/data/{cik}/\"\n",
        "                    f\"{accession_number_clean}/{primary_document}\"\n",
        "                )\n",
        "\n",
        "                metadata = {\n",
        "                    'ticker': ticker.upper(),\n",
        "                    'company_name': company_name,\n",
        "                    'filing_date': filing_date,\n",
        "                    'cik': cik,\n",
        "                    'filing_url': filing_url\n",
        "                }\n",
        "                filings_metadata.append(metadata)\n",
        "\n",
        "                if len(filings_metadata) >= num_filings:\n",
        "                    break\n",
        "\n",
        "        if not filings_metadata:\n",
        "            raise ValueError(f\"No recent 10-Q filings found for ticker '{ticker}'.\")\n",
        "\n",
        "        print(f\"  → Found {len(filings_metadata)} recent 10-Q filing metadata entries.\")\n",
        "        return filings_metadata\n",
        "\n",
        "    @staticmethod\n",
        "    def get_filing_html(filing_url: str) -> str:\n",
        "        \"\"\"Fetches the HTML content for a single filing URL.\"\"\"\n",
        "        time.sleep(0.1)  # Rate limiting\n",
        "        response = requests.get(filing_url, headers=Config.SEC_HEADERS)\n",
        "        response.raise_for_status()\n",
        "        return response.text\n",
        "\n",
        "    @staticmethod\n",
        "    def _normalize_header_text(text: str) -> str:\n",
        "        \"\"\"Normalizes header text to standard format\"\"\"\n",
        "        text = text.strip().upper()\n",
        "\n",
        "        # Match \"PART I\" or \"PART II\"\n",
        "        part_match = re.search(r'^\\s*(PART\\s+I{1,2})', text)\n",
        "        if part_match:\n",
        "            return re.sub(r'\\s+', ' ', part_match.group(1))\n",
        "\n",
        "        # Match \"ITEM 1\", \"ITEM 1A\", etc.\n",
        "        item_match = re.search(r'^\\s*(ITEM\\s+\\d[A-Z]?)', text)\n",
        "        if item_match:\n",
        "            return re.sub(r'\\s+', ' ', item_match.group(1))\n",
        "\n",
        "        return None\n",
        "\n",
        "    @staticmethod\n",
        "    def _parse_html_table(table_tag) -> str:\n",
        "        \"\"\"Converts HTML table to Markdown format\"\"\"\n",
        "        markdown_rows = []\n",
        "\n",
        "        for tr in table_tag.find_all('tr'):\n",
        "            cells = [\" \".join(cell.get_text(strip=True).split())\n",
        "                    for cell in tr.find_all(['td', 'th'])]\n",
        "            if any(cells):\n",
        "                markdown_rows.append(cells)\n",
        "\n",
        "        if not markdown_rows:\n",
        "            return \"\"\n",
        "\n",
        "        md_output = []\n",
        "        header = markdown_rows[0]\n",
        "        md_output.append(\"| \" + \" | \".join(header) + \" |\")\n",
        "        md_output.append(\"| \" + \" | \".join(['---'] * len(header)) + \" |\")\n",
        "\n",
        "        for row in markdown_rows[1:]:\n",
        "            while len(row) < len(header):\n",
        "                row.append(\"\")\n",
        "            row = row[:len(header)]\n",
        "            md_output.append(\"| \" + \" | \".join(row) + \" |\")\n",
        "\n",
        "        return \"\\n\" + \"\\n\".join(md_output) + \"\\n\"\n",
        "\n",
        "    @classmethod\n",
        "    def parse_10q(cls, html_content: str) -> Dict:\n",
        "        \"\"\"Parses HTML content into structured dictionary\"\"\"\n",
        "        # --- KEY CHANGE ---\n",
        "        # Use 'lxml' for better memory efficiency\n",
        "        soup = BeautifulSoup(html_content, 'lxml')\n",
        "\n",
        "        potential_headers = soup.find_all(['p', 'b', 'strong', 'div'])\n",
        "\n",
        "        doc_headers = []\n",
        "        for header in potential_headers:\n",
        "            text = header.get_text(strip=True)\n",
        "            if len(text) > 100:\n",
        "                continue\n",
        "\n",
        "            normalized_key = cls._normalize_header_text(text)\n",
        "            if normalized_key:\n",
        "                if not header.find_parent('a'):\n",
        "                    doc_headers.append({'tag': header, 'key': normalized_key})\n",
        "\n",
        "        if not doc_headers:\n",
        "            return {}\n",
        "\n",
        "        parsed_data = defaultdict(lambda: defaultdict(str))\n",
        "        current_part_key = None\n",
        "\n",
        "        for i, header_info in enumerate(doc_headers):\n",
        "            current_key = header_info['key']\n",
        "\n",
        "            if 'PART' in current_key:\n",
        "                current_part_key = current_key\n",
        "                continue\n",
        "\n",
        "            if 'ITEM' in current_key:\n",
        "                if not current_part_key:\n",
        "                    current_part_key = \"PART I\"\n",
        "\n",
        "                start_node = header_info['tag']\n",
        "                end_node = doc_headers[i + 1]['tag'] if i + 1 < len(doc_headers) else None\n",
        "\n",
        "                content_parts = []\n",
        "                element = start_node.next_element\n",
        "\n",
        "                while element and element != end_node:\n",
        "                    if isinstance(element, NavigableString):\n",
        "                        if not element.find_parent('table'):\n",
        "                            text = element.strip()\n",
        "                            if text:\n",
        "                                content_parts.append(text)\n",
        "                    elif element.name == 'table':\n",
        "                        if not element.find_parent('table'):\n",
        "                            table_markdown = cls._parse_html_table(element)\n",
        "                            if table_markdown:\n",
        "                                content_parts.append(table_markdown)\n",
        "\n",
        "                    element = element.next_element\n",
        "\n",
        "                full_content = \"\\n\".join(content_parts)\n",
        "                clean_content = re.sub(r'\\n{3,}', '\\n\\n', full_content).strip()\n",
        "\n",
        "                parsed_data[current_part_key][current_key] = clean_content\n",
        "\n",
        "        return {part: dict(items) for part, items in parsed_data.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "2cbff8d1",
      "metadata": {
        "id": "2cbff8d1"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PART 3: TEXT CHUNKING & EMBEDDING\n",
        "# ============================================================================\n",
        "import uuid\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "class DocumentProcessor:\n",
        "    \"\"\"\n",
        "    Processes documents into chunks using LangChain's splitter\n",
        "    and then embeds them in batches.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embedding_model_name: str = Config.EMBEDDING_MODEL):\n",
        "        \"\"\"Initialize with embedding model and LangChain text splitter\"\"\"\n",
        "        print(f\"\\n Loading embedding model: {embedding_model_name}\")\n",
        "        self.model = SentenceTransformer(embedding_model_name)\n",
        "        print(f\"   ✓ Model loaded (dimension: {self.model.get_sentence_embedding_dimension()})\")\n",
        "\n",
        "        # Initialize LangChain's splitter\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=Config.CHUNK_SIZE,\n",
        "            chunk_overlap=Config.CHUNK_OVERLAP,\n",
        "            length_function=len,\n",
        "            add_start_index=False, # Simpler metadata\n",
        "        )\n",
        "        print(f\"   ✓ Initialized RecursiveCharacterTextSplitter (chunk: {Config.CHUNK_SIZE}, overlap: {Config.CHUNK_OVERLAP})\")\n",
        "\n",
        "    def generate_document_chunks(self, parsed_data: Dict, metadata: Dict,\n",
        "                                 embed_batch_size: int = 1024):\n",
        "        \"\"\"\n",
        "        Processes parsed 10-Q data using LangChain's splitter,\n",
        "        then YIELDS chunk points one by one after batch-embedding.\n",
        "        \"\"\"\n",
        "\n",
        "        # Convert the parsed dict into a list of LangChain Document objects\n",
        "        all_docs = []\n",
        "        for part, items in parsed_data.items():\n",
        "            for item, content in items.items():\n",
        "                if not content:\n",
        "                    continue\n",
        "\n",
        "                # Create a metadata dict for *this specific document*\n",
        "                # before it gets chunked.\n",
        "                doc_metadata = {\n",
        "                    'ticker': metadata['ticker'],\n",
        "                    'company_name': metadata['company_name'],\n",
        "                    'filing_date': metadata['filing_date'],\n",
        "                    'filing_url': metadata['filing_url'],\n",
        "                    'part': part,\n",
        "                    'item': item\n",
        "                }\n",
        "\n",
        "                doc = Document(\n",
        "                    page_content=content,\n",
        "                    metadata=doc_metadata\n",
        "                )\n",
        "                all_docs.append(doc)\n",
        "\n",
        "        if not all_docs:\n",
        "            return # Stop the generator\n",
        "\n",
        "        # Split all documents at once using the LangChain splitter\n",
        "        print(f\"     → Splitting {len(all_docs)} high-level 'Items' into smaller chunks...\")\n",
        "        chunked_docs = self.text_splitter.split_documents(all_docs)\n",
        "        print(f\"     → Generated {len(chunked_docs)} chunks\")\n",
        "\n",
        "        text_batch = []\n",
        "        metadata_batch = []\n",
        "\n",
        "        # Consume the list one chunk at a time\n",
        "        for chunk in chunked_docs:\n",
        "            text_batch.append(chunk.page_content)\n",
        "            # The splitter automatically copies metadata to each chunk\n",
        "            metadata_batch.append(chunk.metadata)\n",
        "\n",
        "            # If batch is full, process it\n",
        "            if len(text_batch) >= embed_batch_size:\n",
        "                # 1. Embed the entire batch in one call\n",
        "                embeddings = self.model.encode(text_batch, show_progress_bar=False)\n",
        "\n",
        "                # 2. Yield each point from the processed batch\n",
        "                for txt, emb, meta in zip(text_batch, embeddings, metadata_batch):\n",
        "                    # The metadata (meta) already contains everything\n",
        "                    # from the doc_metadata we built above\n",
        "                    payload = {\n",
        "                        'text': txt,\n",
        "                        **meta # Unpack all metadata keys (ticker, item, part, etc.)\n",
        "                    }\n",
        "                    yield PointStruct(\n",
        "                        id=str(uuid.uuid4()),\n",
        "                        vector=emb.tolist(),\n",
        "                        payload=payload\n",
        "                    )\n",
        "\n",
        "                # 3. Reset the batch\n",
        "                text_batch = []\n",
        "                metadata_batch = []\n",
        "\n",
        "        if text_batch:\n",
        "            # 1. Embed the final batch\n",
        "            embeddings = self.model.encode(text_batch, show_progress_bar=False)\n",
        "\n",
        "            # 2. Yield each point\n",
        "            for txt, emb, meta in zip(text_batch, embeddings, metadata_batch):\n",
        "                payload = {\n",
        "                    'text': txt,\n",
        "                    **meta\n",
        "                }\n",
        "                yield PointStruct(\n",
        "                    id=str(uuid.uuid4()),\n",
        "                    vector=emb.tolist(),\n",
        "                    payload=payload\n",
        "                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "80bed17c",
      "metadata": {
        "id": "80bed17c"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PART 4: QDRANT VECTOR DATABASE\n",
        "# ============================================================================\n",
        "\n",
        "from qdrant_client import models\n",
        "\n",
        "class QdrantManager:\n",
        "    \"\"\"Manages Qdrant vector database operations\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize Qdrant client\"\"\"\n",
        "        print(f\"\\nConnecting to Qdrant Cloud...\")\n",
        "        self.client = QdrantClient(\n",
        "            url=Config.QDRANT_URL,\n",
        "            api_key=Config.QDRANT_API_KEY\n",
        "        )\n",
        "        print(f\"   ✓ Connected to Qdrant\")\n",
        "\n",
        "    def create_collection(self, collection_name: str = Config.COLLECTION_NAME,\n",
        "                         vector_size: int = Config.VECTOR_SIZE):\n",
        "        \"\"\"Create or recreate collection AND set up payload indexes\"\"\"\n",
        "        print(f\"\\n Setting up collection: {collection_name}\")\n",
        "\n",
        "        # Check if collection exists\n",
        "        collections = self.client.get_collections().collections\n",
        "        exists = any(col.name == collection_name for col in collections)\n",
        "\n",
        "        if exists:\n",
        "            print(f\"   ⚠ Collection exists, recreating...\")\n",
        "            self.client.delete_collection(collection_name)\n",
        "\n",
        "        # Create collection\n",
        "        self.client.create_collection(\n",
        "            collection_name=collection_name,\n",
        "            vectors_config=models.VectorParams( # Use models.VectorParams\n",
        "                size=vector_size,\n",
        "                distance=models.Distance.COSINE # Use models.Distance\n",
        "            )\n",
        "        )\n",
        "        print(f\"   ✓ Collection created\")\n",
        "\n",
        "        print(f\"   → Creating payload index for 'ticker'...\")\n",
        "        self.client.create_payload_index(\n",
        "            collection_name=collection_name,\n",
        "            field_name=\"ticker\",\n",
        "            field_schema=models.PayloadSchemaType.KEYWORD\n",
        "        )\n",
        "        print(f\"   → Creating payload index for 'item'...\")\n",
        "        self.client.create_payload_index(\n",
        "            collection_name=collection_name,\n",
        "            field_name=\"item\",\n",
        "            field_schema=models.PayloadSchemaType.KEYWORD\n",
        "        )\n",
        "        print(f\"   ✓ Payload indexes created.\")\n",
        "\n",
        "    def upsert_documents(self, points_generator,\n",
        "                        collection_name: str = Config.COLLECTION_NAME,\n",
        "                        batch_size: int = 2048) -> int:\n",
        "        \"\"\"\n",
        "        Uploads document chunks from a generator in batches.\n",
        "\n",
        "        Args:\n",
        "            points_generator: A generator that yields PointStructs\n",
        "            collection_name: Name of the collection\n",
        "            batch_size: Number of points to upload at once\n",
        "\n",
        "        Returns:\n",
        "            Total number of chunks uploaded\n",
        "        \"\"\"\n",
        "        print(f\" Uploading chunks to Qdrant in batches of {batch_size}...\")\n",
        "\n",
        "        batch = []\n",
        "        count = 0\n",
        "\n",
        "        for point in points_generator:\n",
        "            batch.append(point)\n",
        "\n",
        "            if len(batch) >= batch_size:\n",
        "                self.client.upsert(\n",
        "                    collection_name=collection_name,\n",
        "                    points=batch,\n",
        "                    wait=False # Added for speed\n",
        "                )\n",
        "                count += len(batch)\n",
        "                print(f\"     → Uploaded {count} chunks so far...\")\n",
        "                batch = [] # Reset batch\n",
        "\n",
        "        # Upload any remaining points\n",
        "        if batch:\n",
        "            self.client.upsert(\n",
        "                collection_name=collection_name,\n",
        "                points=batch,\n",
        "                wait=False # Added for speed\n",
        "            )\n",
        "            count += len(batch)\n",
        "\n",
        "        print(f\"  ✓ All chunks uploaded for this document. Total: {count}\")\n",
        "        return count\n",
        "\n",
        "    def search(self, query_vector: List[float],\n",
        "              collection_name: str = Config.COLLECTION_NAME,\n",
        "              limit: int = Config.TOP_K,\n",
        "              filter_dict: Dict = None) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Search for similar documents\n",
        "\n",
        "        Args:\n",
        "            query_vector: Embedded query vector\n",
        "            collection_name: Name of the collection\n",
        "            limit: Number of results to return\n",
        "            filter_dict: Optional filter (e.g., {\"ticker\": \"AAPL\"})\n",
        "\n",
        "        Returns:\n",
        "            List of search results with scores and payloads\n",
        "        \"\"\"\n",
        "\n",
        "        qdrant_filter = None\n",
        "        if filter_dict:\n",
        "            qdrant_filter = models.Filter(\n",
        "                must=[\n",
        "                    models.FieldCondition(\n",
        "                        key=key,\n",
        "                        match=models.MatchValue(value=value)\n",
        "                    )\n",
        "                    for key, value in filter_dict.items()\n",
        "                ]\n",
        "            )\n",
        "\n",
        "        results = self.client.search(\n",
        "            collection_name=collection_name,\n",
        "            query_vector=query_vector,\n",
        "            limit=limit,\n",
        "            query_filter=qdrant_filter,\n",
        "            with_payload=True\n",
        "        )\n",
        "\n",
        "        return [\n",
        "            {\n",
        "                'score': result.score,\n",
        "                'payload': result.payload\n",
        "            }\n",
        "            for result in results\n",
        "        ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "8471b72b",
      "metadata": {
        "id": "8471b72b"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PART 5: RAG QUERY PIPELINE\n",
        "# ============================================================================\n",
        "import os\n",
        "from openai import OpenAI\n",
        "from qdrant_client import models\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "\n",
        "class ManualRAGEngine:\n",
        "    \"\"\"\n",
        "    This is the manual RAG query engine that replaces the buggy\n",
        "    LangChainRAGEngine. It performs the RAG steps manually\n",
        "    using the components we've already built.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, document_processor: 'DocumentProcessor', qdrant_manager: 'QdrantManager', query_processor=None):\n",
        "        \"\"\"\n",
        "        Initialize the engine with the processor (for embeddings)\n",
        "        and the manager (for search).\n",
        "        \"\"\"\n",
        "        print(\"\\n Initializing Manual RAG Query Engine...\")\n",
        "\n",
        "        # 1. Get the embedding model from the document processor\n",
        "        #    (the one that is already loaded)\n",
        "        self.embedding_model = document_processor.model\n",
        "        print(\"   ✓ Using existing embedding model from DocumentProcessor\")\n",
        "\n",
        "        # 2. Get the Qdrant client from the Qdrant manager\n",
        "        self.qdrant_manager = qdrant_manager\n",
        "        print(\"   ✓ Using existing QdrantManager for search\")\n",
        "\n",
        "        # 3. Optional QueryProcessor\n",
        "        self.query_processor = query_processor\n",
        "        if self.query_processor is not None:\n",
        "            print(\"   ✓ QueryProcessor attached (query normalization, filters, embeddings)\")\n",
        "        else:\n",
        "            print(\"   No QueryProcessor provided (using legacy behavior)\")\n",
        "\n",
        "        # 4. Initialize the LLM\n",
        "        self.llm = ChatOpenAI(\n",
        "            model=Config.LLM_MODEL,\n",
        "            api_key=Config.OPENAI_API_KEY\n",
        "        )\n",
        "        print(\"   ✓ Initialized ChatOpenAI LLM\")\n",
        "\n",
        "        # 5. Create a prompt template (we can still use this part)\n",
        "        template = \"\"\"You are a helpful financial analyst assistant. Your role is to answer questions about SEC 10-Q filings based ONLY on the provided context.\n",
        "- Base your answer strictly on the provided context from SEC filings\n",
        "- Cite specific sections (e.g., \"According to Item 1A...\") when referencing information\n",
        "- If the answer is not in the context, clearly state that\n",
        "\n",
        "Context:\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "\n",
        "Question: {input}\n",
        "\n",
        "Answer:\"\"\"\n",
        "        self.prompt = ChatPromptTemplate.from_template(template)\n",
        "        print(\"   ✓ Manual RAG Engine ready.\")\n",
        "\n",
        "\n",
        "    def _format_context(self, search_results: List[Dict]) -> str:\n",
        "        \"\"\"Helper function to format the retrieved contexts\"\"\"\n",
        "        context_str = \"\"\n",
        "        for i, result in enumerate(search_results, 1):\n",
        "            payload = result.get('payload', {})\n",
        "            text = payload.get('text', 'No text found')\n",
        "            item = payload.get('item', 'N/A')\n",
        "            ticker = payload.get('ticker', 'N/A')\n",
        "            context_str += f\"Source {i} ({ticker} - {item}):\\n\\\"{text}\\\"\\n\\n\"\n",
        "        return context_str.strip()\n",
        "\n",
        "\n",
        "    def query(self, question: str, ticker_filter: str = None):\n",
        "        \"\"\"\n",
        "        Query the indexed filings using the manual retrieval and generation.\n",
        "        \"\"\"\n",
        "        print(f\"\\n Processing query with Manual Engine: '{question}'\")\n",
        "        # 1. Manually embed the query\n",
        "        print(\"   → Manually embedding query...\")\n",
        "        # --- Query processing first (label, filters, paraphrases, embedding) ---\n",
        "        if self.query_processor is not None:\n",
        "            qobj = self.query_processor.process(question)\n",
        "            # Optional: small trace to console\n",
        "            print(f\"→ Query label: {qobj['label']} | Filters: {qobj['filters']}\")\n",
        "\n",
        "            # Prefer normalized text & the processor’s embedding (model matches your config: all-MiniLM-L6-v2)\n",
        "            query_text = qobj[\"normalized\"]\n",
        "            query_vector = qobj[\"embedding\"] if qobj[\"embedding\"] is not None else self.embedding_model.encode(query_text)\n",
        "\n",
        "            # Build a search filter (your QdrantManager supports 'ticker' today)\n",
        "            filter_dict = None\n",
        "            if qobj[\"filters\"].get(\"ticker\"):\n",
        "                # Use first detected ticker; you can expand to MatchAny later\n",
        "                filter_dict = {\"ticker\": qobj[\"filters\"][\"ticker\"][0]}\n",
        "            elif qobj[\"filters\"].get(\"company\"):\n",
        "                # Map company → ticker via the module’s map (only if no explicit ticker provided)\n",
        "                from query_processor import COMPANY_TICKERS  # same module\n",
        "                comp = qobj[\"filters\"][\"company\"][0].lower()\n",
        "                tkr = COMPANY_TICKERS.get(comp)\n",
        "                if tkr:\n",
        "                    filter_dict = {\"ticker\": tkr}\n",
        "        else:\n",
        "            # Fallback: legacy behavior\n",
        "            query_text = question\n",
        "            query_vector = self.embedding_model.encode(question)\n",
        "            filter_dict = None\n",
        "\n",
        "        # Explicit user override always wins:\n",
        "        if ticker_filter:\n",
        "            print(f\"→ Overriding with explicit ticker filter: {ticker_filter}\")\n",
        "            filter_dict = {\"ticker\": ticker_filter}\n",
        "\n",
        "\n",
        "        # 3. Manually search Qdrant\n",
        "        print(\"   → Manually searching Qdrant...\")\n",
        "        search_results = self.qdrant_manager.search(\n",
        "            query_vector=query_vector,\n",
        "            limit=Config.TOP_K,\n",
        "            filter_dict=filter_dict\n",
        "        )\n",
        "\n",
        "        if not search_results:\n",
        "            return {'answer': 'No relevant context was found in the documents to answer this question.', 'sources': []}\n",
        "\n",
        "        # 4. Manually format the prompt\n",
        "        print(\"   → Formatting context and building prompt...\")\n",
        "        formatted_context = self._format_context(search_results)\n",
        "\n",
        "        # We use the prompt template to create the final message\n",
        "        final_prompt_message = self.prompt.format_messages(\n",
        "            context=formatted_context,\n",
        "            input=question\n",
        "        )\n",
        "\n",
        "        # 5. Manually invoke the LLM\n",
        "        print(\"   → Sending prompt to LLM...\")\n",
        "        llm_response = self.llm.invoke(final_prompt_message)\n",
        "        answer = llm_response.content\n",
        "\n",
        "        # 6. Format sources to match the expected output\n",
        "        sources = []\n",
        "        for i, result in enumerate(search_results, 1):\n",
        "            sources.append({\n",
        "                'ticker': result['payload'].get('ticker'),\n",
        "                'company': result['payload'].get('company_name'),\n",
        "                'item': result['payload'].get('item'),\n",
        "                'part': result['payload'].get('part'),\n",
        "                'filing_date': result['payload'].get('filing_date'),\n",
        "                'score': result['score'] # We get the real score now\n",
        "            })\n",
        "\n",
        "        return {\n",
        "            'answer': answer,\n",
        "            'sources': sources\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "9b12c056",
      "metadata": {
        "id": "9b12c056"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PART 6: MAIN PIPELINE ORCHESTRATOR\n",
        "# ============================================================================\n",
        "\n",
        "class SECFilingRAGPipeline:\n",
        "    \"\"\"Main pipeline orchestrator\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize all components\"\"\"\n",
        "        print(\"=\" * 70)\n",
        "        print(\"SEC 10-Q FILING RAG SYSTEM\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        # Ingestion components (these are working perfectly)\n",
        "        self.loader = SECDocumentLoader()\n",
        "        self.processor = DocumentProcessor()\n",
        "        self.qdrant_manager = QdrantManager()\n",
        "        #query processing\n",
        "        self.query_processor= QueryProcessor()\n",
        "        # Query component (to be initialized later)\n",
        "        self.query_engine = None\n",
        "\n",
        "\n",
        "    def load_and_index_filings(self, tickers: List[str] = Config.TICKERS, num_filings_per_ticker: int = 1):\n",
        "        \"\"\"\n",
        "        Load and index 10-Q filings for multiple companies.\n",
        "        (This method is UNCHANGED and works)\n",
        "        \"\"\"\n",
        "        print(f\"\\n{'=' * 70}\")\n",
        "        print(f\"LOADING & INDEXING PHASE (Streaming)\")\n",
        "        print(f\"{'=' * 70}\")\n",
        "        print(f\"\\nProcessing {len(tickers)} companies: {', '.join(tickers)}\")\n",
        "        print(f\"(Fetching {num_filings_per_ticker} filings per company)\\n\")\n",
        "\n",
        "        # Create collection (do this once at the start)\n",
        "        self.qdrant_manager.create_collection() # Use the manager here\n",
        "\n",
        "        successful_tickers = []\n",
        "        failed_tickers = []\n",
        "        total_chunks_indexed = 0\n",
        "\n",
        "        # Process each ticker\n",
        "        for idx, ticker in enumerate( tickers, 1):\n",
        "            print(f\"\\n[{idx}/{len(tickers)}] Processing {ticker}\")\n",
        "            print(\"-\" * 70)\n",
        "\n",
        "            ticker_chunks_count = 0\n",
        "            num_filings_processed = 0\n",
        "\n",
        "            try:\n",
        "                # 1. Get METADATA (links) for N filings\n",
        "                filings_metadata_list = self.loader.get_recent_10q_metadata(ticker, num_filings=num_filings_per_ticker)\n",
        "\n",
        "                # 2. Loop through each FILING METADATA\n",
        "                for filing_metadata in filings_metadata_list:\n",
        "                    try:\n",
        "                        filing_date = filing_metadata['filing_date']\n",
        "                        filing_url = filing_metadata['filing_url']\n",
        "\n",
        "                        # 3. Download ONE filing's HTML\n",
        "                        print(f\"  → Downloading filing from: {filing_date}...\")\n",
        "                        html_content = self.loader.get_filing_html(filing_url)\n",
        "\n",
        "                        # 4. Parse ONE filing\n",
        "                        print(f\"  → Parsing 10-Q structure...\")\n",
        "                        parsed_data = self.loader.parse_10q(html_content)\n",
        "\n",
        "                        if not parsed_data:\n",
        "                            print(f\"  ⚠ Warning: No structured data parsed for {ticker} on {filing_date}\")\n",
        "                            del html_content\n",
        "                            gc.collect()\n",
        "                            continue\n",
        "\n",
        "                        # 5. Process ONE filing into a GENERATOR\n",
        "                        print(f\"  → Creating chunks and embeddings generator...\")\n",
        "                        # Use the LANGCHAIN-ENHANCED chunker from Part 3\n",
        "                        chunks_generator = self.processor.generate_document_chunks(parsed_data, filing_metadata)\n",
        "\n",
        "                        # 6. Upload ONE filing's chunks from the generator\n",
        "                        num_uploaded = self.qdrant_manager.upsert_documents(chunks_generator) # Use the manager here\n",
        "\n",
        "                        if num_uploaded > 0:\n",
        "                            ticker_chunks_count += num_uploaded\n",
        "                            total_chunks_indexed += num_uploaded\n",
        "                            num_filings_processed += 1\n",
        "\n",
        "                        # 7. MANUALLY CLEAN UP MEMORY\n",
        "                        print(f\"  → Cleaning up memory...\")\n",
        "                        del html_content\n",
        "                        del parsed_data\n",
        "                        del chunks_generator # Clear generator\n",
        "                        gc.collect() # Force garbage collection\n",
        "                        print(f\"  ✓ Memory cleaned.\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"  ✗ Error processing filing {filing_metadata.get('filing_date', 'unknown')} for {ticker}: {str(e)}\")\n",
        "                        gc.collect() # Force cleanup on error\n",
        "\n",
        "                # After all filings for this ticker are done\n",
        "                if ticker_chunks_count > 0:\n",
        "                    successful_tickers.append(ticker)\n",
        "                    print(f\"  ✓ Finished {ticker}. Total chunks: {ticker_chunks_count} across {num_filings_processed} filings\")\n",
        "                else:\n",
        "                    failed_tickers.append(ticker)\n",
        "                    print(f\"  ⚠ No chunks created for {ticker}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  ✗ Error processing {ticker} (failed to get metadata): {str(e)}\")\n",
        "                failed_tickers.append(ticker)\n",
        "\n",
        "            # Rate limiting\n",
        "            if idx < len(tickers):\n",
        "                time.sleep(0.2)\n",
        "\n",
        "        # Summary\n",
        "        print(f\"\\n{'=' * 70}\")\n",
        "        print(f\"INDEXING COMPLETE\")\n",
        "        print(f\"{'=' * 70}\")\n",
        "        print(f\"✓ Successfully processed: {len(successful_tickers)} companies\")\n",
        "        print(f\"  {', '.join(successful_tickers)}\")\n",
        "        if failed_tickers:\n",
        "            print(f\"✗ Failed: {len(failed_tickers)} companies\")\n",
        "            print(f\"  {', '.join(failed_tickers)}\")\n",
        "        print(f\"\\n Total chunks indexed: {total_chunks_indexed}\")\n",
        "        print(f\"{'=' * 70}\\n\")\n",
        "\n",
        "    def query(self, question: str, ticker_filter: str = None):\n",
        "        \"\"\"\n",
        "        Query the indexed filings using the new MANUAL engine\n",
        "        \"\"\"\n",
        "        if self.query_engine is None:\n",
        "            # Initialize the ManualRAGEngine, passing it the\n",
        "            # processor (for the model) and manager (for search)\n",
        "            self.query_engine = ManualRAGEngine(\n",
        "                document_processor=self.processor,\n",
        "                qdrant_manager=self.qdrant_manager,\n",
        "                query_processor=self.query_processor\n",
        "            )\n",
        "\n",
        "        result = self.query_engine.query(question, ticker_filter)\n",
        "\n",
        "        # Print results (this part is the same)\n",
        "        print(f\"\\n{'=' * 70}\")\n",
        "        print(f\"ANSWER\")\n",
        "        print(f\"{'=' * 70}\")\n",
        "        print(f\"\\n{result['answer']}\\n\")\n",
        "\n",
        "        print(f\"{'=' * 70}\")\n",
        "        print(f\"SOURCES ({len(result['sources'])} chunks)\")\n",
        "        print(f\"{'=' * 70}\")\n",
        "        for i, source in enumerate(result['sources'], 1):\n",
        "            print(f\"\\n{i}. {source['company']} ({source['ticker']}) - {source['item']}\")\n",
        "            print(f\"   Filing Date: {source['filing_date']}\")\n",
        "            print(f\"   Relevance Score: {source['score']:.4f}\") # Can now show the real score\n",
        "\n",
        "        print(f\"\\n{'=' * 70}\\\\n\")\n",
        "\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "7fb9524b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fb9524b",
        "outputId": "fdf56be6-a020-4fd5-ef89-e75b8f33330d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "SEC 10-Q FILING RAG SYSTEM\n",
            "======================================================================\n",
            "\n",
            " Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
            "   ✓ Model loaded (dimension: 384)\n",
            "   ✓ Initialized RecursiveCharacterTextSplitter (chunk: 800, overlap: 200)\n",
            "\n",
            "Connecting to Qdrant Cloud...\n",
            "   ✓ Connected to Qdrant\n",
            "\n",
            "======================================================================\n",
            "LOADING & INDEXING PHASE (Streaming)\n",
            "======================================================================\n",
            "\n",
            "Processing 10 companies: NVDA, AAPL, MSFT, AMZN, META, GOOGL, TSLA, ORCL, JPM, AMD\n",
            "(Fetching 4 filings per company)\n",
            "\n",
            "\n",
            " Setting up collection: sec_filings_10q\n",
            "   ⚠ Collection exists, recreating...\n",
            "   ✓ Collection created\n",
            "   → Creating payload index for 'ticker'...\n",
            "   → Creating payload index for 'item'...\n",
            "   ✓ Payload indexes created.\n",
            "\n",
            "[1/10] Processing NVDA\n",
            "----------------------------------------------------------------------\n",
            "  → Fetching CIK for ticker: NVDA...\n",
            "  → Found CIK: 0001045810 (NVIDIA CORP)\n",
            "  → Found 4 recent 10-Q filing metadata entries.\n",
            "  → Downloading filing from: 2025-08-27...\n",
            "  → Parsing 10-Q structure...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2600233645.py:140: XMLParsedAsHTMLWarning: It looks like you're using an HTML parser to parse an XML document.\n",
            "\n",
            "Assuming this really is an XML document, what you're doing might work, but you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the Python package 'lxml' installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
            "\n",
            "If you want or need to use an HTML parser on this document, you can make this warning go away by filtering it. To do that, run this code before calling the BeautifulSoup constructor:\n",
            "\n",
            "    from bs4 import XMLParsedAsHTMLWarning\n",
            "    import warnings\n",
            "\n",
            "    warnings.filterwarnings(\"ignore\", category=XMLParsedAsHTMLWarning)\n",
            "\n",
            "  soup = BeautifulSoup(html_content, 'lxml')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  → Creating chunks and embeddings generator...\n",
            " Uploading chunks to Qdrant in batches of 2048...\n",
            "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
            "     → Generated 245 chunks\n",
            "  ✓ All chunks uploaded for this document. Total: 245\n",
            "  → Cleaning up memory...\n",
            "  ✓ Memory cleaned.\n",
            "  → Downloading filing from: 2025-05-28...\n",
            "  → Parsing 10-Q structure...\n",
            "  → Creating chunks and embeddings generator...\n",
            " Uploading chunks to Qdrant in batches of 2048...\n",
            "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
            "     → Generated 245 chunks\n",
            "  ✓ All chunks uploaded for this document. Total: 245\n",
            "  → Cleaning up memory...\n",
            "  ✓ Memory cleaned.\n",
            "  → Downloading filing from: 2024-11-20...\n",
            "  → Parsing 10-Q structure...\n",
            "  → Creating chunks and embeddings generator...\n",
            " Uploading chunks to Qdrant in batches of 2048...\n",
            "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
            "     → Generated 257 chunks\n",
            "  ✓ All chunks uploaded for this document. Total: 257\n",
            "  → Cleaning up memory...\n",
            "  ✓ Memory cleaned.\n",
            "  → Downloading filing from: 2024-08-28...\n",
            "  → Parsing 10-Q structure...\n",
            "  → Creating chunks and embeddings generator...\n",
            " Uploading chunks to Qdrant in batches of 2048...\n",
            "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
            "     → Generated 271 chunks\n",
            "  ✓ All chunks uploaded for this document. Total: 271\n",
            "  → Cleaning up memory...\n",
            "  ✓ Memory cleaned.\n",
            "  ✓ Finished NVDA. Total chunks: 1018 across 4 filings\n",
            "\n",
            "[2/10] Processing AAPL\n",
            "----------------------------------------------------------------------\n",
            "  → Fetching CIK for ticker: AAPL...\n",
            "  → Found CIK: 0000320193 (Apple Inc.)\n",
            "  → Found 4 recent 10-Q filing metadata entries.\n",
            "  → Downloading filing from: 2025-08-01...\n",
            "  → Parsing 10-Q structure...\n",
            "  → Creating chunks and embeddings generator...\n",
            " Uploading chunks to Qdrant in batches of 2048...\n",
            "     → Splitting 11 high-level 'Items' into smaller chunks...\n",
            "     → Generated 116 chunks\n",
            "  ✓ All chunks uploaded for this document. Total: 116\n",
            "  → Cleaning up memory...\n",
            "  ✓ Memory cleaned.\n",
            "  → Downloading filing from: 2025-05-02...\n",
            "  → Parsing 10-Q structure...\n",
            "  → Creating chunks and embeddings generator...\n",
            " Uploading chunks to Qdrant in batches of 2048...\n",
            "     → Splitting 11 high-level 'Items' into smaller chunks...\n",
            "     → Generated 136 chunks\n",
            "  ✓ All chunks uploaded for this document. Total: 136\n",
            "  → Cleaning up memory...\n",
            "  ✓ Memory cleaned.\n",
            "  → Downloading filing from: 2025-01-31...\n",
            "  → Parsing 10-Q structure...\n",
            "  → Creating chunks and embeddings generator...\n",
            " Uploading chunks to Qdrant in batches of 2048...\n",
            "     → Splitting 11 high-level 'Items' into smaller chunks...\n",
            "     → Generated 123 chunks\n",
            "  ✓ All chunks uploaded for this document. Total: 123\n",
            "  → Cleaning up memory...\n",
            "  ✓ Memory cleaned.\n",
            "  → Downloading filing from: 2024-08-02...\n",
            "  → Parsing 10-Q structure...\n",
            "  → Creating chunks and embeddings generator...\n",
            " Uploading chunks to Qdrant in batches of 2048...\n",
            "     → Splitting 11 high-level 'Items' into smaller chunks...\n",
            "     → Generated 111 chunks\n",
            "  ✓ All chunks uploaded for this document. Total: 111\n",
            "  → Cleaning up memory...\n",
            "  ✓ Memory cleaned.\n",
            "  ✓ Finished AAPL. Total chunks: 486 across 4 filings\n",
            "\n",
            "[3/10] Processing MSFT\n",
            "----------------------------------------------------------------------\n",
            "  → Fetching CIK for ticker: MSFT...\n",
            "  → Found CIK: 0000789019 (MICROSOFT CORP)\n",
            "  → Found 4 recent 10-Q filing metadata entries.\n",
            "  → Downloading filing from: 2025-10-29...\n",
            "  → Parsing 10-Q structure...\n",
            "  → Creating chunks and embeddings generator...\n",
            " Uploading chunks to Qdrant in batches of 2048...\n",
            "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
            "     → Generated 31 chunks\n",
            "  ✓ All chunks uploaded for this document. Total: 31\n",
            "  → Cleaning up memory...\n",
            "  ✓ Memory cleaned.\n",
            "  → Downloading filing from: 2025-04-30...\n",
            "  → Parsing 10-Q structure...\n",
            "  → Creating chunks and embeddings generator...\n",
            " Uploading chunks to Qdrant in batches of 2048...\n",
            "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
            "     → Generated 42 chunks\n",
            "  ✓ All chunks uploaded for this document. Total: 42\n",
            "  → Cleaning up memory...\n",
            "  ✓ Memory cleaned.\n",
            "  → Downloading filing from: 2025-01-29...\n",
            "  → Parsing 10-Q structure...\n",
            "  → Creating chunks and embeddings generator...\n",
            " Uploading chunks to Qdrant in batches of 2048...\n",
            "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
            "     → Generated 36 chunks\n",
            "  ✓ All chunks uploaded for this document. Total: 36\n",
            "  → Cleaning up memory...\n",
            "  ✓ Memory cleaned.\n",
            "  → Downloading filing from: 2024-10-30...\n",
            "  → Parsing 10-Q structure...\n",
            "  → Creating chunks and embeddings generator...\n",
            " Uploading chunks to Qdrant in batches of 2048...\n",
            "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
            "     → Generated 37 chunks\n",
            "  ✓ All chunks uploaded for this document. Total: 37\n",
            "  → Cleaning up memory...\n",
            "  ✓ Memory cleaned.\n",
            "  ✓ Finished MSFT. Total chunks: 146 across 4 filings\n",
            "\n",
            "[4/10] Processing AMZN\n",
            "----------------------------------------------------------------------\n",
            "  → Fetching CIK for ticker: AMZN...\n",
            "  → Found CIK: 0001018724 (AMAZON COM INC)\n",
            "  → Found 4 recent 10-Q filing metadata entries.\n",
            "  → Downloading filing from: 2025-10-31...\n",
            "  → Parsing 10-Q structure...\n",
            "  → Creating chunks and embeddings generator...\n",
            " Uploading chunks to Qdrant in batches of 2048...\n",
            "     → Splitting 11 high-level 'Items' into smaller chunks...\n",
            "     → Generated 352 chunks\n",
            "  ✓ All chunks uploaded for this document. Total: 352\n",
            "  → Cleaning up memory...\n",
            "  ✓ Memory cleaned.\n",
            "  → Downloading filing from: 2025-08-01...\n",
            "  → Parsing 10-Q structure...\n",
            "  → Creating chunks and embeddings generator...\n",
            " Uploading chunks to Qdrant in batches of 2048...\n",
            "     → Splitting 11 high-level 'Items' into smaller chunks...\n",
            "     → Generated 341 chunks\n",
            "  ✓ All chunks uploaded for this document. Total: 341\n",
            "  → Cleaning up memory...\n",
            "  ✓ Memory cleaned.\n",
            "  → Downloading filing from: 2025-05-02...\n",
            "  → Parsing 10-Q structure...\n",
            "  → Creating chunks and embeddings generator...\n",
            " Uploading chunks to Qdrant in batches of 2048...\n",
            "     → Splitting 11 high-level 'Items' into smaller chunks...\n",
            "     → Generated 324 chunks\n",
            "  ✓ All chunks uploaded for this document. Total: 324\n",
            "  → Cleaning up memory...\n",
            "  ✓ Memory cleaned.\n",
            "  → Downloading filing from: 2024-11-01...\n",
            "  → Parsing 10-Q structure...\n",
            "  → Creating chunks and embeddings generator...\n",
            " Uploading chunks to Qdrant in batches of 2048...\n",
            "     → Splitting 11 high-level 'Items' into smaller chunks...\n",
            "     → Generated 357 chunks\n",
            "  ✓ All chunks uploaded for this document. Total: 357\n",
            "  → Cleaning up memory...\n",
            "  ✓ Memory cleaned.\n",
            "  ✓ Finished AMZN. Total chunks: 1374 across 4 filings\n",
            "\n",
            "[5/10] Processing META\n",
            "----------------------------------------------------------------------\n",
            "  → Fetching CIK for ticker: META...\n",
            "  → Found CIK: 0001326801 (Meta Platforms, Inc.)\n",
            "  → Found 4 recent 10-Q filing metadata entries.\n",
            "  → Downloading filing from: 2025-10-30...\n",
            "  → Parsing 10-Q structure...\n",
            "  → Creating chunks and embeddings generator...\n",
            " Uploading chunks to Qdrant in batches of 2048...\n",
            "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
            "     → Generated 704 chunks\n",
            "  ✓ All chunks uploaded for this document. Total: 704\n",
            "  → Cleaning up memory...\n",
            "  ✓ Memory cleaned.\n",
            "  → Downloading filing from: 2025-07-31...\n",
            "  → Parsing 10-Q structure...\n",
            "  → Creating chunks and embeddings generator...\n",
            " Uploading chunks to Qdrant in batches of 2048...\n",
            "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
            "     → Generated 675 chunks\n",
            "  ✓ All chunks uploaded for this document. Total: 675\n",
            "  → Cleaning up memory...\n",
            "  ✓ Memory cleaned.\n",
            "  → Downloading filing from: 2025-05-01...\n",
            "  → Parsing 10-Q structure...\n",
            "  → Creating chunks and embeddings generator...\n",
            " Uploading chunks to Qdrant in batches of 2048...\n",
            "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
            "     → Generated 646 chunks\n",
            "  ✓ All chunks uploaded for this document. Total: 646\n",
            "  → Cleaning up memory...\n",
            "  ✓ Memory cleaned.\n",
            "  → Downloading filing from: 2024-10-31...\n",
            "  → Parsing 10-Q structure...\n",
            "  → Creating chunks and embeddings generator...\n",
            " Uploading chunks to Qdrant in batches of 2048...\n",
            "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
            "     → Generated 663 chunks\n",
            "  ✓ All chunks uploaded for this document. Total: 663\n",
            "  → Cleaning up memory...\n",
            "  ✓ Memory cleaned.\n",
            "  ✓ Finished META. Total chunks: 2688 across 4 filings\n",
            "\n",
            "[6/10] Processing GOOGL\n",
            "----------------------------------------------------------------------\n",
            "  → Fetching CIK for ticker: GOOGL...\n",
            "  → Found CIK: 0001652044 (Alphabet Inc.)\n",
            "  → Found 4 recent 10-Q filing metadata entries.\n",
            "  → Downloading filing from: 2025-10-30...\n",
            "  → Parsing 10-Q structure...\n",
            "  → Creating chunks and embeddings generator...\n",
            " Uploading chunks to Qdrant in batches of 2048...\n",
            "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
            "     → Generated 277 chunks\n",
            "  ✓ All chunks uploaded for this document. Total: 277\n",
            "  → Cleaning up memory...\n",
            "  ✓ Memory cleaned.\n",
            "  → Downloading filing from: 2025-07-24...\n",
            "  → Parsing 10-Q structure...\n",
            "  → Creating chunks and embeddings generator...\n",
            " Uploading chunks to Qdrant in batches of 2048...\n",
            "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
            "     → Generated 274 chunks\n",
            "  ✓ All chunks uploaded for this document. Total: 274\n",
            "  → Cleaning up memory...\n",
            "  ✓ Memory cleaned.\n",
            "  → Downloading filing from: 2025-04-25...\n",
            "  → Parsing 10-Q structure...\n",
            "  → Creating chunks and embeddings generator...\n",
            " Uploading chunks to Qdrant in batches of 2048...\n",
            "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
            "     → Generated 244 chunks\n",
            "  ✓ All chunks uploaded for this document. Total: 244\n",
            "  → Cleaning up memory...\n",
            "  ✓ Memory cleaned.\n",
            "  → Downloading filing from: 2024-10-30...\n",
            "  → Parsing 10-Q structure...\n",
            "  → Creating chunks and embeddings generator...\n",
            " Uploading chunks to Qdrant in batches of 2048...\n",
            "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
            "     → Generated 274 chunks\n",
            "  ✓ All chunks uploaded for this document. Total: 274\n",
            "  → Cleaning up memory...\n",
            "  ✓ Memory cleaned.\n",
            "  ✓ Finished GOOGL. Total chunks: 1069 across 4 filings\n",
            "\n",
            "[7/10] Processing TSLA\n",
            "----------------------------------------------------------------------\n",
            "  → Fetching CIK for ticker: TSLA...\n",
            "  → Found CIK: 0001318605 (Tesla, Inc.)\n",
            "  → Found 4 recent 10-Q filing metadata entries.\n",
            "  → Downloading filing from: 2025-10-23...\n",
            "  → Parsing 10-Q structure...\n",
            "  → Creating chunks and embeddings generator...\n",
            " Uploading chunks to Qdrant in batches of 2048...\n",
            "     → Splitting 11 high-level 'Items' into smaller chunks...\n",
            "     → Generated 247 chunks\n",
            "  ✓ All chunks uploaded for this document. Total: 247\n",
            "  → Cleaning up memory...\n",
            "  ✓ Memory cleaned.\n",
            "  → Downloading filing from: 2025-07-24...\n",
            "  → Parsing 10-Q structure...\n",
            "  → Creating chunks and embeddings generator...\n",
            " Uploading chunks to Qdrant in batches of 2048...\n",
            "     → Splitting 11 high-level 'Items' into smaller chunks...\n",
            "     → Generated 226 chunks\n",
            "  ✓ All chunks uploaded for this document. Total: 226\n",
            "  → Cleaning up memory...\n",
            "  ✓ Memory cleaned.\n",
            "  → Downloading filing from: 2025-04-23...\n",
            "  → Parsing 10-Q structure...\n",
            "  → Creating chunks and embeddings generator...\n",
            " Uploading chunks to Qdrant in batches of 2048...\n",
            "     → Splitting 11 high-level 'Items' into smaller chunks...\n",
            "     → Generated 210 chunks\n",
            "  ✓ All chunks uploaded for this document. Total: 210\n",
            "  → Cleaning up memory...\n",
            "  ✓ Memory cleaned.\n",
            "  → Downloading filing from: 2024-10-24...\n",
            "  → Parsing 10-Q structure...\n",
            "  → Creating chunks and embeddings generator...\n",
            " Uploading chunks to Qdrant in batches of 2048...\n",
            "     → Splitting 11 high-level 'Items' into smaller chunks...\n",
            "     → Generated 225 chunks\n",
            "  ✓ All chunks uploaded for this document. Total: 225\n",
            "  → Cleaning up memory...\n",
            "  ✓ Memory cleaned.\n",
            "  ✓ Finished TSLA. Total chunks: 908 across 4 filings\n",
            "\n",
            "[8/10] Processing ORCL\n",
            "----------------------------------------------------------------------\n",
            "  → Fetching CIK for ticker: ORCL...\n",
            "  → Found CIK: 0001341439 (ORACLE CORP)\n",
            "  → Found 4 recent 10-Q filing metadata entries.\n",
            "  → Downloading filing from: 2025-09-10...\n",
            "  → Parsing 10-Q structure...\n",
            "  → Creating chunks and embeddings generator...\n",
            " Uploading chunks to Qdrant in batches of 2048...\n",
            "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
            "     → Generated 260 chunks\n",
            "  ✓ All chunks uploaded for this document. Total: 260\n",
            "  → Cleaning up memory...\n",
            "  ✓ Memory cleaned.\n",
            "  → Downloading filing from: 2025-03-11...\n",
            "  → Parsing 10-Q structure...\n",
            "  → Creating chunks and embeddings generator...\n",
            " Uploading chunks to Qdrant in batches of 2048...\n",
            "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
            "     → Generated 280 chunks\n",
            "  ✓ All chunks uploaded for this document. Total: 280\n",
            "  → Cleaning up memory...\n",
            "  ✓ Memory cleaned.\n",
            "  → Downloading filing from: 2024-12-10...\n",
            "  → Parsing 10-Q structure...\n",
            "  → Creating chunks and embeddings generator...\n",
            " Uploading chunks to Qdrant in batches of 2048...\n",
            "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
            "     → Generated 284 chunks\n",
            "  ✓ All chunks uploaded for this document. Total: 284\n",
            "  → Cleaning up memory...\n",
            "  ✓ Memory cleaned.\n",
            "  → Downloading filing from: 2024-09-10...\n",
            "  → Parsing 10-Q structure...\n",
            "  → Creating chunks and embeddings generator...\n",
            " Uploading chunks to Qdrant in batches of 2048...\n",
            "     → Splitting 8 high-level 'Items' into smaller chunks...\n",
            "     → Generated 258 chunks\n",
            "  ✓ All chunks uploaded for this document. Total: 258\n",
            "  → Cleaning up memory...\n",
            "  ✓ Memory cleaned.\n",
            "  ✓ Finished ORCL. Total chunks: 1082 across 4 filings\n",
            "\n",
            "[9/10] Processing JPM\n",
            "----------------------------------------------------------------------\n",
            "  → Fetching CIK for ticker: JPM...\n",
            "  → Found CIK: 0000019617 (JPMORGAN CHASE & CO)\n",
            "  → Found 3 recent 10-Q filing metadata entries.\n",
            "  → Downloading filing from: 2025-11-04...\n",
            "  → Parsing 10-Q structure...\n",
            "  → Creating chunks and embeddings generator...\n",
            " Uploading chunks to Qdrant in batches of 2048...\n",
            "     → Splitting 8 high-level 'Items' into smaller chunks...\n",
            "     → Generated 31 chunks\n",
            "  ✓ All chunks uploaded for this document. Total: 31\n",
            "  → Cleaning up memory...\n",
            "  ✓ Memory cleaned.\n",
            "  → Downloading filing from: 2025-08-05...\n",
            "  → Parsing 10-Q structure...\n",
            "  → Creating chunks and embeddings generator...\n",
            " Uploading chunks to Qdrant in batches of 2048...\n",
            "     → Splitting 8 high-level 'Items' into smaller chunks...\n",
            "     → Generated 23 chunks\n",
            "  ✓ All chunks uploaded for this document. Total: 23\n",
            "  → Cleaning up memory...\n",
            "  ✓ Memory cleaned.\n",
            "  → Downloading filing from: 2025-05-01...\n",
            "  → Parsing 10-Q structure...\n",
            "  → Creating chunks and embeddings generator...\n",
            " Uploading chunks to Qdrant in batches of 2048...\n",
            "     → Splitting 8 high-level 'Items' into smaller chunks...\n",
            "     → Generated 24 chunks\n",
            "  ✓ All chunks uploaded for this document. Total: 24\n",
            "  → Cleaning up memory...\n",
            "  ✓ Memory cleaned.\n",
            "  ✓ Finished JPM. Total chunks: 78 across 3 filings\n",
            "\n",
            "[10/10] Processing AMD\n",
            "----------------------------------------------------------------------\n",
            "  → Fetching CIK for ticker: AMD...\n",
            "  → Found CIK: 0000002488 (ADVANCED MICRO DEVICES INC)\n",
            "  → Found 4 recent 10-Q filing metadata entries.\n",
            "  → Downloading filing from: 2025-11-05...\n",
            "  → Parsing 10-Q structure...\n",
            "  → Creating chunks and embeddings generator...\n",
            " Uploading chunks to Qdrant in batches of 2048...\n",
            "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
            "     → Generated 430 chunks\n",
            "  ✓ All chunks uploaded for this document. Total: 430\n",
            "  → Cleaning up memory...\n",
            "  ✓ Memory cleaned.\n",
            "  → Downloading filing from: 2025-08-06...\n",
            "  → Parsing 10-Q structure...\n",
            "  → Creating chunks and embeddings generator...\n",
            " Uploading chunks to Qdrant in batches of 2048...\n",
            "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
            "     → Generated 422 chunks\n",
            "  ✓ All chunks uploaded for this document. Total: 422\n",
            "  → Cleaning up memory...\n",
            "  ✓ Memory cleaned.\n",
            "  → Downloading filing from: 2025-05-07...\n",
            "  → Parsing 10-Q structure...\n",
            "  → Creating chunks and embeddings generator...\n",
            " Uploading chunks to Qdrant in batches of 2048...\n",
            "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
            "     → Generated 390 chunks\n",
            "  ✓ All chunks uploaded for this document. Total: 390\n",
            "  → Cleaning up memory...\n",
            "  ✓ Memory cleaned.\n",
            "  → Downloading filing from: 2024-10-30...\n",
            "  → Parsing 10-Q structure...\n",
            "  → Creating chunks and embeddings generator...\n",
            " Uploading chunks to Qdrant in batches of 2048...\n",
            "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
            "     → Generated 392 chunks\n",
            "  ✓ All chunks uploaded for this document. Total: 392\n",
            "  → Cleaning up memory...\n",
            "  ✓ Memory cleaned.\n",
            "  ✓ Finished AMD. Total chunks: 1634 across 4 filings\n",
            "\n",
            "======================================================================\n",
            "INDEXING COMPLETE\n",
            "======================================================================\n",
            "✓ Successfully processed: 10 companies\n",
            "  NVDA, AAPL, MSFT, AMZN, META, GOOGL, TSLA, ORCL, JPM, AMD\n",
            "\n",
            " Total chunks indexed: 10483\n",
            "======================================================================\n",
            "\n",
            "\n",
            " Initializing Manual RAG Query Engine...\n",
            "   ✓ Using existing embedding model from DocumentProcessor\n",
            "   ✓ Using existing QdrantManager for search\n",
            "   ✓ QueryProcessor attached (query normalization, filters, embeddings)\n",
            "   ✓ Initialized ChatOpenAI LLM\n",
            "   ✓ Manual RAG Engine ready.\n",
            "\n",
            " Processing query with Manual Engine: 'What are the main risk factors mentioned by tech companies?'\n",
            "   → Manually embedding query...\n",
            "→ Query label: risk | Filters: {}\n",
            "   → Manually searching Qdrant...\n",
            "   → Formatting context and building prompt...\n",
            "   → Sending prompt to LLM...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-4230959845.py:129: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
            "  results = self.client.search(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "ANSWER\n",
            "======================================================================\n",
            "\n",
            "According to the provided context from AMD's SEC 10-Q filing, the main risk factors mentioned include:\n",
            "\n",
            "1. **Economic and Strategic Risks**:\n",
            "   - Intel Corporation's dominance of the microprocessor market and its aggressive business practices, which may limit AMD's ability to compete effectively (Source 1, Source 2, Source 5).\n",
            "   - The markets in which AMD's products are sold being highly competitive and rapidly evolving (Source 3, Source 4).\n",
            "   - The semiconductor industry's cyclical nature, having experienced severe downturns (Source 3, Source 4).\n",
            "\n",
            "2. **General Risks**:\n",
            "   - AMD's worldwide operations being subject to political, legal, and economic risks, as well as natural disasters (Source 5).\n",
            "   - Potential future impairments of technology license purchases (Source 5).\n",
            "   - Challenges in attracting and retaining qualified personnel, which may hinder AMD's business (Source 5).\n",
            "   - Volatility of AMD's stock price (Source 5).\n",
            "\n",
            "These risk factors highlight both industry-specific challenges and broader operational risks faced by the company.\n",
            "\n",
            "======================================================================\n",
            "SOURCES (5 chunks)\n",
            "======================================================================\n",
            "\n",
            "1. ADVANCED MICRO DEVICES INC (AMD) - ITEM 1A\n",
            "   Filing Date: 2025-05-07\n",
            "   Relevance Score: 0.7126\n",
            "\n",
            "2. ADVANCED MICRO DEVICES INC (AMD) - ITEM 1A\n",
            "   Filing Date: 2024-10-30\n",
            "   Relevance Score: 0.7126\n",
            "\n",
            "3. ADVANCED MICRO DEVICES INC (AMD) - ITEM 1A\n",
            "   Filing Date: 2025-11-05\n",
            "   Relevance Score: 0.7040\n",
            "\n",
            "4. ADVANCED MICRO DEVICES INC (AMD) - ITEM 1A\n",
            "   Filing Date: 2025-08-06\n",
            "   Relevance Score: 0.7040\n",
            "\n",
            "5. ADVANCED MICRO DEVICES INC (AMD) - ITEM 1A\n",
            "   Filing Date: 2024-10-30\n",
            "   Relevance Score: 0.6687\n",
            "\n",
            "======================================================================\\n\n",
            "\n",
            " Processing query with Manual Engine: 'What risks did Apple disclose in their latest 10-Q?'\n",
            "   → Manually embedding query...\n",
            "→ Query label: risk | Filters: {'company': ['Apple'], 'ticker': ['AAPL']}\n",
            "→ Overriding with explicit ticker filter: AAPL\n",
            "   → Manually searching Qdrant...\n",
            "   → Formatting context and building prompt...\n",
            "   → Sending prompt to LLM...\n",
            "\n",
            "======================================================================\n",
            "ANSWER\n",
            "======================================================================\n",
            "\n",
            "According to Item 1A of Apple's Q3 2024 Form 10-Q, there have been no material changes to the Company's risk factors since their 2023 Form 10-K. Additionally, Apple's Q2 2025 Form 10-Q mentions several factors that could impact the Company, including macroeconomic conditions such as inflation, interest rates, and currency fluctuations, as well as tariffs and other trade measures, which have the potential to materially impact the Company's results of operations and financial condition. It also notes potential impacts on future gross margins due to a variety of factors, which could lead to volatility and downward pressure on gross margins. Furthermore, difficulties in manufacturing and delivering products, increased costs, and negative impacts on consumer demand are risks mentioned in the context of potential events affecting the company's operations (Source 1 and Source 3).\n",
            "\n",
            "======================================================================\n",
            "SOURCES (5 chunks)\n",
            "======================================================================\n",
            "\n",
            "1. Apple Inc. (AAPL) - ITEM 1A\n",
            "   Filing Date: 2024-08-02\n",
            "   Relevance Score: 0.7313\n",
            "\n",
            "2. Apple Inc. (AAPL) - ITEM 2\n",
            "   Filing Date: 2025-05-02\n",
            "   Relevance Score: 0.6400\n",
            "\n",
            "3. Apple Inc. (AAPL) - ITEM 2\n",
            "   Filing Date: 2025-05-02\n",
            "   Relevance Score: 0.6190\n",
            "\n",
            "4. Apple Inc. (AAPL) - ITEM 1A\n",
            "   Filing Date: 2025-05-02\n",
            "   Relevance Score: 0.6020\n",
            "\n",
            "5. Apple Inc. (AAPL) - ITEM 1\n",
            "   Filing Date: 2024-08-02\n",
            "   Relevance Score: 0.5975\n",
            "\n",
            "======================================================================\\n\n",
            "\n",
            " Processing query with Manual Engine: 'Compare the revenue trends of NVIDIA and AMD'\n",
            "   → Manually embedding query...\n",
            "→ Query label: performance | Filters: {'company': ['AMD', 'Nvidia'], 'ticker': ['NVDA']}\n",
            "   → Manually searching Qdrant...\n",
            "   → Formatting context and building prompt...\n",
            "   → Sending prompt to LLM...\n",
            "\n",
            "======================================================================\n",
            "ANSWER\n",
            "======================================================================\n",
            "\n",
            "The provided context from SEC filings only contains information about NVIDIA's revenue trends. It does not include any specific details about AMD's revenue trends, which makes it impossible to provide a comparison between the two companies based on the given context.\n",
            "\n",
            "If you'd like information about NVIDIA's revenue trends, according to Source 4 in ITEM 2, NVIDIA's gaming revenue was up 15% from a year ago and increased 14% sequentially, driven by sales of their GeForce RTX 40 Series GPUs and game console SoCs. Professional Visualization revenue increased by 17% year-over-year and 7% sequentially, while Automotive revenue saw a 72% year-over-year increase and a 30% sequential increase.\n",
            "\n",
            "However, for AMD's revenue trends or a direct comparison between NVIDIA and AMD, the necessary data is not included in the context provided.\n",
            "\n",
            "======================================================================\n",
            "SOURCES (5 chunks)\n",
            "======================================================================\n",
            "\n",
            "1. NVIDIA CORP (NVDA) - ITEM 1\n",
            "   Filing Date: 2024-11-20\n",
            "   Relevance Score: 0.6546\n",
            "\n",
            "2. NVIDIA CORP (NVDA) - ITEM 1\n",
            "   Filing Date: 2024-11-20\n",
            "   Relevance Score: 0.6414\n",
            "\n",
            "3. NVIDIA CORP (NVDA) - ITEM 1\n",
            "   Filing Date: 2024-08-28\n",
            "   Relevance Score: 0.6392\n",
            "\n",
            "4. NVIDIA CORP (NVDA) - ITEM 2\n",
            "   Filing Date: 2024-11-20\n",
            "   Relevance Score: 0.6185\n",
            "\n",
            "5. NVIDIA CORP (NVDA) - ITEM 2\n",
            "   Filing Date: 2025-05-28\n",
            "   Relevance Score: 0.5980\n",
            "\n",
            "======================================================================\\n\n",
            "\n",
            " Processing query with Manual Engine: 'What was Tesla's R&D spending in the latest quarter?'\n",
            "   → Manually embedding query...\n",
            "→ Query label: performance | Filters: {'company': ['Tesla'], 'ticker': ['TSLA']}\n",
            "   → Manually searching Qdrant...\n",
            "   → Formatting context and building prompt...\n",
            "   → Sending prompt to LLM...\n",
            "\n",
            "======================================================================\n",
            "ANSWER\n",
            "======================================================================\n",
            "\n",
            "The exact figure for Tesla's R&D spending in the latest quarter (three months ended September 30, 2025) is not provided in the given context. However, according to Source 2, R&D expenses increased by $591 million, or 57%, in the three months ended September 30, 2025, as compared to the same period in 2024.\n",
            "\n",
            "======================================================================\n",
            "SOURCES (5 chunks)\n",
            "======================================================================\n",
            "\n",
            "1. Tesla, Inc. (TSLA) - ITEM 2\n",
            "   Filing Date: 2024-10-24\n",
            "   Relevance Score: 0.5818\n",
            "\n",
            "2. Tesla, Inc. (TSLA) - ITEM 2\n",
            "   Filing Date: 2025-10-23\n",
            "   Relevance Score: 0.5687\n",
            "\n",
            "3. Tesla, Inc. (TSLA) - ITEM 2\n",
            "   Filing Date: 2025-07-24\n",
            "   Relevance Score: 0.5555\n",
            "\n",
            "4. Tesla, Inc. (TSLA) - ITEM 2\n",
            "   Filing Date: 2024-10-24\n",
            "   Relevance Score: 0.5433\n",
            "\n",
            "5. Tesla, Inc. (TSLA) - ITEM 2\n",
            "   Filing Date: 2025-10-23\n",
            "   Relevance Score: 0.5152\n",
            "\n",
            "======================================================================\\n\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# USAGE EXAMPLE\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # ========================================================================\n",
        "    # INITIALIZE PIPELINE\n",
        "    # ========================================================================\n",
        "    pipeline = SECFilingRAGPipeline()\n",
        "\n",
        "    # ========================================================================\n",
        "    # LOAD AND INDEX FILINGS\n",
        "    # ========================================================================\n",
        "    # This will load the latest 10-Q for each company and index them\n",
        "\n",
        "    pipeline.load_and_index_filings(num_filings_per_ticker=4)\n",
        "\n",
        "    # ========================================================================\n",
        "    # QUERY THE SYSTEM\n",
        "    # ========================================================================\n",
        "\n",
        "    # Example 1: General question\n",
        "    pipeline.query(\"What are the main risk factors mentioned by tech companies?\")\n",
        "\n",
        "    # Example 2: Company-specific question\n",
        "    pipeline.query(\n",
        "        \"What risks did Apple disclose in their latest 10-Q?\",\n",
        "        ticker_filter=\"AAPL\"\n",
        "    )\n",
        "\n",
        "    # Example 3: Comparative question\n",
        "    pipeline.query(\"Compare the revenue trends of NVIDIA and AMD\")\n",
        "\n",
        "    # Example 4: Specific metric question\n",
        "    pipeline.query(\"What was Tesla's R&D spending in the latest quarter?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0aa8ad26",
      "metadata": {
        "id": "0aa8ad26"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
