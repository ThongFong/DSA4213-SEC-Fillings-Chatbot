{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7d3f8234",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting requests==2.32.3 (from -r requirements.txt (line 2))\n",
            "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting beautifulsoup4==4.12.3 (from -r requirements.txt (line 3))\n",
            "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting lxml==5.3.0 (from -r requirements.txt (line 4))\n",
            "  Downloading lxml-5.3.0-cp313-cp313-win_amd64.whl.metadata (3.9 kB)\n",
            "Collecting sentence-transformers==5.0.0 (from -r requirements.txt (line 7))\n",
            "  Downloading sentence_transformers-5.0.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: qdrant-client==1.15.1 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r requirements.txt (line 10)) (1.15.1)\n",
            "Collecting openai==2.6.0 (from -r requirements.txt (line 13))\n",
            "  Downloading openai-2.6.0-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting langchain==1.0.5 (from -r requirements.txt (line 14))\n",
            "  Downloading langchain-1.0.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: langchain-openai==1.0.2 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r requirements.txt (line 15)) (1.0.2)\n",
            "Requirement already satisfied: langchain-qdrant==1.1.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r requirements.txt (line 16)) (1.1.0)\n",
            "Requirement already satisfied: langchain-community==0.4.1 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r requirements.txt (line 17)) (0.4.1)\n",
            "Requirement already satisfied: langchain-huggingface==1.0.1 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r requirements.txt (line 18)) (1.0.1)\n",
            "Collecting langchain-text-splitters==0.3.11 (from -r requirements.txt (line 19))\n",
            "  Downloading langchain_text_splitters-0.3.11-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests==2.32.3->-r requirements.txt (line 2)) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests==2.32.3->-r requirements.txt (line 2)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests==2.32.3->-r requirements.txt (line 2)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests==2.32.3->-r requirements.txt (line 2)) (2025.8.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from beautifulsoup4==4.12.3->-r requirements.txt (line 3)) (2.8)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence-transformers==5.0.0->-r requirements.txt (line 7)) (4.55.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence-transformers==5.0.0->-r requirements.txt (line 7)) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence-transformers==5.0.0->-r requirements.txt (line 7)) (2.8.0)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence-transformers==5.0.0->-r requirements.txt (line 7)) (1.7.1)\n",
            "Requirement already satisfied: scipy in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence-transformers==5.0.0->-r requirements.txt (line 7)) (1.16.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence-transformers==5.0.0->-r requirements.txt (line 7)) (0.34.4)\n",
            "Requirement already satisfied: Pillow in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence-transformers==5.0.0->-r requirements.txt (line 7)) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence-transformers==5.0.0->-r requirements.txt (line 7)) (4.15.0)\n",
            "Requirement already satisfied: grpcio>=1.41.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from qdrant-client==1.15.1->-r requirements.txt (line 10)) (1.76.0)\n",
            "Requirement already satisfied: httpx>=0.20.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx[http2]>=0.20.0->qdrant-client==1.15.1->-r requirements.txt (line 10)) (0.28.1)\n",
            "Requirement already satisfied: numpy>=2.1.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from qdrant-client==1.15.1->-r requirements.txt (line 10)) (2.3.2)\n",
            "Requirement already satisfied: portalocker<4.0,>=2.7.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from qdrant-client==1.15.1->-r requirements.txt (line 10)) (3.2.0)\n",
            "Requirement already satisfied: protobuf>=3.20.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from qdrant-client==1.15.1->-r requirements.txt (line 10)) (6.32.1)\n",
            "Requirement already satisfied: pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from qdrant-client==1.15.1->-r requirements.txt (line 10)) (2.12.4)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai==2.6.0->-r requirements.txt (line 13)) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai==2.6.0->-r requirements.txt (line 13)) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai==2.6.0->-r requirements.txt (line 13)) (0.11.1)\n",
            "Requirement already satisfied: sniffio in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai==2.6.0->-r requirements.txt (line 13)) (1.3.1)\n",
            "Collecting langchain-core<2.0.0,>=1.0.4 (from langchain==1.0.5->-r requirements.txt (line 14))\n",
            "  Downloading langchain_core-1.0.4-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain==1.0.5->-r requirements.txt (line 14)) (1.0.2)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-openai==1.0.2->-r requirements.txt (line 15)) (0.12.0)\n",
            "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-community==0.4.1->-r requirements.txt (line 17)) (1.0.0)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-community==0.4.1->-r requirements.txt (line 17)) (2.0.42)\n",
            "INFO: pip is looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain-qdrant==1.1.0 (from -r requirements.txt (line 16))\n",
            "  Using cached langchain_qdrant-1.1.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting qdrant-client==1.15.1 (from -r requirements.txt (line 10))\n",
            "  Using cached qdrant_client-1.15.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.0.1 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-community==0.4.1->-r requirements.txt (line 17)) (1.0.3)\n",
            "\n",
            "The conflict is caused by:\n",
            "    The user requested requests==2.32.3\n",
            "    langchain-community 0.4.1 depends on requests<3.0.0 and >=2.32.5\n",
            "\n",
            "To fix this you could try to:\n",
            "1. loosen the range of package versions you've specified\n",
            "2. remove package versions to allow pip to attempt to solve the dependency conflict\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Cannot install langchain-community==0.4.1 and requests==2.32.3 because these package versions have conflicting dependencies.\n",
            "\n",
            "[notice] A new release of pip is available: 25.2 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
            "ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3d0b67a3",
      "metadata": {
        "id": "3d0b67a3"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbs4\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup, NavigableString\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# External libraries (install via pip)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mqdrant_client\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m QdrantClient\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mqdrant_client\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Distance, VectorParams, PointStruct\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chan Thong Fong\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sentence_transformers\\__init__.py:10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     11\u001b[39m     export_dynamic_quantized_onnx_model,\n\u001b[32m     12\u001b[39m     export_optimized_onnx_model,\n\u001b[32m     13\u001b[39m     export_static_quantized_openvino_model,\n\u001b[32m     14\u001b[39m )\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcross_encoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     16\u001b[39m     CrossEncoder,\n\u001b[32m     17\u001b[39m     CrossEncoderModelCardData,\n\u001b[32m     18\u001b[39m     CrossEncoderTrainer,\n\u001b[32m     19\u001b[39m     CrossEncoderTrainingArguments,\n\u001b[32m     20\u001b[39m )\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset, SentencesDataset\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chan Thong Fong\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sentence_transformers\\backend\\__init__.py:5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mload\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_onnx_model, load_openvino_model\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptimize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m export_optimized_onnx_model\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mquantize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m export_dynamic_quantized_onnx_model, export_static_quantized_openvino_model\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      7\u001b[39m     _save_pretrained_wrapper,\n\u001b[32m      8\u001b[39m     backend_should_export,\n\u001b[32m      9\u001b[39m     backend_warn_to_save,\n\u001b[32m     10\u001b[39m     save_or_push_to_hub_model,\n\u001b[32m     11\u001b[39m )\n\u001b[32m     13\u001b[39m __all__ = [\n\u001b[32m     14\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mload_onnx_model\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     15\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mload_openvino_model\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     22\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msave_or_push_to_hub_model\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     23\u001b[39m ]\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chan Thong Fong\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sentence_transformers\\backend\\quantize.py:7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, Literal\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m save_or_push_to_hub_model\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m disable_datasets_caching, is_datasets_available\n\u001b[32m      9\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chan Thong Fong\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sentence_transformers\\util\\__init__.py:15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhard_negatives\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mine_hard_negatives\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmisc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m append_to_last_row, disable_datasets_caching, disable_logging, fullname, import_from_string\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mretrieval\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     16\u001b[39m     community_detection,\n\u001b[32m     17\u001b[39m     information_retrieval,\n\u001b[32m     18\u001b[39m     paraphrase_mining,\n\u001b[32m     19\u001b[39m     paraphrase_mining_embeddings,\n\u001b[32m     20\u001b[39m     semantic_search,\n\u001b[32m     21\u001b[39m )\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01msimilarity\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     23\u001b[39m     cos_sim,\n\u001b[32m     24\u001b[39m     dot_score,\n\u001b[32m   (...)\u001b[39m\u001b[32m     32\u001b[39m     pytorch_cos_sim,\n\u001b[32m     33\u001b[39m )\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     35\u001b[39m     _convert_to_batch,\n\u001b[32m     36\u001b[39m     _convert_to_batch_tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m     43\u001b[39m     truncate_embeddings,\n\u001b[32m     44\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chan Thong Fong\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sentence_transformers\\util\\retrieval.py:13\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mautonotebook\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01msimilarity\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cos_sim\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m normalize_embeddings\n\u001b[32m     16\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chan Thong Fong\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sentence_transformers\\util\\similarity.py:5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pairwise_distances\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m logging\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chan Thong Fong\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\__init__.py:73\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# `_distributor_init` allows distributors to run custom init code.\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# For instance, for the Windows wheel, this is used to pre-load the\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# vcomp shared library runtime for OpenMP embedded in the sklearn/.libs\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401 E402\u001b[39;00m\n\u001b[32m     70\u001b[39m     __check_build,\n\u001b[32m     71\u001b[39m     _distributor_init,\n\u001b[32m     72\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clone  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_show_versions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_versions  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[32m     76\u001b[39m _submodules = [\n\u001b[32m     77\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcalibration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     78\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcluster\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    114\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcompose\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    115\u001b[39m ]\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chan Thong Fong\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py:19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_metadata_requests\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester, _routing_enabled\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_missing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_scalar_nan\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chan Thong Fong\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\__init__.py:9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m metadata_routing\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_bunch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Bunch\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_chunking\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gen_batches, gen_even_slices\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Make _safe_indexing importable from here for backward compat as this particular\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# helper is considered semi-private and typically very useful for third-party\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# libraries that want to comply with scikit-learn's estimator API. In particular,\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# _safe_indexing was included in our public API documentation despite the leading\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# `_` in its name.\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_indexing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     17\u001b[39m     _safe_indexing,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m     18\u001b[39m     resample,\n\u001b[32m     19\u001b[39m     shuffle,\n\u001b[32m     20\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chan Thong Fong\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\_chunking.py:11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Interval, validate_params\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchunk_generator\u001b[39m(gen, chunksize):\n\u001b[32m     15\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Chunk generator, ``gen`` into lists of length ``chunksize``. The last\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[33;03m    chunk may have a length less than ``chunksize``.\"\"\"\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chan Thong Fong\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:17\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m csr_matrix, issparse\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvalidation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _is_arraylike_not_scalar\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mInvalidParameterError\u001b[39;00m(\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[32m     21\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Custom exception to be raised when the parameter of a class/method/function\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[33;03m    does not have a valid type or value.\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chan Thong Fong\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:21\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_config \u001b[38;5;28;01mas\u001b[39;00m _get_config\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataConversionWarning, NotFittedError, PositiveSpectrumWarning\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_array_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _asarray_with_order, _is_numpy_namespace, get_namespace\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdeprecation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _deprecate_force_all_finite\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfixes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ComplexWarning, _preserve_dia_indices_dtype\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chan Thong Fong\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexternals\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array_api_extra \u001b[38;5;28;01mas\u001b[39;00m xpx\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexternals\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marray_api_compat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m numpy \u001b[38;5;28;01mas\u001b[39;00m np_compat\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfixes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parse_version\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# TODO: complete __all__\u001b[39;00m\n\u001b[32m     23\u001b[39m __all__ = [\u001b[33m\"\u001b[39m\u001b[33mxpx\u001b[39m\u001b[33m\"\u001b[39m]  \u001b[38;5;66;03m# we import xpx here just to re-export it, need this to appease ruff\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chan Thong Fong\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\fixes.py:16\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msparse\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinalg\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstats\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m optimize\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chan Thong Fong\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scipy\\stats\\__init__.py:626\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03m.. _statsrefmanual:\u001b[39;00m\n\u001b[32m      3\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    621\u001b[39m \n\u001b[32m    622\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[32m    624\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_warnings_errors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (ConstantInputWarning, NearConstantInputWarning,\n\u001b[32m    625\u001b[39m                                DegenerateDataWarning, FitError)\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_stats_py\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    627\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_variation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m variation\n\u001b[32m    628\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chan Thong Fong\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scipy\\stats\\_stats_py.py:52\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# Import unused here but needs to stay until end of deprecation periode\u001b[39;00m\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# See https://github.com/scipy/scipy/issues/15765#issuecomment-1875564522\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m linalg  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m distributions\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _mstats_basic \u001b[38;5;28;01mas\u001b[39;00m mstats_basic\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_stats_mstats_common\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _find_repeats, theilslopes, siegelslopes\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chan Thong Fong\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scipy\\stats\\distributions.py:8\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Author:  Travis Oliphant  2002-2011 with contributions from\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#          SciPy Developers 2004-2011\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      6\u001b[39m \u001b[38;5;66;03m#       instead of `git blame -Lxxx,+x`.\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_distn_infrastructure\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (rv_discrete, rv_continuous, rv_frozen)  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _continuous_distns\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _discrete_distns\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chan Thong Fong\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:27\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m optimize\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# for functions of continuous distributions (e.g. moments, entropy, cdf)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m integrate\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# to approximate the pdf of a continuous distribution given its cdf\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstats\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_finite_differences\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _derivative\n",
            "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1412\u001b[39m, in \u001b[36m_handle_fromlist\u001b[39m\u001b[34m(module, fromlist, import_, recursive)\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chan Thong Fong\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scipy\\__init__.py:143\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(name)\u001b[39m\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getattr__\u001b[39m(name):\n\u001b[32m    142\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m submodules:\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_importlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mscipy.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    145\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chan Thong Fong\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\importlib\\__init__.py:88\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     86\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     87\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chan Thong Fong\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scipy\\integrate\\__init__.py:106\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_odepack_py\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_quadpack_py\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_ode\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_bvp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m solve_bvp\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_ivp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (solve_ivp, OdeSolution, DenseOutput,\n\u001b[32m    109\u001b[39m                    OdeSolver, RK23, RK45, DOP853, Radau, BDF, LSODA)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chan Thong Fong\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scipy\\integrate\\_ode.py:89\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m asarray, array, zeros, isscalar, real, imag, vstack\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _vode\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _dop\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _lsoda\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "SEC 10-Q Filing RAG System\n",
        "A complete pipeline for loading, processing, embedding, and querying SEC 10-Q filings\n",
        "using Qdrant vector database and OpenAI GPT-4o.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import re\n",
        "import uuid\n",
        "import time\n",
        "import requests\n",
        "import gc\n",
        "from typing import List, Dict, Tuple\n",
        "from collections import defaultdict\n",
        "from bs4 import BeautifulSoup, NavigableString\n",
        "\n",
        "# External libraries (install via pip)\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
        "from openai import OpenAI\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "'''\n",
        "from google.colab import userdata\n",
        "\n",
        "# Load secrets from Colab and set them as environment variables\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "os.environ['QDRANT_URL'] = userdata.get('QDRANT_URL')\n",
        "os.environ['QDRANT_API_KEY'] = userdata.get('QDRANT_API_KEY')\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3255ddf",
      "metadata": {
        "id": "f3255ddf"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PART 1: CONFIGURATION & SETUP\n",
        "# ============================================================================\n",
        "\n",
        "class Config:\n",
        "    \"\"\"Configuration class for API keys and model settings\"\"\"\n",
        "\n",
        "    # API Keys - SET THESE BEFORE RUNNING\n",
        "    QDRANT_URL = os.getenv(\"QDRANT_URL\")\n",
        "    QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")\n",
        "    OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "    # SEC EDGAR Configuration\n",
        "    SEC_HEADERS = {'User-Agent': 'SEC10Q-RAG-System research@example.com'}\n",
        "    CIK_MAP_URL = 'https://www.sec.gov/files/company_tickers.json'\n",
        "\n",
        "    # Model Configuration\n",
        "    EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"  # 384 dimensions\n",
        "    LLM_MODEL = \"gpt-4o\"\n",
        "\n",
        "    # Collection Configuration\n",
        "    COLLECTION_NAME = \"sec_filings_10q\"\n",
        "    VECTOR_SIZE = 384  # Dimension for all-MiniLM-L6-v2\n",
        "\n",
        "    # Chunking Configuration\n",
        "    CHUNK_SIZE = 800  # Characters per chunk\n",
        "    CHUNK_OVERLAP = 200  # Overlap between chunks\n",
        "\n",
        "    # Retrieval Configuration\n",
        "    TOP_K = 5  # Number of chunks to retrieve\n",
        "\n",
        "    # Company Tickers\n",
        "    TICKERS = ['NVDA', 'AAPL', 'MSFT', 'AMZN', 'META', 'GOOGL', 'TSLA', 'ORCL', 'JPM', 'AMD']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfb2dfc0",
      "metadata": {
        "id": "bfb2dfc0"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PART 2: DOCUMENT LOADING\n",
        "# ============================================================================\n",
        "\n",
        "class SECDocumentLoader:\n",
        "    \"\"\"Handles fetching and parsing of SEC 10-Q filings\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def get_recent_10q_metadata(ticker: str, num_filings: int = 4) -> List[Dict[str, str]]:\n",
        "        \"\"\"\n",
        "        Fetches the metadata (links, dates, etc.) for the latest N 10-Q filings.\n",
        "        Does NOT fetch the actual HTML content.\n",
        "\n",
        "        Args:\n",
        "            ticker: The company ticker (e.g., 'AAPL')\n",
        "            num_filings: The number of recent 10-Q filings to fetch\n",
        "\n",
        "        Returns:\n",
        "            List of metadata dictionaries\n",
        "        \"\"\"\n",
        "        print(f\"   Fetching CIK for ticker: {ticker}...\")\n",
        "\n",
        "        # Get CIK mapping\n",
        "        response = requests.get(Config.CIK_MAP_URL, headers=Config.SEC_HEADERS)\n",
        "        response.raise_for_status()\n",
        "        company_data = response.json()\n",
        "\n",
        "        # Find CIK\n",
        "        cik = None\n",
        "        company_name = None\n",
        "        for company in company_data.values():\n",
        "            if company['ticker'] == ticker.upper():\n",
        "                cik = str(company['cik_str']).zfill(10)\n",
        "                company_name = company['title']\n",
        "                break\n",
        "\n",
        "        if not cik:\n",
        "            raise ValueError(f\"Ticker '{ticker}' not found in SEC CIK mapping.\")\n",
        "\n",
        "        print(f\"   Found CIK: {cik} ({company_name})\")\n",
        "\n",
        "        # Fetch submission history\n",
        "        submissions_url = f\"https://data.sec.gov/submissions/CIK{cik}.json\"\n",
        "        time.sleep(0.1)  # Rate limiting\n",
        "        response = requests.get(submissions_url, headers=Config.SEC_HEADERS)\n",
        "        response.raise_for_status()\n",
        "        submissions = response.json()\n",
        "\n",
        "        # Find latest N 10-Q filings metadata\n",
        "        filings_metadata = []\n",
        "        for i, form in enumerate(submissions['filings']['recent']['form']):\n",
        "            if form == '10-Q':\n",
        "                accession_number = submissions['filings']['recent']['accessionNumber'][i]\n",
        "                primary_document = submissions['filings']['recent']['primaryDocument'][i]\n",
        "                filing_date = submissions['filings']['recent']['filingDate'][i]\n",
        "\n",
        "                accession_number_clean = accession_number.replace('-', '')\n",
        "\n",
        "                # Construct the filing URL\n",
        "                filing_url = (\n",
        "                    f\"https://www.sec.gov/Archives/edgar/data/{cik}/\"\n",
        "                    f\"{accession_number_clean}/{primary_document}\"\n",
        "                )\n",
        "\n",
        "                metadata = {\n",
        "                    'ticker': ticker.upper(),\n",
        "                    'company_name': company_name,\n",
        "                    'filing_date': filing_date,\n",
        "                    'cik': cik,\n",
        "                    'filing_url': filing_url\n",
        "                }\n",
        "                filings_metadata.append(metadata)\n",
        "\n",
        "                if len(filings_metadata) >= num_filings:\n",
        "                    break\n",
        "\n",
        "        if not filings_metadata:\n",
        "            raise ValueError(f\"No recent 10-Q filings found for ticker '{ticker}'.\")\n",
        "\n",
        "        print(f\"   Found {len(filings_metadata)} recent 10-Q filing metadata entries.\")\n",
        "        return filings_metadata\n",
        "\n",
        "    @staticmethod\n",
        "    def get_filing_html(filing_url: str) -> str:\n",
        "        \"\"\"Fetches the HTML content for a single filing URL.\"\"\"\n",
        "        time.sleep(0.1)  # Rate limiting\n",
        "        response = requests.get(filing_url, headers=Config.SEC_HEADERS)\n",
        "        response.raise_for_status()\n",
        "        return response.text\n",
        "\n",
        "    @staticmethod\n",
        "    def _normalize_header_text(text: str) -> str:\n",
        "        \"\"\"Normalizes header text to standard format\"\"\"\n",
        "        text = text.strip().upper()\n",
        "\n",
        "        # Match \"PART I\" or \"PART II\"\n",
        "        part_match = re.search(r'^\\s*(PART\\s+I{1,2})', text)\n",
        "        if part_match:\n",
        "            return re.sub(r'\\s+', ' ', part_match.group(1))\n",
        "\n",
        "        # Match \"ITEM 1\", \"ITEM 1A\", etc.\n",
        "        item_match = re.search(r'^\\s*(ITEM\\s+\\d[A-Z]?)', text)\n",
        "        if item_match:\n",
        "            return re.sub(r'\\s+', ' ', item_match.group(1))\n",
        "\n",
        "        return None\n",
        "\n",
        "    @staticmethod\n",
        "    def _parse_html_table(table_tag) -> str:\n",
        "        \"\"\"Converts HTML table to Markdown format\"\"\"\n",
        "        markdown_rows = []\n",
        "\n",
        "        for tr in table_tag.find_all('tr'):\n",
        "            cells = [\" \".join(cell.get_text(strip=True).split())\n",
        "                    for cell in tr.find_all(['td', 'th'])]\n",
        "            if any(cells):\n",
        "                markdown_rows.append(cells)\n",
        "\n",
        "        if not markdown_rows:\n",
        "            return \"\"\n",
        "\n",
        "        md_output = []\n",
        "        header = markdown_rows[0]\n",
        "        md_output.append(\"| \" + \" | \".join(header) + \" |\")\n",
        "        md_output.append(\"| \" + \" | \".join(['---'] * len(header)) + \" |\")\n",
        "\n",
        "        for row in markdown_rows[1:]:\n",
        "            while len(row) < len(header):\n",
        "                row.append(\"\")\n",
        "            row = row[:len(header)]\n",
        "            md_output.append(\"| \" + \" | \".join(row) + \" |\")\n",
        "\n",
        "        return \"\\n\" + \"\\n\".join(md_output) + \"\\n\"\n",
        "\n",
        "    @classmethod\n",
        "    def parse_10q(cls, html_content: str) -> Dict:\n",
        "        \"\"\"Parses HTML content into structured dictionary\"\"\"\n",
        "        # --- KEY CHANGE ---\n",
        "        # Use 'lxml' for better memory efficiency\n",
        "        soup = BeautifulSoup(html_content, 'lxml')\n",
        "\n",
        "        potential_headers = soup.find_all(['p', 'b', 'strong', 'div'])\n",
        "\n",
        "        doc_headers = []\n",
        "        for header in potential_headers:\n",
        "            text = header.get_text(strip=True)\n",
        "            if len(text) > 100:\n",
        "                continue\n",
        "\n",
        "            normalized_key = cls._normalize_header_text(text)\n",
        "            if normalized_key:\n",
        "                if not header.find_parent('a'):\n",
        "                    doc_headers.append({'tag': header, 'key': normalized_key})\n",
        "\n",
        "        if not doc_headers:\n",
        "            return {}\n",
        "\n",
        "        parsed_data = defaultdict(lambda: defaultdict(str))\n",
        "        current_part_key = None\n",
        "\n",
        "        for i, header_info in enumerate(doc_headers):\n",
        "            current_key = header_info['key']\n",
        "\n",
        "            if 'PART' in current_key:\n",
        "                current_part_key = current_key\n",
        "                continue\n",
        "\n",
        "            if 'ITEM' in current_key:\n",
        "                if not current_part_key:\n",
        "                    current_part_key = \"PART I\"\n",
        "\n",
        "                start_node = header_info['tag']\n",
        "                end_node = doc_headers[i + 1]['tag'] if i + 1 < len(doc_headers) else None\n",
        "\n",
        "                content_parts = []\n",
        "                element = start_node.next_element\n",
        "\n",
        "                while element and element != end_node:\n",
        "                    if isinstance(element, NavigableString):\n",
        "                        if not element.find_parent('table'):\n",
        "                            text = element.strip()\n",
        "                            if text:\n",
        "                                content_parts.append(text)\n",
        "                    elif element.name == 'table':\n",
        "                        if not element.find_parent('table'):\n",
        "                            table_markdown = cls._parse_html_table(element)\n",
        "                            if table_markdown:\n",
        "                                content_parts.append(table_markdown)\n",
        "\n",
        "                    element = element.next_element\n",
        "\n",
        "                full_content = \"\\n\".join(content_parts)\n",
        "                clean_content = re.sub(r'\\n{3,}', '\\n\\n', full_content).strip()\n",
        "\n",
        "                parsed_data[current_part_key][current_key] = clean_content\n",
        "\n",
        "        return {part: dict(items) for part, items in parsed_data.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cbff8d1",
      "metadata": {
        "id": "2cbff8d1"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PART 3: TEXT CHUNKING & EMBEDDING\n",
        "# ============================================================================\n",
        "import uuid\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "class DocumentProcessor:\n",
        "    \"\"\"\n",
        "    Processes documents into chunks using LangChain's splitter\n",
        "    and then embeds them in batches.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embedding_model_name: str = Config.EMBEDDING_MODEL):\n",
        "        \"\"\"Initialize with embedding model and LangChain text splitter\"\"\"\n",
        "        print(f\"\\n Loading embedding model: {embedding_model_name}\")\n",
        "        self.model = SentenceTransformer(embedding_model_name)\n",
        "        print(f\"    Model loaded (dimension: {self.model.get_sentence_embedding_dimension()})\")\n",
        "\n",
        "        # Initialize LangChain's splitter\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=Config.CHUNK_SIZE,\n",
        "            chunk_overlap=Config.CHUNK_OVERLAP,\n",
        "            length_function=len,\n",
        "            add_start_index=False, # Simpler metadata\n",
        "        )\n",
        "        print(f\"    Initialized RecursiveCharacterTextSplitter (chunk: {Config.CHUNK_SIZE}, overlap: {Config.CHUNK_OVERLAP})\")\n",
        "\n",
        "    def generate_document_chunks(self, parsed_data: Dict, metadata: Dict,\n",
        "                                 embed_batch_size: int = 1024):\n",
        "        \"\"\"\n",
        "        Processes parsed 10-Q data using LangChain's splitter,\n",
        "        then YIELDS chunk points one by one after batch-embedding.\n",
        "        \"\"\"\n",
        "\n",
        "        # Convert the parsed dict into a list of LangChain Document objects\n",
        "        all_docs = []\n",
        "        for part, items in parsed_data.items():\n",
        "            for item, content in items.items():\n",
        "                if not content:\n",
        "                    continue\n",
        "\n",
        "                # Create a metadata dict for *this specific document*\n",
        "                # before it gets chunked.\n",
        "                doc_metadata = {\n",
        "                    'ticker': metadata['ticker'],\n",
        "                    'company_name': metadata['company_name'],\n",
        "                    'filing_date': metadata['filing_date'],\n",
        "                    'filing_url': metadata['filing_url'],\n",
        "                    'part': part,\n",
        "                    'item': item\n",
        "                }\n",
        "\n",
        "                doc = Document(\n",
        "                    page_content=content,\n",
        "                    metadata=doc_metadata\n",
        "                )\n",
        "                all_docs.append(doc)\n",
        "\n",
        "        if not all_docs:\n",
        "            return # Stop the generator\n",
        "\n",
        "        # Split all documents at once using the LangChain splitter\n",
        "        print(f\"      Splitting {len(all_docs)} high-level 'Items' into smaller chunks...\")\n",
        "        chunked_docs = self.text_splitter.split_documents(all_docs)\n",
        "        print(f\"      Generated {len(chunked_docs)} chunks\")\n",
        "\n",
        "        text_batch = []\n",
        "        metadata_batch = []\n",
        "\n",
        "        # Consume the list one chunk at a time\n",
        "        for chunk in chunked_docs:\n",
        "            text_batch.append(chunk.page_content)\n",
        "            # The splitter automatically copies metadata to each chunk\n",
        "            metadata_batch.append(chunk.metadata)\n",
        "\n",
        "            # If batch is full, process it\n",
        "            if len(text_batch) >= embed_batch_size:\n",
        "                # 1. Embed the entire batch in one call\n",
        "                embeddings = self.model.encode(text_batch, show_progress_bar=False)\n",
        "\n",
        "                # 2. Yield each point from the processed batch\n",
        "                for txt, emb, meta in zip(text_batch, embeddings, metadata_batch):\n",
        "                    # The metadata (meta) already contains everything\n",
        "                    # from the doc_metadata we built above\n",
        "                    payload = {\n",
        "                        'text': txt,\n",
        "                        **meta # Unpack all metadata keys (ticker, item, part, etc.)\n",
        "                    }\n",
        "                    yield PointStruct(\n",
        "                        id=str(uuid.uuid4()),\n",
        "                        vector=emb.tolist(),\n",
        "                        payload=payload\n",
        "                    )\n",
        "\n",
        "                # 3. Reset the batch\n",
        "                text_batch = []\n",
        "                metadata_batch = []\n",
        "\n",
        "        if text_batch:\n",
        "            # 1. Embed the final batch\n",
        "            embeddings = self.model.encode(text_batch, show_progress_bar=False)\n",
        "\n",
        "            # 2. Yield each point\n",
        "            for txt, emb, meta in zip(text_batch, embeddings, metadata_batch):\n",
        "                payload = {\n",
        "                    'text': txt,\n",
        "                    **meta\n",
        "                }\n",
        "                yield PointStruct(\n",
        "                    id=str(uuid.uuid4()),\n",
        "                    vector=emb.tolist(),\n",
        "                    payload=payload\n",
        "                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80bed17c",
      "metadata": {
        "id": "80bed17c"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PART 4: QDRANT VECTOR DATABASE\n",
        "# ============================================================================\n",
        "\n",
        "from qdrant_client import models\n",
        "\n",
        "class QdrantManager:\n",
        "    \"\"\"Manages Qdrant vector database operations\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize Qdrant client\"\"\"\n",
        "        print(f\"\\nConnecting to Qdrant Cloud...\")\n",
        "        self.client = QdrantClient(\n",
        "            url=Config.QDRANT_URL,\n",
        "            api_key=Config.QDRANT_API_KEY\n",
        "        )\n",
        "        print(f\"    Connected to Qdrant\")\n",
        "\n",
        "    def create_collection(self, collection_name: str = Config.COLLECTION_NAME,\n",
        "                         vector_size: int = Config.VECTOR_SIZE):\n",
        "        \"\"\"Create or recreate collection AND set up payload indexes\"\"\"\n",
        "        print(f\"\\n Setting up collection: {collection_name}\")\n",
        "\n",
        "        # Check if collection exists\n",
        "        collections = self.client.get_collections().collections\n",
        "        exists = any(col.name == collection_name for col in collections)\n",
        "\n",
        "        if exists:\n",
        "            print(f\"    Collection exists, recreating...\")\n",
        "            self.client.delete_collection(collection_name)\n",
        "\n",
        "        # Create collection\n",
        "        self.client.create_collection(\n",
        "            collection_name=collection_name,\n",
        "            vectors_config=models.VectorParams( # Use models.VectorParams\n",
        "                size=vector_size,\n",
        "                distance=models.Distance.COSINE # Use models.Distance\n",
        "            )\n",
        "        )\n",
        "        print(f\"    Collection created\")\n",
        "\n",
        "        print(f\"    Creating payload index for 'ticker'...\")\n",
        "        self.client.create_payload_index(\n",
        "            collection_name=collection_name,\n",
        "            field_name=\"ticker\",\n",
        "            field_schema=models.PayloadSchemaType.KEYWORD\n",
        "        )\n",
        "        print(f\"    Creating payload index for 'item'...\")\n",
        "        self.client.create_payload_index(\n",
        "            collection_name=collection_name,\n",
        "            field_name=\"item\",\n",
        "            field_schema=models.PayloadSchemaType.KEYWORD\n",
        "        )\n",
        "        print(f\"    Payload indexes created.\")\n",
        "\n",
        "    def upsert_documents(self, points_generator,\n",
        "                        collection_name: str = Config.COLLECTION_NAME,\n",
        "                        batch_size: int = 2048) -> int:\n",
        "        \"\"\"\n",
        "        Uploads document chunks from a generator in batches.\n",
        "\n",
        "        Args:\n",
        "            points_generator: A generator that yields PointStructs\n",
        "            collection_name: Name of the collection\n",
        "            batch_size: Number of points to upload at once\n",
        "\n",
        "        Returns:\n",
        "            Total number of chunks uploaded\n",
        "        \"\"\"\n",
        "        print(f\" Uploading chunks to Qdrant in batches of {batch_size}...\")\n",
        "\n",
        "        batch = []\n",
        "        count = 0\n",
        "\n",
        "        for point in points_generator:\n",
        "            batch.append(point)\n",
        "\n",
        "            if len(batch) >= batch_size:\n",
        "                self.client.upsert(\n",
        "                    collection_name=collection_name,\n",
        "                    points=batch,\n",
        "                    wait=False # Added for speed\n",
        "                )\n",
        "                count += len(batch)\n",
        "                print(f\"      Uploaded {count} chunks so far...\")\n",
        "                batch = [] # Reset batch\n",
        "\n",
        "        # Upload any remaining points\n",
        "        if batch:\n",
        "            self.client.upsert(\n",
        "                collection_name=collection_name,\n",
        "                points=batch,\n",
        "                wait=False # Added for speed\n",
        "            )\n",
        "            count += len(batch)\n",
        "\n",
        "        print(f\"   All chunks uploaded for this document. Total: {count}\")\n",
        "        return count\n",
        "\n",
        "    def search(self, query_vector: List[float],\n",
        "              collection_name: str = Config.COLLECTION_NAME,\n",
        "              limit: int = Config.TOP_K,\n",
        "              filter_dict: Dict = None) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Search for similar documents\n",
        "\n",
        "        Args:\n",
        "            query_vector: Embedded query vector\n",
        "            collection_name: Name of the collection\n",
        "            limit: Number of results to return\n",
        "            filter_dict: Optional filter (e.g., {\"ticker\": \"AAPL\"})\n",
        "\n",
        "        Returns:\n",
        "            List of search results with scores and payloads\n",
        "        \"\"\"\n",
        "\n",
        "        qdrant_filter = None\n",
        "        if filter_dict:\n",
        "            qdrant_filter = models.Filter(\n",
        "                must=[\n",
        "                    models.FieldCondition(\n",
        "                        key=key,\n",
        "                        match=models.MatchValue(value=value)\n",
        "                    )\n",
        "                    for key, value in filter_dict.items()\n",
        "                ]\n",
        "            )\n",
        "\n",
        "        results = self.client.search(\n",
        "            collection_name=collection_name,\n",
        "            query_vector=query_vector,\n",
        "            limit=limit,\n",
        "            query_filter=qdrant_filter,\n",
        "            with_payload=True\n",
        "        )\n",
        "\n",
        "        return [\n",
        "            {\n",
        "                'score': result.score,\n",
        "                'payload': result.payload\n",
        "            }\n",
        "            for result in results\n",
        "        ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8471b72b",
      "metadata": {
        "id": "8471b72b"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PART 5: RAG QUERY PIPELINE\n",
        "# ============================================================================\n",
        "import os\n",
        "from openai import OpenAI\n",
        "from qdrant_client import models\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "\n",
        "class ManualRAGEngine:\n",
        "    \"\"\"\n",
        "    This is the manual RAG query engine that replaces the buggy\n",
        "    LangChainRAGEngine. It performs the RAG steps manually\n",
        "    using the components we've already built.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, document_processor: 'DocumentProcessor', qdrant_manager: 'QdrantManager'):\n",
        "        \"\"\"\n",
        "        Initialize the engine with the processor (for embeddings)\n",
        "        and the manager (for search).\n",
        "        \"\"\"\n",
        "        print(\"\\n Initializing Manual RAG Query Engine...\")\n",
        "\n",
        "        # 1. Get the embedding model from the document processor\n",
        "        #    (the one that is already loaded)\n",
        "        self.embedding_model = document_processor.model\n",
        "        print(\"    Using existing embedding model from DocumentProcessor\")\n",
        "\n",
        "        # 2. Get the Qdrant client from the Qdrant manager\n",
        "        self.qdrant_manager = qdrant_manager\n",
        "        print(\"    Using existing QdrantManager for search\")\n",
        "\n",
        "        # 3. Initialize the LLM\n",
        "        self.llm = ChatOpenAI(\n",
        "            model=Config.LLM_MODEL,\n",
        "            api_key=Config.OPENAI_API_KEY\n",
        "        )\n",
        "        print(\"    Initialized ChatOpenAI LLM\")\n",
        "\n",
        "        # 4. Create a prompt template (we can still use this part)\n",
        "        template = \"\"\"You are a helpful financial analyst assistant. Your role is to answer questions about SEC 10-Q filings based ONLY on the provided context.\n",
        "- Base your answer strictly on the provided context from SEC filings\n",
        "- Cite specific sections (e.g., \"According to Item 1A...\") when referencing information\n",
        "- If the answer is not in the context, clearly state that\n",
        "\n",
        "Context:\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "\n",
        "Question: {input}\n",
        "\n",
        "Answer:\"\"\"\n",
        "        self.prompt = ChatPromptTemplate.from_template(template)\n",
        "        print(\"    Manual RAG Engine ready.\")\n",
        "\n",
        "\n",
        "    def _format_context(self, search_results: List[Dict]) -> str:\n",
        "        \"\"\"Helper function to format the retrieved contexts\"\"\"\n",
        "        context_str = \"\"\n",
        "        for i, result in enumerate(search_results, 1):\n",
        "            payload = result.get('payload', {})\n",
        "            text = payload.get('text', 'No text found')\n",
        "            item = payload.get('item', 'N/A')\n",
        "            ticker = payload.get('ticker', 'N/A')\n",
        "            context_str += f\"Source {i} ({ticker} - {item}):\\n\\\"{text}\\\"\\n\\n\"\n",
        "        return context_str.strip()\n",
        "\n",
        "\n",
        "    def query(self, question: str, ticker_filter: str = None):\n",
        "        \"\"\"\n",
        "        Query the indexed filings using the manual retrieval and generation.\n",
        "        \"\"\"\n",
        "        print(f\"\\n Processing query with Manual Engine: '{question}'\")\n",
        "\n",
        "        # 1. Manually embed the query\n",
        "        print(\"    Manually embedding query...\")\n",
        "        query_vector = self.embedding_model.encode(question).tolist()\n",
        "\n",
        "        # 2. Manually create the filter\n",
        "        filter_dict = None\n",
        "        if ticker_filter:\n",
        "            print(f\"    Applying ticker filter: {ticker_filter}\")\n",
        "            filter_dict = {\"ticker\": ticker_filter}\n",
        "\n",
        "        # 3. Manually search Qdrant\n",
        "        print(\"    Manually searching Qdrant...\")\n",
        "        search_results = self.qdrant_manager.search(\n",
        "            query_vector=query_vector,\n",
        "            limit=Config.TOP_K,\n",
        "            filter_dict=filter_dict\n",
        "        )\n",
        "\n",
        "        if not search_results:\n",
        "            return {'answer': 'No relevant context was found in the documents to answer this question.', 'sources': []}\n",
        "\n",
        "        # 4. Manually format the prompt\n",
        "        print(\"    Formatting context and building prompt...\")\n",
        "        formatted_context = self._format_context(search_results)\n",
        "\n",
        "        # We use the prompt template to create the final message\n",
        "        final_prompt_message = self.prompt.format_messages(\n",
        "            context=formatted_context,\n",
        "            input=question\n",
        "        )\n",
        "\n",
        "        # 5. Manually invoke the LLM\n",
        "        print(\"    Sending prompt to LLM...\")\n",
        "        llm_response = self.llm.invoke(final_prompt_message)\n",
        "        answer = llm_response.content\n",
        "\n",
        "        # 6. Format sources to match the expected output\n",
        "        sources = []\n",
        "        for i, result in enumerate(search_results, 1):\n",
        "            sources.append({\n",
        "                'ticker': result['payload'].get('ticker'),\n",
        "                'company': result['payload'].get('company_name'),\n",
        "                'item': result['payload'].get('item'),\n",
        "                'part': result['payload'].get('part'),\n",
        "                'filing_date': result['payload'].get('filing_date'),\n",
        "                'score': result['score'] # We get the real score now\n",
        "            })\n",
        "\n",
        "        return {\n",
        "            'answer': answer,\n",
        "            'sources': sources\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b12c056",
      "metadata": {
        "id": "9b12c056"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PART 6: MAIN PIPELINE ORCHESTRATOR\n",
        "# ============================================================================\n",
        "\n",
        "class SECFilingRAGPipeline:\n",
        "    \"\"\"Main pipeline orchestrator\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize all components\"\"\"\n",
        "        print(\"=\" * 70)\n",
        "        print(\"SEC 10-Q FILING RAG SYSTEM\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        # Ingestion components (these are working perfectly)\n",
        "        self.loader = SECDocumentLoader()\n",
        "        self.processor = DocumentProcessor()\n",
        "        self.qdrant_manager = QdrantManager()\n",
        "\n",
        "        # Query component (to be initialized later)\n",
        "        self.query_engine = None\n",
        "\n",
        "\n",
        "    def load_and_index_filings(self, tickers: List[str] = Config.TICKERS, num_filings_per_ticker: int = 1):\n",
        "        \"\"\"\n",
        "        Load and index 10-Q filings for multiple companies.\n",
        "        (This method is UNCHANGED and works)\n",
        "        \"\"\"\n",
        "        print(f\"\\n{'=' * 70}\")\n",
        "        print(f\"LOADING & INDEXING PHASE (Streaming)\")\n",
        "        print(f\"{'=' * 70}\")\n",
        "        print(f\"\\nProcessing {len(tickers)} companies: {', '.join(tickers)}\")\n",
        "        print(f\"(Fetching {num_filings_per_ticker} filings per company)\\n\")\n",
        "\n",
        "        # Create collection (do this once at the start)\n",
        "        self.qdrant_manager.create_collection() # Use the manager here\n",
        "\n",
        "        successful_tickers = []\n",
        "        failed_tickers = []\n",
        "        total_chunks_indexed = 0\n",
        "\n",
        "        # Process each ticker\n",
        "        for idx, ticker in enumerate( tickers, 1):\n",
        "            print(f\"\\n[{idx}/{len(tickers)}] Processing {ticker}\")\n",
        "            print(\"-\" * 70)\n",
        "\n",
        "            ticker_chunks_count = 0\n",
        "            num_filings_processed = 0\n",
        "\n",
        "            try:\n",
        "                # 1. Get METADATA (links) for N filings\n",
        "                filings_metadata_list = self.loader.get_recent_10q_metadata(ticker, num_filings=num_filings_per_ticker)\n",
        "\n",
        "                # 2. Loop through each FILING METADATA\n",
        "                for filing_metadata in filings_metadata_list:\n",
        "                    try:\n",
        "                        filing_date = filing_metadata['filing_date']\n",
        "                        filing_url = filing_metadata['filing_url']\n",
        "\n",
        "                        # 3. Download ONE filing's HTML\n",
        "                        print(f\"   Downloading filing from: {filing_date}...\")\n",
        "                        html_content = self.loader.get_filing_html(filing_url)\n",
        "\n",
        "                        # 4. Parse ONE filing\n",
        "                        print(f\"   Parsing 10-Q structure...\")\n",
        "                        parsed_data = self.loader.parse_10q(html_content)\n",
        "\n",
        "                        if not parsed_data:\n",
        "                            print(f\"   Warning: No structured data parsed for {ticker} on {filing_date}\")\n",
        "                            del html_content\n",
        "                            gc.collect()\n",
        "                            continue\n",
        "\n",
        "                        # 5. Process ONE filing into a GENERATOR\n",
        "                        print(f\"   Creating chunks and embeddings generator...\")\n",
        "                        # Use the LANGCHAIN-ENHANCED chunker from Part 3\n",
        "                        chunks_generator = self.processor.generate_document_chunks(parsed_data, filing_metadata)\n",
        "\n",
        "                        # 6. Upload ONE filing's chunks from the generator\n",
        "                        num_uploaded = self.qdrant_manager.upsert_documents(chunks_generator) # Use the manager here\n",
        "\n",
        "                        if num_uploaded > 0:\n",
        "                            ticker_chunks_count += num_uploaded\n",
        "                            total_chunks_indexed += num_uploaded\n",
        "                            num_filings_processed += 1\n",
        "\n",
        "                        # 7. MANUALLY CLEAN UP MEMORY\n",
        "                        print(f\"   Cleaning up memory...\")\n",
        "                        del html_content\n",
        "                        del parsed_data\n",
        "                        del chunks_generator # Clear generator\n",
        "                        gc.collect() # Force garbage collection\n",
        "                        print(f\"   Memory cleaned.\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"   Error processing filing {filing_metadata.get('filing_date', 'unknown')} for {ticker}: {str(e)}\")\n",
        "                        gc.collect() # Force cleanup on error\n",
        "\n",
        "                # After all filings for this ticker are done\n",
        "                if ticker_chunks_count > 0:\n",
        "                    successful_tickers.append(ticker)\n",
        "                    print(f\"   Finished {ticker}. Total chunks: {ticker_chunks_count} across {num_filings_processed} filings\")\n",
        "                else:\n",
        "                    failed_tickers.append(ticker)\n",
        "                    print(f\"   No chunks created for {ticker}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"   Error processing {ticker} (failed to get metadata): {str(e)}\")\n",
        "                failed_tickers.append(ticker)\n",
        "\n",
        "            # Rate limiting\n",
        "            if idx < len(tickers):\n",
        "                time.sleep(0.2)\n",
        "\n",
        "        # Summary\n",
        "        print(f\"\\n{'=' * 70}\")\n",
        "        print(f\"INDEXING COMPLETE\")\n",
        "        print(f\"{'=' * 70}\")\n",
        "        print(f\" Successfully processed: {len(successful_tickers)} companies\")\n",
        "        print(f\"  {', '.join(successful_tickers)}\")\n",
        "        if failed_tickers:\n",
        "            print(f\" Failed: {len(failed_tickers)} companies\")\n",
        "            print(f\"  {', '.join(failed_tickers)}\")\n",
        "        print(f\"\\n Total chunks indexed: {total_chunks_indexed}\")\n",
        "        print(f\"{'=' * 70}\\n\")\n",
        "\n",
        "    def query(self, question: str, ticker_filter: str = None):\n",
        "        \"\"\"\n",
        "        Query the indexed filings using the new MANUAL engine\n",
        "        \"\"\"\n",
        "        if self.query_engine is None:\n",
        "            # Initialize the ManualRAGEngine, passing it the\n",
        "            # processor (for the model) and manager (for search)\n",
        "            self.query_engine = ManualRAGEngine(\n",
        "                document_processor=self.processor,\n",
        "                qdrant_manager=self.qdrant_manager\n",
        "            )\n",
        "\n",
        "        result = self.query_engine.query(question, ticker_filter)\n",
        "\n",
        "        # Print results (this part is the same)\n",
        "        print(f\"\\n{'=' * 70}\")\n",
        "        print(f\"ANSWER\")\n",
        "        print(f\"{'=' * 70}\")\n",
        "        print(f\"\\n{result['answer']}\\n\")\n",
        "\n",
        "        print(f\"{'=' * 70}\")\n",
        "        print(f\"SOURCES ({len(result['sources'])} chunks)\")\n",
        "        print(f\"{'=' * 70}\")\n",
        "        for i, source in enumerate(result['sources'], 1):\n",
        "            print(f\"\\n{i}. {source['company']} ({source['ticker']}) - {source['item']}\")\n",
        "            print(f\"   Filing Date: {source['filing_date']}\")\n",
        "            print(f\"   Relevance Score: {source['score']:.4f}\") # Can now show the real score\n",
        "\n",
        "        print(f\"\\n{'=' * 70}\\\\n\")\n",
        "\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fb9524b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fb9524b",
        "outputId": "68e39ca6-af07-4360-9353-71f56d4754e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "SEC 10-Q FILING RAG SYSTEM\n",
            "======================================================================\n",
            "\n",
            " Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
            "    Model loaded (dimension: 384)\n",
            "    Initialized RecursiveCharacterTextSplitter (chunk: 800, overlap: 200)\n",
            "\n",
            "Connecting to Qdrant Cloud...\n",
            "    Connected to Qdrant\n",
            "\n",
            "======================================================================\n",
            "LOADING & INDEXING PHASE (Streaming)\n",
            "======================================================================\n",
            "\n",
            "Processing 10 companies: NVDA, AAPL, AMD, MSFT, AMZN, META, GOOGL, TSLA, BRK.B, JPM\n",
            "(Fetching 1 filings per company)\n",
            "\n",
            "\n",
            " Setting up collection: sec_filings_10q\n",
            "    Collection exists, recreating...\n",
            "    Collection created\n",
            "    Creating payload index for 'ticker'...\n",
            "    Creating payload index for 'item'...\n",
            "    Payload indexes created.\n",
            "\n",
            "[1/10] Processing NVDA\n",
            "----------------------------------------------------------------------\n",
            "   Fetching CIK for ticker: NVDA...\n",
            "   Found CIK: 0001045810 (NVIDIA CORP)\n",
            "   Found 1 recent 10-Q filing metadata entries.\n",
            "   Downloading filing from: 2025-08-27...\n",
            "   Parsing 10-Q structure...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Chan Thong Fong\\AppData\\Local\\Temp\\ipykernel_27076\\2600233645.py:140: XMLParsedAsHTMLWarning: It looks like you're using an HTML parser to parse an XML document.\n",
            "\n",
            "Assuming this really is an XML document, what you're doing might work, but you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the Python package 'lxml' installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
            "\n",
            "If you want or need to use an HTML parser on this document, you can make this warning go away by filtering it. To do that, run this code before calling the BeautifulSoup constructor:\n",
            "\n",
            "    from bs4 import XMLParsedAsHTMLWarning\n",
            "    import warnings\n",
            "\n",
            "    warnings.filterwarnings(\"ignore\", category=XMLParsedAsHTMLWarning)\n",
            "\n",
            "  soup = BeautifulSoup(html_content, 'lxml')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Creating chunks and embeddings generator...\n",
            " Uploading chunks to Qdrant in batches of 2048...\n",
            "      Splitting 9 high-level 'Items' into smaller chunks...\n",
            "      Generated 245 chunks\n",
            "   All chunks uploaded for this document. Total: 245\n",
            "   Cleaning up memory...\n",
            "   Memory cleaned.\n",
            "   Finished NVDA. Total chunks: 245 across 1 filings\n",
            "\n",
            "[2/10] Processing AAPL\n",
            "----------------------------------------------------------------------\n",
            "   Fetching CIK for ticker: AAPL...\n",
            "   Found CIK: 0000320193 (Apple Inc.)\n",
            "   Found 1 recent 10-Q filing metadata entries.\n",
            "   Downloading filing from: 2025-08-01...\n",
            "   Parsing 10-Q structure...\n",
            "   Creating chunks and embeddings generator...\n",
            " Uploading chunks to Qdrant in batches of 2048...\n",
            "      Splitting 11 high-level 'Items' into smaller chunks...\n",
            "      Generated 116 chunks\n",
            "   All chunks uploaded for this document. Total: 116\n",
            "   Cleaning up memory...\n",
            "   Memory cleaned.\n",
            "   Finished AAPL. Total chunks: 116 across 1 filings\n",
            "\n",
            "[3/10] Processing AMD\n",
            "----------------------------------------------------------------------\n",
            "   Fetching CIK for ticker: AMD...\n",
            "   Found CIK: 0000002488 (ADVANCED MICRO DEVICES INC)\n",
            "   Found 1 recent 10-Q filing metadata entries.\n",
            "   Downloading filing from: 2025-11-05...\n",
            "   Parsing 10-Q structure...\n",
            "   Creating chunks and embeddings generator...\n",
            " Uploading chunks to Qdrant in batches of 2048...\n",
            "      Splitting 9 high-level 'Items' into smaller chunks...\n",
            "      Generated 430 chunks\n",
            "   All chunks uploaded for this document. Total: 430\n",
            "   Cleaning up memory...\n",
            "   Memory cleaned.\n",
            "   Finished AMD. Total chunks: 430 across 1 filings\n",
            "\n",
            "[4/10] Processing MSFT\n",
            "----------------------------------------------------------------------\n",
            "   Fetching CIK for ticker: MSFT...\n",
            "   Found CIK: 0000789019 (MICROSOFT CORP)\n",
            "   Found 1 recent 10-Q filing metadata entries.\n",
            "   Downloading filing from: 2025-10-29...\n",
            "   Parsing 10-Q structure...\n",
            "   Creating chunks and embeddings generator...\n",
            " Uploading chunks to Qdrant in batches of 2048...\n",
            "      Splitting 9 high-level 'Items' into smaller chunks...\n",
            "      Generated 31 chunks\n",
            "   All chunks uploaded for this document. Total: 31\n",
            "   Cleaning up memory...\n",
            "   Memory cleaned.\n",
            "   Finished MSFT. Total chunks: 31 across 1 filings\n",
            "\n",
            "[5/10] Processing AMZN\n",
            "----------------------------------------------------------------------\n",
            "   Fetching CIK for ticker: AMZN...\n",
            "   Found CIK: 0001018724 (AMAZON COM INC)\n",
            "   Found 1 recent 10-Q filing metadata entries.\n",
            "   Downloading filing from: 2025-10-31...\n",
            "   Parsing 10-Q structure...\n",
            "   Creating chunks and embeddings generator...\n",
            " Uploading chunks to Qdrant in batches of 2048...\n",
            "      Splitting 11 high-level 'Items' into smaller chunks...\n",
            "      Generated 352 chunks\n",
            "   All chunks uploaded for this document. Total: 352\n",
            "   Cleaning up memory...\n",
            "   Memory cleaned.\n",
            "   Finished AMZN. Total chunks: 352 across 1 filings\n",
            "\n",
            "[6/10] Processing META\n",
            "----------------------------------------------------------------------\n",
            "   Fetching CIK for ticker: META...\n",
            "   Found CIK: 0001326801 (Meta Platforms, Inc.)\n",
            "   Found 1 recent 10-Q filing metadata entries.\n",
            "   Downloading filing from: 2025-10-30...\n",
            "   Parsing 10-Q structure...\n",
            "   Creating chunks and embeddings generator...\n",
            " Uploading chunks to Qdrant in batches of 2048...\n",
            "      Splitting 9 high-level 'Items' into smaller chunks...\n",
            "      Generated 704 chunks\n",
            "   All chunks uploaded for this document. Total: 704\n",
            "   Cleaning up memory...\n",
            "   Memory cleaned.\n",
            "   Finished META. Total chunks: 704 across 1 filings\n",
            "\n",
            "[7/10] Processing GOOGL\n",
            "----------------------------------------------------------------------\n",
            "   Fetching CIK for ticker: GOOGL...\n",
            "   Found CIK: 0001652044 (Alphabet Inc.)\n",
            "   Found 1 recent 10-Q filing metadata entries.\n",
            "   Downloading filing from: 2025-10-30...\n",
            "   Parsing 10-Q structure...\n",
            "   Creating chunks and embeddings generator...\n",
            " Uploading chunks to Qdrant in batches of 2048...\n",
            "      Splitting 9 high-level 'Items' into smaller chunks...\n",
            "      Generated 277 chunks\n",
            "   All chunks uploaded for this document. Total: 277\n",
            "   Cleaning up memory...\n",
            "   Memory cleaned.\n",
            "   Finished GOOGL. Total chunks: 277 across 1 filings\n",
            "\n",
            "[8/10] Processing TSLA\n",
            "----------------------------------------------------------------------\n",
            "   Fetching CIK for ticker: TSLA...\n",
            "   Found CIK: 0001318605 (Tesla, Inc.)\n",
            "   Found 1 recent 10-Q filing metadata entries.\n",
            "   Downloading filing from: 2025-10-23...\n",
            "   Parsing 10-Q structure...\n",
            "   Creating chunks and embeddings generator...\n",
            " Uploading chunks to Qdrant in batches of 2048...\n",
            "      Splitting 11 high-level 'Items' into smaller chunks...\n",
            "      Generated 247 chunks\n",
            "   All chunks uploaded for this document. Total: 247\n",
            "   Cleaning up memory...\n",
            "   Memory cleaned.\n",
            "   Finished TSLA. Total chunks: 247 across 1 filings\n",
            "\n",
            "[9/10] Processing BRK.B\n",
            "----------------------------------------------------------------------\n",
            "   Fetching CIK for ticker: BRK.B...\n",
            "   Error processing BRK.B (failed to get metadata): Ticker 'BRK.B' not found in SEC CIK mapping.\n",
            "\n",
            "[10/10] Processing JPM\n",
            "----------------------------------------------------------------------\n",
            "   Fetching CIK for ticker: JPM...\n",
            "   Found CIK: 0000019617 (JPMORGAN CHASE & CO)\n",
            "   Found 1 recent 10-Q filing metadata entries.\n",
            "   Downloading filing from: 2025-11-04...\n",
            "   Parsing 10-Q structure...\n",
            "   Creating chunks and embeddings generator...\n",
            " Uploading chunks to Qdrant in batches of 2048...\n",
            "      Splitting 8 high-level 'Items' into smaller chunks...\n",
            "      Generated 31 chunks\n",
            "   All chunks uploaded for this document. Total: 31\n",
            "   Cleaning up memory...\n",
            "   Memory cleaned.\n",
            "   Finished JPM. Total chunks: 31 across 1 filings\n",
            "\n",
            "======================================================================\n",
            "INDEXING COMPLETE\n",
            "======================================================================\n",
            " Successfully processed: 9 companies\n",
            "  NVDA, AAPL, AMD, MSFT, AMZN, META, GOOGL, TSLA, JPM\n",
            " Failed: 1 companies\n",
            "  BRK.B\n",
            "\n",
            " Total chunks indexed: 2433\n",
            "======================================================================\n",
            "\n",
            "\n",
            " Initializing Manual RAG Query Engine...\n",
            "    Using existing embedding model from DocumentProcessor\n",
            "    Using existing QdrantManager for search\n",
            "    Initialized ChatOpenAI LLM\n",
            "    Manual RAG Engine ready.\n",
            "\n",
            " Processing query with Manual Engine: 'What are the main risk factors mentioned by tech companies?'\n",
            "    Manually embedding query...\n",
            "    Manually searching Qdrant...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Chan Thong Fong\\AppData\\Local\\Temp\\ipykernel_27076\\4230959845.py:129: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
            "  results = self.client.search(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Formatting context and building prompt...\n",
            "    Sending prompt to LLM...\n",
            "\n",
            "======================================================================\n",
            "ANSWER\n",
            "======================================================================\n",
            "\n",
            "Based on the provided context from the SEC 10-Q filings for AMD, META, and AMZN, the main risk factors mentioned by tech companies include:\n",
            "\n",
            "1. **Economic and Strategic Risks (AMD)**:\n",
            "   - Highly competitive and rapidly evolving markets for products.\n",
            "   - The semiconductor industry's cyclical nature and history of severe downturns.\n",
            "   - Political, legal, and economic risks, and natural disasters affecting worldwide operations.\n",
            "   - Potential impairments of technology license purchases.\n",
            "   - Challenges in attracting and retaining key employees.\n",
            "   - Stock price volatility. (Sources 1 and 2)\n",
            "\n",
            "2. **Product Offering Risks (META)**:\n",
            "   - Challenges in adding and retaining users while maintaining user engagement.\n",
            "   - Potential loss or decrease in spending by marketers.\n",
            "   - Reduced availability of data signals affecting ad targeting and measurement.\n",
            "   - Issues with mobile operating systems and changes in partnerships with system partners. (Source 3)\n",
            "\n",
            "3. **Design, Manufacturing, and Supply Chain Risks (META)**:\n",
            "   - Risks related to design, manufacturing, and supply chain management of consumer hardware products, including potential quality issues. (Source 5)\n",
            "\n",
            "4. **Acquisition and Investment Risks (AMZN)**:\n",
            "   - Difficulties in implementing appropriate controls and policies at acquired companies.\n",
            "   - Risks associated with acquired businesses, which may differ or be more significant than existing businesses.\n",
            "   - Potential unknown liabilities from acquisitions or investments.\n",
            "   - Additional risks from foreign transactions involving integration across different cultures and regulatory environments. (Source 4)\n",
            "\n",
            "======================================================================\n",
            "SOURCES (5 chunks)\n",
            "======================================================================\n",
            "\n",
            "1. ADVANCED MICRO DEVICES INC (AMD) - ITEM 1A\n",
            "   Filing Date: 2025-11-05\n",
            "   Relevance Score: 0.7040\n",
            "\n",
            "2. ADVANCED MICRO DEVICES INC (AMD) - ITEM 1A\n",
            "   Filing Date: 2025-11-05\n",
            "   Relevance Score: 0.6638\n",
            "\n",
            "3. Meta Platforms, Inc. (META) - ITEM 1A\n",
            "   Filing Date: 2025-10-30\n",
            "   Relevance Score: 0.6379\n",
            "\n",
            "4. AMAZON COM INC (AMZN) - ITEM 1A\n",
            "   Filing Date: 2025-10-31\n",
            "   Relevance Score: 0.6292\n",
            "\n",
            "5. Meta Platforms, Inc. (META) - ITEM 1A\n",
            "   Filing Date: 2025-10-30\n",
            "   Relevance Score: 0.6172\n",
            "\n",
            "======================================================================\\n\n",
            "\n",
            " Processing query with Manual Engine: 'What risks did Apple disclose in their latest 10-Q?'\n",
            "    Manually embedding query...\n",
            "    Applying ticker filter: AAPL\n",
            "    Manually searching Qdrant...\n",
            "    Formatting context and building prompt...\n",
            "    Sending prompt to LLM...\n",
            "\n",
            "======================================================================\n",
            "ANSWER\n",
            "======================================================================\n",
            "\n",
            "The provided context does not specify the exact risks disclosed by Apple in their latest 10-Q. However, it is mentioned that for a detailed discussion of Apple's exposure to market risk, one should refer to the market risk disclosures in Part II, Item 7A of the 2024 Form 10-K (Source 5). Additionally, it is indicated that gross margins are subject to volatility and downward pressure, as discussed in the \"Risk Factors\" section of Part II, Item 1A of the Form 10-Q, although specific risks are not listed in the provided context (Source 3).\n",
            "\n",
            "======================================================================\n",
            "SOURCES (5 chunks)\n",
            "======================================================================\n",
            "\n",
            "1. Apple Inc. (AAPL) - ITEM 2\n",
            "   Filing Date: 2025-08-01\n",
            "   Relevance Score: 0.5772\n",
            "\n",
            "2. Apple Inc. (AAPL) - ITEM 1\n",
            "   Filing Date: 2025-08-01\n",
            "   Relevance Score: 0.5732\n",
            "\n",
            "3. Apple Inc. (AAPL) - ITEM 2\n",
            "   Filing Date: 2025-08-01\n",
            "   Relevance Score: 0.5631\n",
            "\n",
            "4. Apple Inc. (AAPL) - ITEM 5\n",
            "   Filing Date: 2025-08-01\n",
            "   Relevance Score: 0.5606\n",
            "\n",
            "5. Apple Inc. (AAPL) - ITEM 3\n",
            "   Filing Date: 2025-08-01\n",
            "   Relevance Score: 0.5535\n",
            "\n",
            "======================================================================\\n\n",
            "\n",
            " Processing query with Manual Engine: 'Compare the revenue trends of NVIDIA and AMD'\n",
            "    Manually embedding query...\n",
            "    Manually searching Qdrant...\n",
            "    Formatting context and building prompt...\n",
            "    Sending prompt to LLM...\n",
            "\n",
            "======================================================================\n",
            "ANSWER\n",
            "======================================================================\n",
            "\n",
            "Based on the provided context, AMD has experienced a notable revenue trend where their net revenue for the three months ended September 27, 2025, was $9.2 billion, which signifies a 36% increase compared to the prior year period. This growth was mainly driven by strong demand in the Client and Gaming segments, particularly for AMD Ryzen processors, semi-custom game console SoCs, Radeon gaming GPUs, and also in the Data Center segment due to demand for AMD EPYC processors and AMD Instinct MI350 Series GPUs (Source 1 - AMD, ITEM 2).\n",
            "\n",
            "For NVIDIA, the context does not provide explicit figures or percentage changes in revenue for a similar timeframe. However, it does mention a concentration of revenue with diverse types of direct and indirect customers, including significant segments like CSPs and consumer internet companies (Source 2 - NVDA, ITEM 2; Source 3 - NVDA, ITEM 1).\n",
            "\n",
            "Thus, while AMD's revenue trend shows a clear 36% year-over-year growth for the specified quarter, specific revenue changes or trends for NVIDIA in the comparable period are not detailed in the provided context.\n",
            "\n",
            "======================================================================\n",
            "SOURCES (5 chunks)\n",
            "======================================================================\n",
            "\n",
            "1. ADVANCED MICRO DEVICES INC (AMD) - ITEM 2\n",
            "   Filing Date: 2025-11-05\n",
            "   Relevance Score: 0.6258\n",
            "\n",
            "2. NVIDIA CORP (NVDA) - ITEM 2\n",
            "   Filing Date: 2025-08-27\n",
            "   Relevance Score: 0.5972\n",
            "\n",
            "3. NVIDIA CORP (NVDA) - ITEM 1\n",
            "   Filing Date: 2025-08-27\n",
            "   Relevance Score: 0.5922\n",
            "\n",
            "4. NVIDIA CORP (NVDA) - ITEM 1\n",
            "   Filing Date: 2025-08-27\n",
            "   Relevance Score: 0.5783\n",
            "\n",
            "5. ADVANCED MICRO DEVICES INC (AMD) - ITEM 1A\n",
            "   Filing Date: 2025-11-05\n",
            "   Relevance Score: 0.5747\n",
            "\n",
            "======================================================================\\n\n",
            "\n",
            " Processing query with Manual Engine: 'What was Tesla's R&D spending in the latest quarter?'\n",
            "    Manually embedding query...\n",
            "    Manually searching Qdrant...\n",
            "    Formatting context and building prompt...\n",
            "    Sending prompt to LLM...\n",
            "\n",
            "======================================================================\n",
            "ANSWER\n",
            "======================================================================\n",
            "\n",
            "The specific R&D spending amount for Tesla in the latest quarter is not provided in the context. However, according to Source 1, R&D expenses increased by $591 million, or 57%, in the three months ended September 30, 2025, compared to the same period in 2024. Additionally, in Source 4, it is noted that R&D expenses as a percentage of revenue increased from 4% to 6% in the latest quarter compared to the previous year's corresponding quarter. The exact dollar amount for the latest quarter's R&D spending is not specified in the provided context.\n",
            "\n",
            "======================================================================\n",
            "SOURCES (5 chunks)\n",
            "======================================================================\n",
            "\n",
            "1. Tesla, Inc. (TSLA) - ITEM 2\n",
            "   Filing Date: 2025-10-23\n",
            "   Relevance Score: 0.5687\n",
            "\n",
            "2. Alphabet Inc. (GOOGL) - ITEM 2\n",
            "   Filing Date: 2025-10-30\n",
            "   Relevance Score: 0.5300\n",
            "\n",
            "3. Tesla, Inc. (TSLA) - ITEM 2\n",
            "   Filing Date: 2025-10-23\n",
            "   Relevance Score: 0.5152\n",
            "\n",
            "4. Tesla, Inc. (TSLA) - ITEM 2\n",
            "   Filing Date: 2025-10-23\n",
            "   Relevance Score: 0.4850\n",
            "\n",
            "5. Alphabet Inc. (GOOGL) - ITEM 2\n",
            "   Filing Date: 2025-10-30\n",
            "   Relevance Score: 0.4646\n",
            "\n",
            "======================================================================\\n\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# USAGE EXAMPLE\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # ========================================================================\n",
        "    # INITIALIZE PIPELINE\n",
        "    # ========================================================================\n",
        "    pipeline = SECFilingRAGPipeline()\n",
        "\n",
        "    # ========================================================================\n",
        "    # LOAD AND INDEX FILINGS\n",
        "    # ========================================================================\n",
        "    # This will load the latest 10-Q for each company and index them\n",
        "\n",
        "    pipeline.load_and_index_filings(num_filings_per_ticker=4)\n",
        "\n",
        "    # ========================================================================\n",
        "    # QUERY THE SYSTEM\n",
        "    # ========================================================================\n",
        "\n",
        "    # Example 1: General question\n",
        "    pipeline.query(\"What are the main risk factors mentioned by tech companies?\")\n",
        "\n",
        "    # Example 2: Company-specific question\n",
        "    pipeline.query(\n",
        "        \"What risks did Apple disclose in their latest 10-Q?\",\n",
        "        ticker_filter=\"AAPL\"\n",
        "    )\n",
        "\n",
        "    # Example 3: Comparative question\n",
        "    pipeline.query(\"Compare the revenue trends of NVIDIA and AMD\")\n",
        "\n",
        "    # Example 4: Specific metric question\n",
        "    pipeline.query(\"What was Tesla's R&D spending in the latest quarter?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0aa8ad26",
      "metadata": {
        "id": "0aa8ad26"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
