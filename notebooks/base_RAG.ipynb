{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7d3f8234",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests==2.32.5 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r requirements.txt (line 2)) (2.32.5)\n",
            "Requirement already satisfied: beautifulsoup4==4.12.3 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r requirements.txt (line 3)) (4.12.3)\n",
            "Requirement already satisfied: lxml==5.3.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r requirements.txt (line 4)) (5.3.0)\n",
            "Collecting sentence-transformers==5.0.0 (from -r requirements.txt (line 7))\n",
            "  Using cached sentence_transformers-5.0.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: qdrant-client==1.15.1 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r requirements.txt (line 10)) (1.15.1)\n",
            "Requirement already satisfied: openai==2.6.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r requirements.txt (line 13)) (2.6.0)\n",
            "Collecting langchain==1.0.5 (from -r requirements.txt (line 14))\n",
            "  Using cached langchain-1.0.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting langchain-core==1.0.4 (from -r requirements.txt (line 15))\n",
            "  Using cached langchain_core-1.0.4-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: langchain-openai==1.0.2 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r requirements.txt (line 16)) (1.0.2)\n",
            "Requirement already satisfied: langchain-qdrant==1.1.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r requirements.txt (line 17)) (1.1.0)\n",
            "Requirement already satisfied: langchain-community==0.4.1 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r requirements.txt (line 18)) (0.4.1)\n",
            "Requirement already satisfied: langchain-huggingface==1.0.1 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r requirements.txt (line 19)) (1.0.1)\n",
            "Requirement already satisfied: langchain-text-splitters==1.0.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from -r requirements.txt (line 20)) (1.0.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests==2.32.5->-r requirements.txt (line 2)) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests==2.32.5->-r requirements.txt (line 2)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests==2.32.5->-r requirements.txt (line 2)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests==2.32.5->-r requirements.txt (line 2)) (2025.8.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from beautifulsoup4==4.12.3->-r requirements.txt (line 3)) (2.8)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence-transformers==5.0.0->-r requirements.txt (line 7)) (4.55.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence-transformers==5.0.0->-r requirements.txt (line 7)) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence-transformers==5.0.0->-r requirements.txt (line 7)) (2.8.0)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence-transformers==5.0.0->-r requirements.txt (line 7)) (1.7.1)\n",
            "Requirement already satisfied: scipy in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence-transformers==5.0.0->-r requirements.txt (line 7)) (1.16.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence-transformers==5.0.0->-r requirements.txt (line 7)) (0.34.4)\n",
            "Requirement already satisfied: Pillow in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence-transformers==5.0.0->-r requirements.txt (line 7)) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence-transformers==5.0.0->-r requirements.txt (line 7)) (4.15.0)\n",
            "Requirement already satisfied: grpcio>=1.41.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from qdrant-client==1.15.1->-r requirements.txt (line 10)) (1.76.0)\n",
            "Requirement already satisfied: httpx>=0.20.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx[http2]>=0.20.0->qdrant-client==1.15.1->-r requirements.txt (line 10)) (0.28.1)\n",
            "Requirement already satisfied: numpy>=2.1.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from qdrant-client==1.15.1->-r requirements.txt (line 10)) (2.3.2)\n",
            "Requirement already satisfied: portalocker<4.0,>=2.7.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from qdrant-client==1.15.1->-r requirements.txt (line 10)) (3.2.0)\n",
            "Requirement already satisfied: protobuf>=3.20.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from qdrant-client==1.15.1->-r requirements.txt (line 10)) (6.32.1)\n",
            "Requirement already satisfied: pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from qdrant-client==1.15.1->-r requirements.txt (line 10)) (2.12.4)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai==2.6.0->-r requirements.txt (line 13)) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai==2.6.0->-r requirements.txt (line 13)) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai==2.6.0->-r requirements.txt (line 13)) (0.11.1)\n",
            "Requirement already satisfied: sniffio in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai==2.6.0->-r requirements.txt (line 13)) (1.3.1)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain==1.0.5->-r requirements.txt (line 14)) (1.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core==1.0.4->-r requirements.txt (line 15)) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core==1.0.4->-r requirements.txt (line 15)) (0.4.41)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core==1.0.4->-r requirements.txt (line 15)) (24.2)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core==1.0.4->-r requirements.txt (line 15)) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core==1.0.4->-r requirements.txt (line 15)) (9.1.2)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-openai==1.0.2->-r requirements.txt (line 16)) (0.12.0)\n",
            "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-community==0.4.1->-r requirements.txt (line 18)) (1.0.0)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-community==0.4.1->-r requirements.txt (line 18)) (2.0.42)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-community==0.4.1->-r requirements.txt (line 18)) (3.13.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-community==0.4.1->-r requirements.txt (line 18)) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-community==0.4.1->-r requirements.txt (line 18)) (2.11.0)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-community==0.4.1->-r requirements.txt (line 18)) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<1.0.0,>=0.19.1 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-huggingface==1.0.1->-r requirements.txt (line 19)) (0.21.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.4.1->-r requirements.txt (line 18)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.4.1->-r requirements.txt (line 18)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.4.1->-r requirements.txt (line 18)) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.4.1->-r requirements.txt (line 18)) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.4.1->-r requirements.txt (line 18)) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.4.1->-r requirements.txt (line 18)) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.4.1->-r requirements.txt (line 18)) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community==0.4.1->-r requirements.txt (line 18)) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community==0.4.1->-r requirements.txt (line 18)) (0.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client==1.15.1->-r requirements.txt (line 10)) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client==1.15.1->-r requirements.txt (line 10)) (0.16.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers==5.0.0->-r requirements.txt (line 7)) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers==5.0.0->-r requirements.txt (line 7)) (2025.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core==1.0.4->-r requirements.txt (line 15)) (3.0.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain==1.0.5->-r requirements.txt (line 14)) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain==1.0.5->-r requirements.txt (line 14)) (1.0.2)\n",
            "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain==1.0.5->-r requirements.txt (line 14)) (0.2.9)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain==1.0.5->-r requirements.txt (line 14)) (3.6.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain==1.0.5->-r requirements.txt (line 14)) (1.12.0)\n",
            "Requirement already satisfied: orjson>=3.10.1 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain==1.0.5->-r requirements.txt (line 14)) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core==1.0.4->-r requirements.txt (line 15)) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core==1.0.4->-r requirements.txt (line 15)) (0.25.0)\n",
            "Requirement already satisfied: pywin32>=226 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from portalocker<4.0,>=2.7.0->qdrant-client==1.15.1->-r requirements.txt (line 10)) (311)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client==1.15.1->-r requirements.txt (line 10)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client==1.15.1->-r requirements.txt (line 10)) (2.41.5)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client==1.15.1->-r requirements.txt (line 10)) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community==0.4.1->-r requirements.txt (line 18)) (1.1.1)\n",
            "Requirement already satisfied: greenlet>=1 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community==0.4.1->-r requirements.txt (line 18)) (3.2.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai==1.0.2->-r requirements.txt (line 16)) (2025.9.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers==5.0.0->-r requirements.txt (line 7)) (0.6.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community==0.4.1->-r requirements.txt (line 18)) (1.1.0)\n",
            "Requirement already satisfied: h2<5,>=3 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx[http2]>=0.20.0->qdrant-client==1.15.1->-r requirements.txt (line 10)) (4.3.0)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client==1.15.1->-r requirements.txt (line 10)) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client==1.15.1->-r requirements.txt (line 10)) (4.1.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=1.11.0->sentence-transformers==5.0.0->-r requirements.txt (line 7)) (1.14.0)\n",
            "Requirement already satisfied: networkx in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=1.11.0->sentence-transformers==5.0.0->-r requirements.txt (line 7)) (3.5)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=1.11.0->sentence-transformers==5.0.0->-r requirements.txt (line 7)) (3.1.6)\n",
            "Requirement already satisfied: setuptools in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=1.11.0->sentence-transformers==5.0.0->-r requirements.txt (line 7)) (75.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers==5.0.0->-r requirements.txt (line 7)) (1.3.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm->sentence-transformers==5.0.0->-r requirements.txt (line 7)) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers==5.0.0->-r requirements.txt (line 7)) (3.0.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn->sentence-transformers==5.0.0->-r requirements.txt (line 7)) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\chan thong fong\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn->sentence-transformers==5.0.0->-r requirements.txt (line 7)) (3.6.0)\n",
            "Using cached sentence_transformers-5.0.0-py3-none-any.whl (470 kB)\n",
            "Using cached langchain-1.0.5-py3-none-any.whl (93 kB)\n",
            "Using cached langchain_core-1.0.4-py3-none-any.whl (471 kB)\n",
            "Installing collected packages: langchain-core, sentence-transformers, langchain\n",
            "\n",
            "  Attempting uninstall: langchain-core\n",
            "\n",
            "    Found existing installation: langchain-core 1.0.3\n",
            "\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "    Uninstalling langchain-core-1.0.3:\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "      Successfully uninstalled langchain-core-1.0.3\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "  Attempting uninstall: sentence-transformers\n",
            "   ---------------------------------------- 0/3 [langchain-core]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "    Found existing installation: sentence-transformers 5.1.2\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "    Uninstalling sentence-transformers-5.1.2:\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "      Successfully uninstalled sentence-transformers-5.1.2\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "  Attempting uninstall: langchain\n",
            "   ------------- -------------------------- 1/3 [sentence-transformers]\n",
            "   -------------------------- ------------- 2/3 [langchain]\n",
            "    Found existing installation: langchain 1.0.4\n",
            "   -------------------------- ------------- 2/3 [langchain]\n",
            "    Uninstalling langchain-1.0.4:\n",
            "   -------------------------- ------------- 2/3 [langchain]\n",
            "      Successfully uninstalled langchain-1.0.4\n",
            "   -------------------------- ------------- 2/3 [langchain]\n",
            "   -------------------------- ------------- 2/3 [langchain]\n",
            "   -------------------------- ------------- 2/3 [langchain]\n",
            "   -------------------------- ------------- 2/3 [langchain]\n",
            "   -------------------------- ------------- 2/3 [langchain]\n",
            "   -------------------------- ------------- 2/3 [langchain]\n",
            "   -------------------------- ------------- 2/3 [langchain]\n",
            "   -------------------------- ------------- 2/3 [langchain]\n",
            "   -------------------------- ------------- 2/3 [langchain]\n",
            "   ---------------------------------------- 3/3 [langchain]\n",
            "\n",
            "Successfully installed langchain-1.0.5 langchain-core-1.0.4 sentence-transformers-5.0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution ~penai (C:\\Users\\Chan Thong Fong\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~penai (C:\\Users\\Chan Thong Fong\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~penai (C:\\Users\\Chan Thong Fong\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
            "\n",
            "[notice] A new release of pip is available: 25.2 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3d0b67a3",
      "metadata": {
        "id": "3d0b67a3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\nfrom google.colab import userdata\\n\\n# Load secrets from Colab and set them as environment variables\\nos.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\\nos.environ['QDRANT_URL'] = userdata.get('QDRANT_URL')\\nos.environ['QDRANT_API_KEY'] = userdata.get('QDRANT_API_KEY')\\n\""
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "SEC 10-Q Filing RAG System\n",
        "A complete pipeline for loading, processing, embedding, and querying SEC 10-Q filings\n",
        "using Qdrant vector database and OpenAI GPT-4o.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import re\n",
        "import uuid\n",
        "import time\n",
        "import requests\n",
        "import gc\n",
        "from typing import List, Dict, Tuple\n",
        "from collections import defaultdict\n",
        "from bs4 import BeautifulSoup, NavigableString\n",
        "\n",
        "# External libraries (install via pip)\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
        "from openai import OpenAI\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "'''\n",
        "from google.colab import userdata\n",
        "\n",
        "# Load secrets from Colab and set them as environment variables\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "os.environ['QDRANT_URL'] = userdata.get('QDRANT_URL')\n",
        "os.environ['QDRANT_API_KEY'] = userdata.get('QDRANT_API_KEY')\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f3255ddf",
      "metadata": {
        "id": "f3255ddf"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PART 1: CONFIGURATION & SETUP\n",
        "# ============================================================================\n",
        "\n",
        "class Config:\n",
        "    \"\"\"Configuration class for API keys and model settings\"\"\"\n",
        "\n",
        "    # API Keys - SET THESE BEFORE RUNNING\n",
        "    QDRANT_URL = os.getenv(\"QDRANT_URL\")\n",
        "    QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")\n",
        "    OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "    # SEC EDGAR Configuration\n",
        "    SEC_HEADERS = {'User-Agent': 'SEC10Q-RAG-System research@example.com'}\n",
        "    CIK_MAP_URL = 'https://www.sec.gov/files/company_tickers.json'\n",
        "\n",
        "    # Model Configuration\n",
        "    EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"  # 384 dimensions\n",
        "    LLM_MODEL = \"gpt-4o\"\n",
        "\n",
        "    # Collection Configuration\n",
        "    COLLECTION_NAME = \"sec_filings_10q\"\n",
        "    VECTOR_SIZE = 384  # Dimension for all-MiniLM-L6-v2\n",
        "\n",
        "    # Chunking Configuration\n",
        "    CHUNK_SIZE = 800  # Characters per chunk\n",
        "    CHUNK_OVERLAP = 200  # Overlap between chunks\n",
        "\n",
        "    # Retrieval Configuration\n",
        "    TOP_K = 5  # Number of chunks to retrieve\n",
        "\n",
        "    # Company Tickers\n",
        "    TICKERS = ['NVDA', 'AAPL', 'MSFT', 'AMZN', 'META', 'GOOGL', 'TSLA', 'ORCL', 'JPM', 'AMD']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "bfb2dfc0",
      "metadata": {
        "id": "bfb2dfc0"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PART 2: DOCUMENT LOADING\n",
        "# ============================================================================\n",
        "\n",
        "class SECDocumentLoader:\n",
        "    \"\"\"Handles fetching and parsing of SEC 10-Q filings\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def get_recent_10q_metadata(ticker: str, num_filings: int = 4) -> List[Dict[str, str]]:\n",
        "        \"\"\"\n",
        "        Fetches the metadata (links, dates, etc.) for the latest N 10-Q filings.\n",
        "        Does NOT fetch the actual HTML content.\n",
        "\n",
        "        Args:\n",
        "            ticker: The company ticker (e.g., 'AAPL')\n",
        "            num_filings: The number of recent 10-Q filings to fetch\n",
        "\n",
        "        Returns:\n",
        "            List of metadata dictionaries\n",
        "        \"\"\"\n",
        "        print(f\"  → Fetching CIK for ticker: {ticker}...\")\n",
        "\n",
        "        # Get CIK mapping\n",
        "        response = requests.get(Config.CIK_MAP_URL, headers=Config.SEC_HEADERS)\n",
        "        response.raise_for_status()\n",
        "        company_data = response.json()\n",
        "\n",
        "        # Find CIK\n",
        "        cik = None\n",
        "        company_name = None\n",
        "        for company in company_data.values():\n",
        "            if company['ticker'] == ticker.upper():\n",
        "                cik = str(company['cik_str']).zfill(10)\n",
        "                company_name = company['title']\n",
        "                break\n",
        "\n",
        "        if not cik:\n",
        "            raise ValueError(f\"Ticker '{ticker}' not found in SEC CIK mapping.\")\n",
        "\n",
        "        print(f\"  → Found CIK: {cik} ({company_name})\")\n",
        "\n",
        "        # Fetch submission history\n",
        "        submissions_url = f\"https://data.sec.gov/submissions/CIK{cik}.json\"\n",
        "        time.sleep(0.1)  # Rate limiting\n",
        "        response = requests.get(submissions_url, headers=Config.SEC_HEADERS)\n",
        "        response.raise_for_status()\n",
        "        submissions = response.json()\n",
        "\n",
        "        # Find latest N 10-Q filings metadata\n",
        "        filings_metadata = []\n",
        "        for i, form in enumerate(submissions['filings']['recent']['form']):\n",
        "            if form == '10-Q':\n",
        "                accession_number = submissions['filings']['recent']['accessionNumber'][i]\n",
        "                primary_document = submissions['filings']['recent']['primaryDocument'][i]\n",
        "                filing_date = submissions['filings']['recent']['filingDate'][i]\n",
        "\n",
        "                accession_number_clean = accession_number.replace('-', '')\n",
        "\n",
        "                # Construct the filing URL\n",
        "                filing_url = (\n",
        "                    f\"https://www.sec.gov/Archives/edgar/data/{cik}/\"\n",
        "                    f\"{accession_number_clean}/{primary_document}\"\n",
        "                )\n",
        "\n",
        "                metadata = {\n",
        "                    'ticker': ticker.upper(),\n",
        "                    'company_name': company_name,\n",
        "                    'filing_date': filing_date,\n",
        "                    'cik': cik,\n",
        "                    'filing_url': filing_url\n",
        "                }\n",
        "                filings_metadata.append(metadata)\n",
        "\n",
        "                if len(filings_metadata) >= num_filings:\n",
        "                    break\n",
        "\n",
        "        if not filings_metadata:\n",
        "            raise ValueError(f\"No recent 10-Q filings found for ticker '{ticker}'.\")\n",
        "\n",
        "        print(f\"  → Found {len(filings_metadata)} recent 10-Q filing metadata entries.\")\n",
        "        return filings_metadata\n",
        "\n",
        "    @staticmethod\n",
        "    def get_filing_html(filing_url: str) -> str:\n",
        "        \"\"\"Fetches the HTML content for a single filing URL.\"\"\"\n",
        "        time.sleep(0.1)  # Rate limiting\n",
        "        response = requests.get(filing_url, headers=Config.SEC_HEADERS)\n",
        "        response.raise_for_status()\n",
        "        return response.text\n",
        "\n",
        "    @staticmethod\n",
        "    def _normalize_header_text(text: str) -> str:\n",
        "        \"\"\"Normalizes header text to standard format\"\"\"\n",
        "        text = text.strip().upper()\n",
        "\n",
        "        # Match \"PART I\" or \"PART II\"\n",
        "        part_match = re.search(r'^\\s*(PART\\s+I{1,2})', text)\n",
        "        if part_match:\n",
        "            return re.sub(r'\\s+', ' ', part_match.group(1))\n",
        "\n",
        "        # Match \"ITEM 1\", \"ITEM 1A\", etc.\n",
        "        item_match = re.search(r'^\\s*(ITEM\\s+\\d[A-Z]?)', text)\n",
        "        if item_match:\n",
        "            return re.sub(r'\\s+', ' ', item_match.group(1))\n",
        "\n",
        "        return None\n",
        "\n",
        "    @staticmethod\n",
        "    def _parse_html_table(table_tag) -> str:\n",
        "        \"\"\"Converts HTML table to Markdown format\"\"\"\n",
        "        markdown_rows = []\n",
        "\n",
        "        for tr in table_tag.find_all('tr'):\n",
        "            cells = [\" \".join(cell.get_text(strip=True).split())\n",
        "                    for cell in tr.find_all(['td', 'th'])]\n",
        "            if any(cells):\n",
        "                markdown_rows.append(cells)\n",
        "\n",
        "        if not markdown_rows:\n",
        "            return \"\"\n",
        "\n",
        "        md_output = []\n",
        "        header = markdown_rows[0]\n",
        "        md_output.append(\"| \" + \" | \".join(header) + \" |\")\n",
        "        md_output.append(\"| \" + \" | \".join(['---'] * len(header)) + \" |\")\n",
        "\n",
        "        for row in markdown_rows[1:]:\n",
        "            while len(row) < len(header):\n",
        "                row.append(\"\")\n",
        "            row = row[:len(header)]\n",
        "            md_output.append(\"| \" + \" | \".join(row) + \" |\")\n",
        "\n",
        "        return \"\\n\" + \"\\n\".join(md_output) + \"\\n\"\n",
        "\n",
        "    @classmethod\n",
        "    def parse_10q(cls, html_content: str) -> Dict:\n",
        "        \"\"\"Parses HTML content into structured dictionary\"\"\"\n",
        "        # --- KEY CHANGE ---\n",
        "        # Use 'lxml' for better memory efficiency\n",
        "        soup = BeautifulSoup(html_content, 'lxml')\n",
        "\n",
        "        potential_headers = soup.find_all(['p', 'b', 'strong', 'div'])\n",
        "\n",
        "        doc_headers = []\n",
        "        for header in potential_headers:\n",
        "            text = header.get_text(strip=True)\n",
        "            if len(text) > 100:\n",
        "                continue\n",
        "\n",
        "            normalized_key = cls._normalize_header_text(text)\n",
        "            if normalized_key:\n",
        "                if not header.find_parent('a'):\n",
        "                    doc_headers.append({'tag': header, 'key': normalized_key})\n",
        "\n",
        "        if not doc_headers:\n",
        "            return {}\n",
        "\n",
        "        parsed_data = defaultdict(lambda: defaultdict(str))\n",
        "        current_part_key = None\n",
        "\n",
        "        for i, header_info in enumerate(doc_headers):\n",
        "            current_key = header_info['key']\n",
        "\n",
        "            if 'PART' in current_key:\n",
        "                current_part_key = current_key\n",
        "                continue\n",
        "\n",
        "            if 'ITEM' in current_key:\n",
        "                if not current_part_key:\n",
        "                    current_part_key = \"PART I\"\n",
        "\n",
        "                start_node = header_info['tag']\n",
        "                end_node = doc_headers[i + 1]['tag'] if i + 1 < len(doc_headers) else None\n",
        "\n",
        "                content_parts = []\n",
        "                element = start_node.next_element\n",
        "\n",
        "                while element and element != end_node:\n",
        "                    if isinstance(element, NavigableString):\n",
        "                        if not element.find_parent('table'):\n",
        "                            text = element.strip()\n",
        "                            if text:\n",
        "                                content_parts.append(text)\n",
        "                    elif element.name == 'table':\n",
        "                        if not element.find_parent('table'):\n",
        "                            table_markdown = cls._parse_html_table(element)\n",
        "                            if table_markdown:\n",
        "                                content_parts.append(table_markdown)\n",
        "\n",
        "                    element = element.next_element\n",
        "\n",
        "                full_content = \"\\n\".join(content_parts)\n",
        "                clean_content = re.sub(r'\\n{3,}', '\\n\\n', full_content).strip()\n",
        "\n",
        "                parsed_data[current_part_key][current_key] = clean_content\n",
        "\n",
        "        return {part: dict(items) for part, items in parsed_data.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2cbff8d1",
      "metadata": {
        "id": "2cbff8d1"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PART 3: TEXT CHUNKING & EMBEDDING\n",
        "# ============================================================================\n",
        "import uuid\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "class DocumentProcessor:\n",
        "    \"\"\"\n",
        "    Processes documents into chunks using LangChain's splitter\n",
        "    and then embeds them in batches.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embedding_model_name: str = Config.EMBEDDING_MODEL):\n",
        "        \"\"\"Initialize with embedding model and LangChain text splitter\"\"\"\n",
        "        print(f\"\\n Loading embedding model: {embedding_model_name}\")\n",
        "        self.model = SentenceTransformer(embedding_model_name)\n",
        "        print(f\"   ✓ Model loaded (dimension: {self.model.get_sentence_embedding_dimension()})\")\n",
        "\n",
        "        # Initialize LangChain's splitter\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=Config.CHUNK_SIZE,\n",
        "            chunk_overlap=Config.CHUNK_OVERLAP,\n",
        "            length_function=len,\n",
        "            add_start_index=False, # Simpler metadata\n",
        "        )\n",
        "        print(f\"   ✓ Initialized RecursiveCharacterTextSplitter (chunk: {Config.CHUNK_SIZE}, overlap: {Config.CHUNK_OVERLAP})\")\n",
        "\n",
        "    def generate_document_chunks(self, parsed_data: Dict, metadata: Dict,\n",
        "                                 embed_batch_size: int = 1024):\n",
        "        \"\"\"\n",
        "        Processes parsed 10-Q data using LangChain's splitter,\n",
        "        then YIELDS chunk points one by one after batch-embedding.\n",
        "        \"\"\"\n",
        "\n",
        "        # Convert the parsed dict into a list of LangChain Document objects\n",
        "        all_docs = []\n",
        "        for part, items in parsed_data.items():\n",
        "            for item, content in items.items():\n",
        "                if not content:\n",
        "                    continue\n",
        "\n",
        "                # Create a metadata dict for *this specific document*\n",
        "                # before it gets chunked.\n",
        "                doc_metadata = {\n",
        "                    'ticker': metadata['ticker'],\n",
        "                    'company_name': metadata['company_name'],\n",
        "                    'filing_date': metadata['filing_date'],\n",
        "                    'filing_url': metadata['filing_url'],\n",
        "                    'part': part,\n",
        "                    'item': item\n",
        "                }\n",
        "\n",
        "                doc = Document(\n",
        "                    page_content=content,\n",
        "                    metadata=doc_metadata\n",
        "                )\n",
        "                all_docs.append(doc)\n",
        "\n",
        "        if not all_docs:\n",
        "            return # Stop the generator\n",
        "\n",
        "        # Split all documents at once using the LangChain splitter\n",
        "        print(f\"     → Splitting {len(all_docs)} high-level 'Items' into smaller chunks...\")\n",
        "        chunked_docs = self.text_splitter.split_documents(all_docs)\n",
        "        print(f\"     → Generated {len(chunked_docs)} chunks\")\n",
        "\n",
        "        text_batch = []\n",
        "        metadata_batch = []\n",
        "\n",
        "        # Consume the list one chunk at a time\n",
        "        for chunk in chunked_docs:\n",
        "            text_batch.append(chunk.page_content)\n",
        "            # The splitter automatically copies metadata to each chunk\n",
        "            metadata_batch.append(chunk.metadata)\n",
        "\n",
        "            # If batch is full, process it\n",
        "            if len(text_batch) >= embed_batch_size:\n",
        "                # 1. Embed the entire batch in one call\n",
        "                embeddings = self.model.encode(text_batch, show_progress_bar=False)\n",
        "\n",
        "                # 2. Yield each point from the processed batch\n",
        "                for txt, emb, meta in zip(text_batch, embeddings, metadata_batch):\n",
        "                    # The metadata (meta) already contains everything\n",
        "                    # from the doc_metadata we built above\n",
        "                    payload = {\n",
        "                        'text': txt,\n",
        "                        **meta # Unpack all metadata keys (ticker, item, part, etc.)\n",
        "                    }\n",
        "                    yield PointStruct(\n",
        "                        id=str(uuid.uuid4()),\n",
        "                        vector=emb.tolist(),\n",
        "                        payload=payload\n",
        "                    )\n",
        "\n",
        "                # 3. Reset the batch\n",
        "                text_batch = []\n",
        "                metadata_batch = []\n",
        "\n",
        "        if text_batch:\n",
        "            # 1. Embed the final batch\n",
        "            embeddings = self.model.encode(text_batch, show_progress_bar=False)\n",
        "\n",
        "            # 2. Yield each point\n",
        "            for txt, emb, meta in zip(text_batch, embeddings, metadata_batch):\n",
        "                payload = {\n",
        "                    'text': txt,\n",
        "                    **meta\n",
        "                }\n",
        "                yield PointStruct(\n",
        "                    id=str(uuid.uuid4()),\n",
        "                    vector=emb.tolist(),\n",
        "                    payload=payload\n",
        "                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "80bed17c",
      "metadata": {
        "id": "80bed17c"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PART 4: QDRANT VECTOR DATABASE\n",
        "# ============================================================================\n",
        "\n",
        "from qdrant_client import models\n",
        "\n",
        "class QdrantManager:\n",
        "    \"\"\"Manages Qdrant vector database operations\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize Qdrant client\"\"\"\n",
        "        print(f\"\\nConnecting to Qdrant Cloud...\")\n",
        "        self.client = QdrantClient(\n",
        "            url=Config.QDRANT_URL,\n",
        "            api_key=Config.QDRANT_API_KEY\n",
        "        )\n",
        "        print(f\"   ✓ Connected to Qdrant\")\n",
        "\n",
        "    def create_collection(self, collection_name: str = Config.COLLECTION_NAME,\n",
        "                         vector_size: int = Config.VECTOR_SIZE):\n",
        "        \"\"\"Create or recreate collection AND set up payload indexes\"\"\"\n",
        "        print(f\"\\n Setting up collection: {collection_name}\")\n",
        "\n",
        "        # Check if collection exists\n",
        "        collections = self.client.get_collections().collections\n",
        "        exists = any(col.name == collection_name for col in collections)\n",
        "\n",
        "        if exists:\n",
        "            print(f\"   ⚠ Collection exists, recreating...\")\n",
        "            self.client.delete_collection(collection_name)\n",
        "\n",
        "        # Create collection\n",
        "        self.client.create_collection(\n",
        "            collection_name=collection_name,\n",
        "            vectors_config=models.VectorParams( # Use models.VectorParams\n",
        "                size=vector_size,\n",
        "                distance=models.Distance.COSINE # Use models.Distance\n",
        "            )\n",
        "        )\n",
        "        print(f\"   ✓ Collection created\")\n",
        "\n",
        "        print(f\"   → Creating payload index for 'ticker'...\")\n",
        "        self.client.create_payload_index(\n",
        "            collection_name=collection_name,\n",
        "            field_name=\"ticker\",\n",
        "            field_schema=models.PayloadSchemaType.KEYWORD\n",
        "        )\n",
        "        print(f\"   → Creating payload index for 'item'...\")\n",
        "        self.client.create_payload_index(\n",
        "            collection_name=collection_name,\n",
        "            field_name=\"item\",\n",
        "            field_schema=models.PayloadSchemaType.KEYWORD\n",
        "        )\n",
        "        print(f\"   ✓ Payload indexes created.\")\n",
        "\n",
        "    def upsert_documents(self, points_generator,\n",
        "                        collection_name: str = Config.COLLECTION_NAME,\n",
        "                        batch_size: int = 2048) -> int:\n",
        "        \"\"\"\n",
        "        Uploads document chunks from a generator in batches.\n",
        "\n",
        "        Args:\n",
        "            points_generator: A generator that yields PointStructs\n",
        "            collection_name: Name of the collection\n",
        "            batch_size: Number of points to upload at once\n",
        "\n",
        "        Returns:\n",
        "            Total number of chunks uploaded\n",
        "        \"\"\"\n",
        "        print(f\" Uploading chunks to Qdrant in batches of {batch_size}...\")\n",
        "\n",
        "        batch = []\n",
        "        count = 0\n",
        "\n",
        "        for point in points_generator:\n",
        "            batch.append(point)\n",
        "\n",
        "            if len(batch) >= batch_size:\n",
        "                self.client.upsert(\n",
        "                    collection_name=collection_name,\n",
        "                    points=batch,\n",
        "                    wait=False # Added for speed\n",
        "                )\n",
        "                count += len(batch)\n",
        "                print(f\"     → Uploaded {count} chunks so far...\")\n",
        "                batch = [] # Reset batch\n",
        "\n",
        "        # Upload any remaining points\n",
        "        if batch:\n",
        "            self.client.upsert(\n",
        "                collection_name=collection_name,\n",
        "                points=batch,\n",
        "                wait=False # Added for speed\n",
        "            )\n",
        "            count += len(batch)\n",
        "\n",
        "        print(f\"  ✓ All chunks uploaded for this document. Total: {count}\")\n",
        "        return count\n",
        "\n",
        "    def search(self, query_vector: List[float],\n",
        "              collection_name: str = Config.COLLECTION_NAME,\n",
        "              limit: int = Config.TOP_K,\n",
        "              filter_dict: Dict = None) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Search for similar documents\n",
        "\n",
        "        Args:\n",
        "            query_vector: Embedded query vector\n",
        "            collection_name: Name of the collection\n",
        "            limit: Number of results to return\n",
        "            filter_dict: Optional filter (e.g., {\"ticker\": \"AAPL\"})\n",
        "\n",
        "        Returns:\n",
        "            List of search results with scores and payloads\n",
        "        \"\"\"\n",
        "\n",
        "        qdrant_filter = None\n",
        "        if filter_dict:\n",
        "            qdrant_filter = models.Filter(\n",
        "                must=[\n",
        "                    models.FieldCondition(\n",
        "                        key=key,\n",
        "                        match=models.MatchValue(value=value)\n",
        "                    )\n",
        "                    for key, value in filter_dict.items()\n",
        "                ]\n",
        "            )\n",
        "\n",
        "        results = self.client.search(\n",
        "            collection_name=collection_name,\n",
        "            query_vector=query_vector,\n",
        "            limit=limit,\n",
        "            query_filter=qdrant_filter,\n",
        "            with_payload=True\n",
        "        )\n",
        "\n",
        "        return [\n",
        "            {\n",
        "                'score': result.score,\n",
        "                'payload': result.payload\n",
        "            }\n",
        "            for result in results\n",
        "        ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "8471b72b",
      "metadata": {
        "id": "8471b72b"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PART 5: RAG QUERY PIPELINE\n",
        "# ============================================================================\n",
        "import os\n",
        "from openai import OpenAI\n",
        "from qdrant_client import models\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "\n",
        "class ManualRAGEngine:\n",
        "    \"\"\"\n",
        "    This is the manual RAG query engine that replaces the buggy\n",
        "    LangChainRAGEngine. It performs the RAG steps manually\n",
        "    using the components we've already built.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, document_processor: 'DocumentProcessor', qdrant_manager: 'QdrantManager'):\n",
        "        \"\"\"\n",
        "        Initialize the engine with the processor (for embeddings)\n",
        "        and the manager (for search).\n",
        "        \"\"\"\n",
        "        print(\"\\n Initializing Manual RAG Query Engine...\")\n",
        "\n",
        "        # 1. Get the embedding model from the document processor\n",
        "        #    (the one that is already loaded)\n",
        "        self.embedding_model = document_processor.model\n",
        "        print(\"   ✓ Using existing embedding model from DocumentProcessor\")\n",
        "\n",
        "        # 2. Get the Qdrant client from the Qdrant manager\n",
        "        self.qdrant_manager = qdrant_manager\n",
        "        print(\"   ✓ Using existing QdrantManager for search\")\n",
        "\n",
        "        # 3. Initialize the LLM\n",
        "        self.llm = ChatOpenAI(\n",
        "            model=Config.LLM_MODEL,\n",
        "            api_key=Config.OPENAI_API_KEY\n",
        "        )\n",
        "        print(\"   ✓ Initialized ChatOpenAI LLM\")\n",
        "\n",
        "        # 4. Create a prompt template (we can still use this part)\n",
        "        template = \"\"\"You are a helpful financial analyst assistant. Your role is to answer questions about SEC 10-Q filings based ONLY on the provided context.\n",
        "- Base your answer strictly on the provided context from SEC filings\n",
        "- Cite specific sections (e.g., \"According to Item 1A...\") when referencing information\n",
        "- If the answer is not in the context, clearly state that\n",
        "\n",
        "Context:\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "\n",
        "Question: {input}\n",
        "\n",
        "Answer:\"\"\"\n",
        "        self.prompt = ChatPromptTemplate.from_template(template)\n",
        "        print(\"   ✓ Manual RAG Engine ready.\")\n",
        "\n",
        "\n",
        "    def _format_context(self, search_results: List[Dict]) -> str:\n",
        "        \"\"\"Helper function to format the retrieved contexts\"\"\"\n",
        "        context_str = \"\"\n",
        "        for i, result in enumerate(search_results, 1):\n",
        "            payload = result.get('payload', {})\n",
        "            text = payload.get('text', 'No text found')\n",
        "            item = payload.get('item', 'N/A')\n",
        "            ticker = payload.get('ticker', 'N/A')\n",
        "            context_str += f\"Source {i} ({ticker} - {item}):\\n\\\"{text}\\\"\\n\\n\"\n",
        "        return context_str.strip()\n",
        "\n",
        "\n",
        "    def query(self, question: str, ticker_filter: str = None):\n",
        "        \"\"\"\n",
        "        Query the indexed filings using the manual retrieval and generation.\n",
        "        \"\"\"\n",
        "        print(f\"\\n Processing query with Manual Engine: '{question}'\")\n",
        "\n",
        "        # 1. Manually embed the query\n",
        "        print(\"   → Manually embedding query...\")\n",
        "        query_vector = self.embedding_model.encode(question).tolist()\n",
        "\n",
        "        # 2. Manually create the filter\n",
        "        filter_dict = None\n",
        "        if ticker_filter:\n",
        "            print(f\"   → Applying ticker filter: {ticker_filter}\")\n",
        "            filter_dict = {\"ticker\": ticker_filter}\n",
        "\n",
        "        # 3. Manually search Qdrant\n",
        "        print(\"   → Manually searching Qdrant...\")\n",
        "        search_results = self.qdrant_manager.search(\n",
        "            query_vector=query_vector,\n",
        "            limit=Config.TOP_K,\n",
        "            filter_dict=filter_dict\n",
        "        )\n",
        "\n",
        "        if not search_results:\n",
        "            return {'answer': 'No relevant context was found in the documents to answer this question.', 'sources': []}\n",
        "\n",
        "        # 4. Manually format the prompt\n",
        "        print(\"   → Formatting context and building prompt...\")\n",
        "        formatted_context = self._format_context(search_results)\n",
        "\n",
        "        # We use the prompt template to create the final message\n",
        "        final_prompt_message = self.prompt.format_messages(\n",
        "            context=formatted_context,\n",
        "            input=question\n",
        "        )\n",
        "\n",
        "        # 5. Manually invoke the LLM\n",
        "        print(\"   → Sending prompt to LLM...\")\n",
        "        llm_response = self.llm.invoke(final_prompt_message)\n",
        "        answer = llm_response.content\n",
        "\n",
        "        # 6. Format sources to match the expected output\n",
        "        sources = []\n",
        "        for i, result in enumerate(search_results, 1):\n",
        "            sources.append({\n",
        "                'ticker': result['payload'].get('ticker'),\n",
        "                'company': result['payload'].get('company_name'),\n",
        "                'item': result['payload'].get('item'),\n",
        "                'part': result['payload'].get('part'),\n",
        "                'filing_date': result['payload'].get('filing_date'),\n",
        "                'score': result['score'] # We get the real score now\n",
        "            })\n",
        "\n",
        "        return {\n",
        "            'answer': answer,\n",
        "            'sources': sources\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "9b12c056",
      "metadata": {
        "id": "9b12c056"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PART 6: MAIN PIPELINE ORCHESTRATOR\n",
        "# ============================================================================\n",
        "\n",
        "class SECFilingRAGPipeline:\n",
        "    \"\"\"Main pipeline orchestrator\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize all components\"\"\"\n",
        "        print(\"=\" * 70)\n",
        "        print(\"SEC 10-Q FILING RAG SYSTEM\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        # Ingestion components (these are working perfectly)\n",
        "        self.loader = SECDocumentLoader()\n",
        "        self.processor = DocumentProcessor()\n",
        "        self.qdrant_manager = QdrantManager()\n",
        "\n",
        "        # Query component (to be initialized later)\n",
        "        self.query_engine = None\n",
        "\n",
        "\n",
        "    def load_and_index_filings(self, tickers: List[str] = Config.TICKERS, num_filings_per_ticker: int = 1):\n",
        "        \"\"\"\n",
        "        Load and index 10-Q filings for multiple companies.\n",
        "        (This method is UNCHANGED and works)\n",
        "        \"\"\"\n",
        "        print(f\"\\n{'=' * 70}\")\n",
        "        print(f\"LOADING & INDEXING PHASE (Streaming)\")\n",
        "        print(f\"{'=' * 70}\")\n",
        "        print(f\"\\nProcessing {len(tickers)} companies: {', '.join(tickers)}\")\n",
        "        print(f\"(Fetching {num_filings_per_ticker} filings per company)\\n\")\n",
        "\n",
        "        # Create collection (do this once at the start)\n",
        "        self.qdrant_manager.create_collection() # Use the manager here\n",
        "\n",
        "        successful_tickers = []\n",
        "        failed_tickers = []\n",
        "        total_chunks_indexed = 0\n",
        "\n",
        "        # Process each ticker\n",
        "        for idx, ticker in enumerate( tickers, 1):\n",
        "            print(f\"\\n[{idx}/{len(tickers)}] Processing {ticker}\")\n",
        "            print(\"-\" * 70)\n",
        "\n",
        "            ticker_chunks_count = 0\n",
        "            num_filings_processed = 0\n",
        "\n",
        "            try:\n",
        "                # 1. Get METADATA (links) for N filings\n",
        "                filings_metadata_list = self.loader.get_recent_10q_metadata(ticker, num_filings=num_filings_per_ticker)\n",
        "\n",
        "                # 2. Loop through each FILING METADATA\n",
        "                for filing_metadata in filings_metadata_list:\n",
        "                    try:\n",
        "                        filing_date = filing_metadata['filing_date']\n",
        "                        filing_url = filing_metadata['filing_url']\n",
        "\n",
        "                        # 3. Download ONE filing's HTML\n",
        "                        print(f\"  → Downloading filing from: {filing_date}...\")\n",
        "                        html_content = self.loader.get_filing_html(filing_url)\n",
        "\n",
        "                        # 4. Parse ONE filing\n",
        "                        print(f\"  → Parsing 10-Q structure...\")\n",
        "                        parsed_data = self.loader.parse_10q(html_content)\n",
        "\n",
        "                        if not parsed_data:\n",
        "                            print(f\"  ⚠ Warning: No structured data parsed for {ticker} on {filing_date}\")\n",
        "                            del html_content\n",
        "                            gc.collect()\n",
        "                            continue\n",
        "\n",
        "                        # 5. Process ONE filing into a GENERATOR\n",
        "                        print(f\"  → Creating chunks and embeddings generator...\")\n",
        "                        # Use the LANGCHAIN-ENHANCED chunker from Part 3\n",
        "                        chunks_generator = self.processor.generate_document_chunks(parsed_data, filing_metadata)\n",
        "\n",
        "                        # 6. Upload ONE filing's chunks from the generator\n",
        "                        num_uploaded = self.qdrant_manager.upsert_documents(chunks_generator) # Use the manager here\n",
        "\n",
        "                        if num_uploaded > 0:\n",
        "                            ticker_chunks_count += num_uploaded\n",
        "                            total_chunks_indexed += num_uploaded\n",
        "                            num_filings_processed += 1\n",
        "\n",
        "                        # 7. MANUALLY CLEAN UP MEMORY\n",
        "                        print(f\"  → Cleaning up memory...\")\n",
        "                        del html_content\n",
        "                        del parsed_data\n",
        "                        del chunks_generator # Clear generator\n",
        "                        gc.collect() # Force garbage collection\n",
        "                        print(f\"  ✓ Memory cleaned.\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"  ✗ Error processing filing {filing_metadata.get('filing_date', 'unknown')} for {ticker}: {str(e)}\")\n",
        "                        gc.collect() # Force cleanup on error\n",
        "\n",
        "                # After all filings for this ticker are done\n",
        "                if ticker_chunks_count > 0:\n",
        "                    successful_tickers.append(ticker)\n",
        "                    print(f\"  ✓ Finished {ticker}. Total chunks: {ticker_chunks_count} across {num_filings_processed} filings\")\n",
        "                else:\n",
        "                    failed_tickers.append(ticker)\n",
        "                    print(f\"  ⚠ No chunks created for {ticker}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  ✗ Error processing {ticker} (failed to get metadata): {str(e)}\")\n",
        "                failed_tickers.append(ticker)\n",
        "\n",
        "            # Rate limiting\n",
        "            if idx < len(tickers):\n",
        "                time.sleep(0.2)\n",
        "\n",
        "        # Summary\n",
        "        print(f\"\\n{'=' * 70}\")\n",
        "        print(f\"INDEXING COMPLETE\")\n",
        "        print(f\"{'=' * 70}\")\n",
        "        print(f\"✓ Successfully processed: {len(successful_tickers)} companies\")\n",
        "        print(f\"  {', '.join(successful_tickers)}\")\n",
        "        if failed_tickers:\n",
        "            print(f\"✗ Failed: {len(failed_tickers)} companies\")\n",
        "            print(f\"  {', '.join(failed_tickers)}\")\n",
        "        print(f\"\\n Total chunks indexed: {total_chunks_indexed}\")\n",
        "        print(f\"{'=' * 70}\\n\")\n",
        "\n",
        "    def query(self, question: str, ticker_filter: str = None):\n",
        "        \"\"\"\n",
        "        Query the indexed filings using the new MANUAL engine\n",
        "        \"\"\"\n",
        "        if self.query_engine is None:\n",
        "            # Initialize the ManualRAGEngine, passing it the\n",
        "            # processor (for the model) and manager (for search)\n",
        "            self.query_engine = ManualRAGEngine(\n",
        "                document_processor=self.processor,\n",
        "                qdrant_manager=self.qdrant_manager\n",
        "            )\n",
        "\n",
        "        result = self.query_engine.query(question, ticker_filter)\n",
        "\n",
        "        # Print results (this part is the same)\n",
        "        print(f\"\\n{'=' * 70}\")\n",
        "        print(f\"ANSWER\")\n",
        "        print(f\"{'=' * 70}\")\n",
        "        print(f\"\\n{result['answer']}\\n\")\n",
        "\n",
        "        print(f\"{'=' * 70}\")\n",
        "        print(f\"SOURCES ({len(result['sources'])} chunks)\")\n",
        "        print(f\"{'=' * 70}\")\n",
        "        for i, source in enumerate(result['sources'], 1):\n",
        "            print(f\"\\n{i}. {source['company']} ({source['ticker']}) - {source['item']}\")\n",
        "            print(f\"   Filing Date: {source['filing_date']}\")\n",
        "            print(f\"   Relevance Score: {source['score']:.4f}\") # Can now show the real score\n",
        "\n",
        "        print(f\"\\n{'=' * 70}\\\\n\")\n",
        "\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "7fb9524b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fb9524b",
        "outputId": "68e39ca6-af07-4360-9353-71f56d4754e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "SEC 10-Q FILING RAG SYSTEM\n",
            "======================================================================\n",
            "\n",
            " Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
            "   ✓ Model loaded (dimension: 384)\n",
            "   ✓ Initialized RecursiveCharacterTextSplitter (chunk: 800, overlap: 200)\n",
            "\n",
            "Connecting to Qdrant Cloud...\n",
            "   ✓ Connected to Qdrant\n",
            "\n",
            "======================================================================\n",
            "LOADING & INDEXING PHASE (Streaming)\n",
            "======================================================================\n",
            "\n",
            "Processing 10 companies: NVDA, AAPL, MSFT, AMZN, META, GOOGL, TSLA, ORCL, JPM, AMD\n",
            "(Fetching 4 filings per company)\n",
            "\n",
            "\n",
            " Setting up collection: sec_filings_10q\n",
            "   ⚠ Collection exists, recreating...\n",
            "   ✓ Collection created\n",
            "   → Creating payload index for 'ticker'...\n",
            "   → Creating payload index for 'item'...\n",
            "   ✓ Payload indexes created.\n",
            "\n",
            "[1/10] Processing NVDA\n",
            "----------------------------------------------------------------------\n",
            "  → Fetching CIK for ticker: NVDA...\n",
            "  → Found CIK: 0001045810 (NVIDIA CORP)\n",
            "  → Found 4 recent 10-Q filing metadata entries.\n",
            "  → Downloading filing from: 2025-08-27...\n",
            "  → Parsing 10-Q structure...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Chan Thong Fong\\AppData\\Local\\Temp\\ipykernel_26352\\2600233645.py:140: XMLParsedAsHTMLWarning: It looks like you're using an HTML parser to parse an XML document.\n",
            "\n",
            "Assuming this really is an XML document, what you're doing might work, but you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the Python package 'lxml' installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
            "\n",
            "If you want or need to use an HTML parser on this document, you can make this warning go away by filtering it. To do that, run this code before calling the BeautifulSoup constructor:\n",
            "\n",
            "    from bs4 import XMLParsedAsHTMLWarning\n",
            "    import warnings\n",
            "\n",
            "    warnings.filterwarnings(\"ignore\", category=XMLParsedAsHTMLWarning)\n",
            "\n",
            "  soup = BeautifulSoup(html_content, 'lxml')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  → Creating chunks and embeddings generator...\n",
            " Uploading chunks to Qdrant in batches of 2048...\n",
            "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
            "     → Generated 245 chunks\n",
            "  ✓ All chunks uploaded for this document. Total: 245\n",
            "  → Cleaning up memory...\n",
            "  ✓ Memory cleaned.\n",
            "  → Downloading filing from: 2025-05-28...\n",
            "  → Parsing 10-Q structure...\n",
            "  → Creating chunks and embeddings generator...\n",
            " Uploading chunks to Qdrant in batches of 2048...\n",
            "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
            "     → Generated 245 chunks\n",
            "  ✓ All chunks uploaded for this document. Total: 245\n",
            "  → Cleaning up memory...\n",
            "  ✓ Memory cleaned.\n",
            "  → Downloading filing from: 2024-11-20...\n",
            "  → Parsing 10-Q structure...\n",
            "  → Creating chunks and embeddings generator...\n",
            " Uploading chunks to Qdrant in batches of 2048...\n",
            "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
            "     → Generated 257 chunks\n",
            "  ✓ All chunks uploaded for this document. Total: 257\n",
            "  → Cleaning up memory...\n",
            "  ✓ Memory cleaned.\n",
            "  → Downloading filing from: 2024-08-28...\n",
            "  → Parsing 10-Q structure...\n",
            "  → Creating chunks and embeddings generator...\n",
            " Uploading chunks to Qdrant in batches of 2048...\n",
            "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
            "     → Generated 271 chunks\n",
            "  ✓ All chunks uploaded for this document. Total: 271\n",
            "  → Cleaning up memory...\n",
            "  ✓ Memory cleaned.\n",
            "  ✓ Finished NVDA. Total chunks: 1018 across 4 filings\n",
            "\n",
            "[2/10] Processing AAPL\n",
            "----------------------------------------------------------------------\n",
            "  → Fetching CIK for ticker: AAPL...\n",
            "  → Found CIK: 0000320193 (Apple Inc.)\n",
            "  → Found 4 recent 10-Q filing metadata entries.\n",
            "  → Downloading filing from: 2025-08-01...\n",
            "  → Parsing 10-Q structure...\n",
            "  → Creating chunks and embeddings generator...\n",
            " Uploading chunks to Qdrant in batches of 2048...\n",
            "     → Splitting 11 high-level 'Items' into smaller chunks...\n",
            "     → Generated 116 chunks\n",
            "  ✓ All chunks uploaded for this document. Total: 116\n",
            "  → Cleaning up memory...\n",
            "  ✓ Memory cleaned.\n",
            "  → Downloading filing from: 2025-05-02...\n",
            "  → Parsing 10-Q structure...\n",
            "  → Creating chunks and embeddings generator...\n",
            " Uploading chunks to Qdrant in batches of 2048...\n",
            "     → Splitting 11 high-level 'Items' into smaller chunks...\n",
            "     → Generated 136 chunks\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     10\u001b[39m pipeline = SECFilingRAGPipeline()\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# ========================================================================\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# LOAD AND INDEX FILINGS\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# ========================================================================\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# This will load the latest 10-Q for each company and index them\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_and_index_filings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_filings_per_ticker\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# ========================================================================\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# QUERY THE SYSTEM\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# ========================================================================\u001b[39;00m\n\u001b[32m     22\u001b[39m \n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Example 1: General question\u001b[39;00m\n\u001b[32m     24\u001b[39m pipeline.query(\u001b[33m\"\u001b[39m\u001b[33mWhat are the main risk factors mentioned by tech companies?\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 79\u001b[39m, in \u001b[36mSECFilingRAGPipeline.load_and_index_filings\u001b[39m\u001b[34m(self, tickers, num_filings_per_ticker)\u001b[39m\n\u001b[32m     76\u001b[39m chunks_generator = \u001b[38;5;28mself\u001b[39m.processor.generate_document_chunks(parsed_data, filing_metadata)\n\u001b[32m     78\u001b[39m \u001b[38;5;66;03m# 6. Upload ONE filing's chunks from the generator\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m num_uploaded = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mqdrant_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupsert_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks_generator\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Use the manager here\u001b[39;00m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_uploaded > \u001b[32m0\u001b[39m:\n\u001b[32m     82\u001b[39m     ticker_chunks_count += num_uploaded\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 75\u001b[39m, in \u001b[36mQdrantManager.upsert_documents\u001b[39m\u001b[34m(self, points_generator, collection_name, batch_size)\u001b[39m\n\u001b[32m     72\u001b[39m batch = []\n\u001b[32m     73\u001b[39m count = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpoint\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpoints_generator\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m:\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 102\u001b[39m, in \u001b[36mDocumentProcessor.generate_document_chunks\u001b[39m\u001b[34m(self, parsed_data, metadata, embed_batch_size)\u001b[39m\n\u001b[32m     98\u001b[39m         metadata_batch = []\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m text_batch:\n\u001b[32m    101\u001b[39m     \u001b[38;5;66;03m# 1. Embed the final batch\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m     embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    104\u001b[39m     \u001b[38;5;66;03m# 2. Yield each point\u001b[39;00m\n\u001b[32m    105\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m txt, emb, meta \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(text_batch, embeddings, metadata_batch):\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chan Thong Fong\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chan Thong Fong\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:1094\u001b[39m, in \u001b[36mSentenceTransformer.encode\u001b[39m\u001b[34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, truncate_dim, pool, chunk_size, **kwargs)\u001b[39m\n\u001b[32m   1091\u001b[39m features.update(extra_features)\n\u001b[32m   1093\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m-> \u001b[39m\u001b[32m1094\u001b[39m     out_features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1095\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.device.type == \u001b[33m\"\u001b[39m\u001b[33mhpu\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1096\u001b[39m         out_features = copy.deepcopy(out_features)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chan Thong Fong\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:1175\u001b[39m, in \u001b[36mSentenceTransformer.forward\u001b[39m\u001b[34m(self, input, **kwargs)\u001b[39m\n\u001b[32m   1169\u001b[39m             module_kwarg_keys = \u001b[38;5;28mself\u001b[39m.module_kwargs.get(module_name, [])\n\u001b[32m   1170\u001b[39m         module_kwargs = {\n\u001b[32m   1171\u001b[39m             key: value\n\u001b[32m   1172\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs.items()\n\u001b[32m   1173\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mhasattr\u001b[39m(module, \u001b[33m\"\u001b[39m\u001b[33mforward_kwargs\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module.forward_kwargs)\n\u001b[32m   1174\u001b[39m         }\n\u001b[32m-> \u001b[39m\u001b[32m1175\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chan Thong Fong\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chan Thong Fong\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chan Thong Fong\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:261\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, features, **kwargs)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    239\u001b[39m \u001b[33;03mForward pass through the transformer model.\u001b[39;00m\n\u001b[32m    240\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    257\u001b[39m \u001b[33;03m        - 'all_layer_embeddings': If the model outputs hidden states, contains embeddings from all layers\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    259\u001b[39m trans_features = {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m features.items() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model_forward_params}\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    262\u001b[39m token_embeddings = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    263\u001b[39m features[\u001b[33m\"\u001b[39m\u001b[33mtoken_embeddings\u001b[39m\u001b[33m\"\u001b[39m] = token_embeddings\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chan Thong Fong\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chan Thong Fong\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chan Thong Fong\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1006\u001b[39m, in \u001b[36mBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m    999\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m   1000\u001b[39m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[32m   1001\u001b[39m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[32m   1002\u001b[39m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[32m   1003\u001b[39m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[32m   1004\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m-> \u001b[39m\u001b[32m1006\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1019\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1020\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chan Thong Fong\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chan Thong Fong\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chan Thong Fong\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:653\u001b[39m, in \u001b[36mBertEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m    649\u001b[39m     all_hidden_states = all_hidden_states + (hidden_states,)\n\u001b[32m    651\u001b[39m layer_head_mask = head_mask[i] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m653\u001b[39m layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001b[39;49;00m\n\u001b[32m    658\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    659\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    660\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    661\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    662\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    664\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    665\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chan Thong Fong\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chan Thong Fong\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chan Thong Fong\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chan Thong Fong\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:562\u001b[39m, in \u001b[36mBertLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, cache_position)\u001b[39m\n\u001b[32m    551\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    552\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    553\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    560\u001b[39m     cache_position: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    561\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[torch.Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m562\u001b[39m     self_attention_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    565\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    566\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    568\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    569\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    570\u001b[39m     attention_output = self_attention_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    571\u001b[39m     outputs = self_attention_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add self attentions if we output attention weights\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chan Thong Fong\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chan Thong Fong\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chan Thong Fong\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:493\u001b[39m, in \u001b[36mBertAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, past_key_value, output_attentions, cache_position)\u001b[39m\n\u001b[32m    483\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    484\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    485\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    491\u001b[39m     cache_position: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    492\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[torch.Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m     self_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    502\u001b[39m     attention_output = \u001b[38;5;28mself\u001b[39m.output(self_outputs[\u001b[32m0\u001b[39m], hidden_states)\n\u001b[32m    503\u001b[39m     outputs = (attention_output,) + self_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chan Thong Fong\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chan Thong Fong\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chan Thong Fong\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:390\u001b[39m, in \u001b[36mBertSdpaSelfAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, past_key_value, output_attentions, cache_position)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    384\u001b[39m     key_layer = (\n\u001b[32m    385\u001b[39m         \u001b[38;5;28mself\u001b[39m.key(current_states)\n\u001b[32m    386\u001b[39m         .view(bsz, -\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.num_attention_heads, \u001b[38;5;28mself\u001b[39m.attention_head_size)\n\u001b[32m    387\u001b[39m         .transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    388\u001b[39m     )\n\u001b[32m    389\u001b[39m     value_layer = (\n\u001b[32m--> \u001b[39m\u001b[32m390\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    391\u001b[39m         .view(bsz, -\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.num_attention_heads, \u001b[38;5;28mself\u001b[39m.attention_head_size)\n\u001b[32m    392\u001b[39m         .transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    393\u001b[39m     )\n\u001b[32m    395\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    396\u001b[39m         \u001b[38;5;66;03m# save all key/value_layer to cache to be re-used for fast auto-regressive generation\u001b[39;00m\n\u001b[32m    397\u001b[39m         cache_position = cache_position \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cross_attention \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chan Thong Fong\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chan Thong Fong\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chan Thong Fong\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# USAGE EXAMPLE\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # ========================================================================\n",
        "    # INITIALIZE PIPELINE\n",
        "    # ========================================================================\n",
        "    pipeline = SECFilingRAGPipeline()\n",
        "\n",
        "    # ========================================================================\n",
        "    # LOAD AND INDEX FILINGS\n",
        "    # ========================================================================\n",
        "    # This will load the latest 10-Q for each company and index them\n",
        "\n",
        "    pipeline.load_and_index_filings(num_filings_per_ticker=4)\n",
        "\n",
        "    # ========================================================================\n",
        "    # QUERY THE SYSTEM\n",
        "    # ========================================================================\n",
        "\n",
        "    # Example 1: General question\n",
        "    pipeline.query(\"What are the main risk factors mentioned by tech companies?\")\n",
        "\n",
        "    # Example 2: Company-specific question\n",
        "    pipeline.query(\n",
        "        \"What risks did Apple disclose in their latest 10-Q?\",\n",
        "        ticker_filter=\"AAPL\"\n",
        "    )\n",
        "\n",
        "    # Example 3: Comparative question\n",
        "    pipeline.query(\"Compare the revenue trends of NVIDIA and AMD\")\n",
        "\n",
        "    # Example 4: Specific metric question\n",
        "    pipeline.query(\"What was Tesla's R&D spending in the latest quarter?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0aa8ad26",
      "metadata": {
        "id": "0aa8ad26"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
