{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b96bd09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chan Thong Fong\\OneDrive - National University of Singapore\\Documents\\Y4S1\\DSA4213\\DSA4213-SEC-Fillings-Chatbot\\venv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import uuid\n",
    "import time\n",
    "import requests\n",
    "import gc\n",
    "import json\n",
    "from typing import List, Dict, Tuple\n",
    "from collections import defaultdict\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "\n",
    "# External libraries\n",
    "from dotenv import load_dotenv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient, models\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
    "from openai import OpenAI\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Imports for NER\n",
    "import spacy\n",
    "from transformers import pipeline\n",
    "\n",
    "# Suppress warnings (especially from transformers)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb1f3dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# PART 1: CONFIGURATION & SETUP\n",
    "# ===========================================================================\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Configuration class for API keys and model settings\"\"\"\n",
    "    QDRANT_URL = os.getenv(\"QDRANT_URL\")\n",
    "    QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")\n",
    "    OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "    SEC_HEADERS = {'User-Agent': 'EnhancedRAG-Project research@example.com'}\n",
    "    CIK_MAP_URL = 'https://www.sec.gov/files/company_tickers.json'\n",
    "\n",
    "    # Model Configuration\n",
    "    EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"  # 384 dimensions\n",
    "    LLM_MODEL = \"gpt-4o\"\n",
    "    NER_MODEL = \"dslim/bert-base-NER\"\n",
    "    SPACY_MODEL = \"en_core_web_lg\"\n",
    "\n",
    "    COLLECTION_NAME = \"sec_filings_10q_GOLDEN_BENCHMARK\" # New collection name\n",
    "    VECTOR_SIZE = 384\n",
    "    CHUNK_SIZE = 800\n",
    "    CHUNK_OVERLAP = 200\n",
    "    TOP_K = 5\n",
    "    TICKERS = ['NVDA', 'AAPL', 'MSFT', 'AMZN', 'META', 'GOOGL', 'TSLA', 'ORCL', 'JPM', 'AMD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "779fd8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# PART 2: DOCUMENT LOADING\n",
    "# ===========================================================================\n",
    "\n",
    "class SECDocumentLoader:\n",
    "    \"\"\"Handles fetching and parsing of SEC 10-Q filings\"\"\"\n",
    "    @staticmethod\n",
    "    def get_recent_10q_metadata(ticker: str, num_filings: int = 4) -> List[Dict[str, str]]:\n",
    "        print(f\"  → Fetching CIK for ticker: {ticker}...\")\n",
    "        response = requests.get(Config.CIK_MAP_URL, headers=Config.SEC_HEADERS)\n",
    "        response.raise_for_status()\n",
    "        company_data = response.json()\n",
    "\n",
    "        cik = None\n",
    "        company_name = None\n",
    "        for company in company_data.values():\n",
    "            if company['ticker'] == ticker.upper():\n",
    "                cik = str(company['cik_str']).zfill(10)\n",
    "                company_name = company['title']\n",
    "                break\n",
    "        if not cik:\n",
    "            raise ValueError(f\"Ticker '{ticker}' not found in SEC CIK mapping.\")\n",
    "        print(f\"  → Found CIK: {cik} ({company_name})\")\n",
    "\n",
    "        submissions_url = f\"https://data.sec.gov/submissions/CIK{cik}.json\"\n",
    "        time.sleep(0.1)\n",
    "        response = requests.get(submissions_url, headers=Config.SEC_HEADERS)\n",
    "        response.raise_for_status()\n",
    "        submissions = response.json()\n",
    "\n",
    "        filings_metadata = []\n",
    "        for i, form in enumerate(submissions['filings']['recent']['form']):\n",
    "            if form == '10-Q':\n",
    "                accession_number = submissions['filings']['recent']['accessionNumber'][i]\n",
    "                primary_document = submissions['filings']['recent']['primaryDocument'][i]\n",
    "                filing_date = submissions['filings']['recent']['filingDate'][i]\n",
    "                accession_number_clean = accession_number.replace('-', '')\n",
    "                filing_url = (\n",
    "                    f\"https://www.sec.gov/Archives/edgar/data/{cik}/\"\n",
    "                    f\"{accession_number_clean}/{primary_document}\"\n",
    "                )\n",
    "                metadata = {\n",
    "                    'ticker': ticker.upper(),\n",
    "                    'company_name': company_name,\n",
    "                    'filing_date': filing_date,\n",
    "                    'cik': cik,\n",
    "                    'filing_url': filing_url\n",
    "                }\n",
    "                filings_metadata.append(metadata)\n",
    "                if len(filings_metadata) >= num_filings:\n",
    "                    break\n",
    "        if not filings_metadata:\n",
    "            raise ValueError(f\"No recent 10-Q filings found for ticker '{ticker}'.\")\n",
    "        print(f\"  → Found {len(filings_metadata)} recent 10-Q filing metadata entries.\")\n",
    "        return filings_metadata\n",
    "\n",
    "    @staticmethod\n",
    "    def get_filing_html(filing_url: str) -> str:\n",
    "        time.sleep(0.1)\n",
    "        response = requests.get(filing_url, headers=Config.SEC_HEADERS)\n",
    "        response.raise_for_status()\n",
    "        return response.text\n",
    "\n",
    "    @staticmethod\n",
    "    def _normalize_header_text(text: str) -> str:\n",
    "        text = text.strip().upper()\n",
    "        part_match = re.search(r'^\\s*(PART\\s+I{1,2})', text)\n",
    "        if part_match:\n",
    "            return re.sub(r'\\s+', ' ', part_match.group(1))\n",
    "        item_match = re.search(r'^\\s*(ITEM\\s+\\d[A-Z]?)', text)\n",
    "        if item_match:\n",
    "            return re.sub(r'\\s+', ' ', item_match.group(1))\n",
    "        return None\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_html_table(table_tag) -> str:\n",
    "        markdown_rows = []\n",
    "        for tr in table_tag.find_all('tr'):\n",
    "            cells = [\" \".join(cell.get_text(strip=True).split())\n",
    "                    for cell in tr.find_all(['td', 'th'])]\n",
    "            if any(cells):\n",
    "                markdown_rows.append(cells)\n",
    "        if not markdown_rows: return \"\"\n",
    "        md_output = []\n",
    "        header = markdown_rows[0]\n",
    "        md_output.append(\"| \" + \" | \".join(header) + \" |\")\n",
    "        md_output.append(\"| \" + \" | \".join(['---'] * len(header)) + \" |\")\n",
    "        for row in markdown_rows[1:]:\n",
    "            while len(row) < len(header): row.append(\"\")\n",
    "            row = row[:len(header)]\n",
    "            md_output.append(\"| \" + \" | \".join(row) + \" |\")\n",
    "        return \"\\n\" + \"\\n\".join(md_output) + \"\\n\"\n",
    "\n",
    "    @classmethod\n",
    "    def parse_10q(cls, html_content: str) -> Dict:\n",
    "        # Use 'lxml' for speed and robustness\n",
    "        soup = BeautifulSoup(html_content, 'lxml')\n",
    "        potential_headers = soup.find_all(['p', 'b', 'strong', 'div'])\n",
    "        doc_headers = []\n",
    "        for header in potential_headers:\n",
    "            text = header.get_text(strip=True)\n",
    "            if len(text) > 100: continue\n",
    "            normalized_key = cls._normalize_header_text(text)\n",
    "            if normalized_key and not header.find_parent('a'):\n",
    "                doc_headers.append({'tag': header, 'key': normalized_key})\n",
    "        if not doc_headers: return {}\n",
    "        parsed_data = defaultdict(lambda: defaultdict(str))\n",
    "        current_part_key = None\n",
    "        for i, header_info in enumerate(doc_headers):\n",
    "            current_key = header_info['key']\n",
    "            if 'PART' in current_key:\n",
    "                current_part_key = current_key\n",
    "                continue\n",
    "            if 'ITEM' in current_key:\n",
    "                if not current_part_key: current_part_key = \"PART I\"\n",
    "                start_node = header_info['tag']\n",
    "                end_node = doc_headers[i + 1]['tag'] if i + 1 < len(doc_headers) else None\n",
    "                content_parts = []\n",
    "                element = start_node.next_element\n",
    "                while element and element != end_node:\n",
    "                    if isinstance(element, NavigableString):\n",
    "                        if not element.find_parent('table'):\n",
    "                            text = element.strip()\n",
    "                            if text: content_parts.append(text)\n",
    "                    elif element.name == 'table':\n",
    "                        if not element.find_parent('table'):\n",
    "                            table_markdown = cls._parse_html_table(element)\n",
    "                            if table_markdown: content_parts.append(table_markdown)\n",
    "                    element = element.next_element\n",
    "                full_content = \"\\n\".join(content_parts)\n",
    "                clean_content = re.sub(r'\\n{3,}', '\\n\\n', full_content).strip()\n",
    "                parsed_data[current_part_key][current_key] = clean_content\n",
    "        return {part: dict(items) for part, items in parsed_data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "831b48b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# PART 3: TEXT CHUNKING & EMBEDDING\n",
    "# ===========================================================================\n",
    "\n",
    "class DocumentProcessor:\n",
    "    def __init__(self, embedding_model_name: str = Config.EMBEDDING_MODEL):\n",
    "        print(f\"\\n Loading embedding model: {embedding_model_name}\")\n",
    "        self.model = SentenceTransformer(embedding_model_name)\n",
    "        print(f\"   ✓ Model loaded (dimension: {self.model.get_sentence_embedding_dimension()})\")\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=Config.CHUNK_SIZE,\n",
    "            chunk_overlap=Config.CHUNK_OVERLAP,\n",
    "            length_function=len,\n",
    "            add_start_index=False,\n",
    "        )\n",
    "        print(f\"   ✓ Initialized RecursiveCharacterTextSplitter\")\n",
    "\n",
    "    def generate_document_chunks(self, parsed_data: Dict, metadata: Dict,\n",
    "                                 embed_batch_size: int = 1024):\n",
    "        all_docs = []\n",
    "        for part, items in parsed_data.items():\n",
    "            for item, content in items.items():\n",
    "                if not content: continue\n",
    "                doc_metadata = {\n",
    "                    'ticker': metadata['ticker'],\n",
    "                    'company_name': metadata['company_name'],\n",
    "                    'filing_date': metadata['filing_date'],\n",
    "                    'filing_url': metadata['filing_url'],\n",
    "                    'part': part,\n",
    "                    'item': item\n",
    "                }\n",
    "                doc = Document(page_content=content, metadata=doc_metadata)\n",
    "                all_docs.append(doc)\n",
    "        if not all_docs: return\n",
    "        print(f\"     → Splitting {len(all_docs)} high-level 'Items' into smaller chunks...\")\n",
    "        chunked_docs = self.text_splitter.split_documents(all_docs)\n",
    "        print(f\"     → Generated {len(chunked_docs)} chunks\")\n",
    "        text_batch, metadata_batch = [], []\n",
    "        for chunk in chunked_docs:\n",
    "            text_batch.append(chunk.page_content)\n",
    "            metadata_batch.append(chunk.metadata)\n",
    "            if len(text_batch) >= embed_batch_size:\n",
    "                embeddings = self.model.encode(text_batch, show_progress_bar=False)\n",
    "                for txt, emb, meta in zip(text_batch, embeddings, metadata_batch):\n",
    "                    payload = {'text': txt, **meta}\n",
    "                    yield PointStruct(id=str(uuid.uuid4()), vector=emb.tolist(), payload=payload)\n",
    "                text_batch, metadata_batch = [], []\n",
    "        if text_batch:\n",
    "            embeddings = self.model.encode(text_batch, show_progress_bar=False)\n",
    "            for txt, emb, meta in zip(text_batch, embeddings, metadata_batch):\n",
    "                payload = {'text': txt, **meta}\n",
    "                yield PointStruct(id=str(uuid.uuid4()), vector=emb.tolist(), payload=payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7ca28ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# PART 4: QDRANT VECTOR DATABASE\n",
    "# ===========================================================================\n",
    "\n",
    "class QdrantManager:\n",
    "    def __init__(self):\n",
    "        print(f\"\\nConnecting to Qdrant Cloud...\")\n",
    "        self.client = QdrantClient(url=Config.QDRANT_URL, api_key=Config.QDRANT_API_KEY)\n",
    "        print(f\"   ✓ Connected to Qdrant\")\n",
    "\n",
    "    def create_collection(self, collection_name: str = Config.COLLECTION_NAME,\n",
    "                         vector_size: int = Config.VECTOR_SIZE):\n",
    "        print(f\"\\n Setting up collection: {collection_name}\")\n",
    "        collections = self.client.get_collections().collections\n",
    "        exists = any(col.name == collection_name for col in collections)\n",
    "        if exists:\n",
    "            print(f\"   ⚠ Collection exists, recreating...\")\n",
    "            self.client.delete_collection(collection_name)\n",
    "        self.client.create_collection(\n",
    "            collection_name=collection_name,\n",
    "            vectors_config=models.VectorParams(size=vector_size, distance=models.Distance.COSINE)\n",
    "        )\n",
    "        print(f\"   ✓ Collection created\")\n",
    "        print(f\"   → Creating payload index for 'ticker'...\")\n",
    "        self.client.create_payload_index(\n",
    "            collection_name=collection_name,\n",
    "            field_name=\"ticker\",\n",
    "            field_schema=models.PayloadSchemaType.KEYWORD\n",
    "        )\n",
    "        print(f\"   → Creating payload index for 'item'...\")\n",
    "        self.client.create_payload_index(\n",
    "            collection_name=collection_name,\n",
    "            field_name=\"item\",\n",
    "            field_schema=models.PayloadSchemaType.KEYWORD\n",
    "        )\n",
    "        print(f\"   ✓ Payload indexes created.\")\n",
    "\n",
    "    def upsert_documents(self, points_generator,\n",
    "                        collection_name: str = Config.COLLECTION_NAME,\n",
    "                        batch_size: int = 2048) -> int:\n",
    "        print(f\" Uploading chunks to Qdrant in batches of {batch_size}...\")\n",
    "        batch, count = [], 0\n",
    "        for point in points_generator:\n",
    "            batch.append(point)\n",
    "            if len(batch) >= batch_size:\n",
    "                self.client.upsert(collection_name=collection_name, points=batch, wait=False)\n",
    "                count += len(batch)\n",
    "                print(f\"     → Uploaded {count} chunks so far...\")\n",
    "                batch = []\n",
    "        if batch:\n",
    "            self.client.upsert(collection_name=collection_name, points=batch, wait=False)\n",
    "            count += len(batch)\n",
    "        print(f\"  ✓ All chunks uploaded for this document. Total: {count}\")\n",
    "        return count\n",
    "\n",
    "    def search(self, query_vector: List[float],\n",
    "              collection_name: str = Config.COLLECTION_NAME,\n",
    "              limit: int = Config.TOP_K,\n",
    "              ticker_list: List[str] = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Search for similar documents.\n",
    "        Now accepts a list of tickers to create an 'OR' filter.\n",
    "        \"\"\"\n",
    "        qdrant_filter = None\n",
    "        if ticker_list:\n",
    "            if len(ticker_list) == 1:\n",
    "                # 'must' is efficient for a single item\n",
    "                qdrant_filter = models.Filter(must=[\n",
    "                    models.FieldCondition(key=\"ticker\", match=models.MatchValue(value=ticker_list[0]))\n",
    "                ])\n",
    "            else:\n",
    "                # 'should' acts as an 'OR' filter for multiple tickers\n",
    "                qdrant_filter = models.Filter(should=[\n",
    "                    models.FieldCondition(key=\"ticker\", match=models.MatchValue(value=ticker))\n",
    "                    for ticker in ticker_list\n",
    "                ])\n",
    "        \n",
    "        results = self.client.search(\n",
    "            collection_name=collection_name,\n",
    "            query_vector=query_vector,\n",
    "            limit=limit,\n",
    "            query_filter=qdrant_filter,\n",
    "            with_payload=True\n",
    "        )\n",
    "        return [{'score': result.score, 'payload': result.payload} for result in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f86ff98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# PART 5: NER PIPELINE COMPONENTS\n",
    "# ===========================================================================\n",
    "\n",
    "class FinancialKnowledgeBase:\n",
    "    \"\"\"Comprehensive knowledge base for financial entity normalization\"\"\"\n",
    "    def __init__(self):\n",
    "        print(\"   → Initializing Financial Knowledge Base...\")\n",
    "        self.company_to_ticker = self._build_company_ticker_map()\n",
    "        self.financial_metrics = {\n",
    "            \"revenue\": \"REVENUE\", \"revenues\": \"REVENUE\", \"total revenue\": \"REVENUE\",\n",
    "            \"net income\": \"NET_INCOME\", \"net earnings\": \"NET_INCOME\", \"profit\": \"NET_INCOME\",\n",
    "            \"operating income\": \"OPERATING_INCOME\", \"ebit\": \"EBIT\", \"ebitda\": \"EBITDA\",\n",
    "            \"cash flow\": \"CASH_FLOW\", \"operating cash flow\": \"OPERATING_CASH_FLOW\",\n",
    "            \"free cash flow\": \"FREE_CASH_FLOW\", \"fcf\": \"FREE_CASH_FLOW\",\n",
    "            \"total assets\": \"TOTAL_ASSETS\", \"total liabilities\": \"TOTAL_LIABILITIES\",\n",
    "            \"equity\": \"SHAREHOLDERS_EQUITY\", \"shareholders' equity\": \"SHAREHOLDERS_EQUITY\",\n",
    "            \"earnings per share\": \"EPS\", \"eps\": \"EPS\",\n",
    "        }\n",
    "        self.risk_types = {\n",
    "            \"market risk\": \"MARKET_RISK\", \"credit risk\": \"CREDIT_RISK\",\n",
    "            \"operational risk\": \"OPERATIONAL_RISK\", \"liquidity risk\": \"LIQUIDITY_RISK\",\n",
    "            \"interest rate risk\": \"INTEREST_RATE_RISK\", \"currency risk\": \"CURRENCY_RISK\",\n",
    "            \"foreign exchange risk\": \"CURRENCY_RISK\", \"regulatory risk\": \"REGULATORY_RISK\",\n",
    "            \"compliance risk\": \"COMPLIANCE_RISK\", \"legal risk\": \"LEGAL_RISK\",\n",
    "            \"reputational risk\": \"REPUTATIONAL_RISK\", \"strategic risk\": \"STRATEGIC_RISK\",\n",
    "            \"cybersecurity risk\": \"CYBERSECURITY_RISK\", \"technology risk\": \"TECHNOLOGY_RISK\",\n",
    "            \"political risk\": \"POLITICAL_RISK\", \"climate risk\": \"CLIMATE_RISK\",\n",
    "        }\n",
    "\n",
    "    def _build_company_ticker_map(self):\n",
    "        try:\n",
    "            response = requests.get(Config.CIK_MAP_URL, headers=Config.SEC_HEADERS)\n",
    "            response.raise_for_status()\n",
    "            company_data = response.json()\n",
    "            mapping = {}\n",
    "            for company in company_data.values():\n",
    "                ticker = company['ticker']\n",
    "                title = company['title']\n",
    "                mapping[title] = ticker\n",
    "                mapping[title.upper()] = ticker\n",
    "                base_name = re.sub(r'\\s+(Inc\\.?|Corp\\.?|Corporation|Company|Co\\.?|Ltd\\.?|LLC|L\\.P\\.)\\s*,?', '', title, flags=re.IGNORECASE).strip()\n",
    "                mapping[base_name] = ticker\n",
    "                mapping[base_name.upper()] = ticker\n",
    "                mapping[ticker] = ticker\n",
    "                mapping[ticker.upper()] = ticker\n",
    "            print(f\"     ✓ Built company map with {len(mapping)} entries\")\n",
    "            return mapping\n",
    "        except Exception as e:\n",
    "            print(f\"     ⚠ Warning: Could not build company map from SEC. Using fallback. Error: {e}\")\n",
    "            return {\n",
    "                \"Alphabet\": \"GOOGL\", \"Alphabet Inc.\": \"GOOGL\", \"Google\": \"GOOGL\",\n",
    "                \"Microsoft\": \"MSFT\", \"Microsoft Corporation\": \"MSFT\",\n",
    "                \"Tesla\": \"TSLA\", \"Tesla, Inc.\": \"TSLA\",\n",
    "                \"Apple\": \"AAPL\", \"Apple Inc.\": \"AAPL\",\n",
    "                \"Amazon\": \"AMZN\", \"Amazon.com\": \"AMZN\", \"AMZN\": \"AMZN\",\n",
    "                \"Meta\": \"META\", \"Meta Platforms\": \"META\", \"Facebook\": \"META\",\n",
    "                \"NVIDIA\": \"NVDA\", \"Nvidia\": \"NVDA\", \"NVDA\": \"NVDA\",\n",
    "                \"AMD\": \"AMD\", \"Advanced Micro Devices\": \"AMD\",\n",
    "                \"Oracle\": \"ORCL\", \"JPMorgan\": \"JPM\",\n",
    "            }\n",
    "\n",
    "    def normalize_company(self, text):\n",
    "        text_upper = text.upper()\n",
    "        if text_upper in self.company_to_ticker:\n",
    "            return self.company_to_ticker[text_upper]\n",
    "        clean_text = re.sub(r'\\s+(Inc\\.?|Corp\\.?|Corporation|Company|Co\\.?|Ltd\\.?|LLC)\\s*,?', '', text, flags=re.IGNORECASE).strip().upper()\n",
    "        if clean_text in self.company_to_ticker:\n",
    "            return self.company_to_ticker[clean_text]\n",
    "        return None\n",
    "\n",
    "    def normalize_metric(self, text):\n",
    "        return self.financial_metrics.get(text.lower().strip())\n",
    "\n",
    "    def normalize_risk(self, text):\n",
    "        return self.risk_types.get(text.lower().strip())\n",
    "\n",
    "class FinancialEntityExtractor:\n",
    "    \"\"\"Enhanced entity extractor with financial domain knowledge.\"\"\"\n",
    "    def __init__(self, knowledge_base: FinancialKnowledgeBase):\n",
    "        self.kb = knowledge_base\n",
    "        print(\"   → Loading NER models...\")\n",
    "        try:\n",
    "            self.spacy_nlp = spacy.load(Config.SPACY_MODEL)\n",
    "            print(f\"     ✓ Loaded spaCy model: {Config.SPACY_MODEL}\")\n",
    "        except Exception as e:\n",
    "            print(f\"     ✗ ERROR: Failed to load spaCy model '{Config.SPACY_MODEL}'.\")\n",
    "            print(\"     Please run: python -m spacy download en_core_web_lg\")\n",
    "            raise e\n",
    "\n",
    "        try:\n",
    "            self.transformer_ner = pipeline(\n",
    "                \"token-classification\",\n",
    "                model=Config.NER_MODEL,\n",
    "                aggregation_strategy=\"simple\",\n",
    "                device=-1 # Use CPU (-1) for broader compatibility\n",
    "            )\n",
    "            print(f\"     ✓ Loaded Transformer model: {Config.NER_MODEL}\")\n",
    "        except Exception as e:\n",
    "            print(f\"     ⚠ Warning: Could not load transformer model: {e}\")\n",
    "            self.transformer_ner = None\n",
    "\n",
    "    def extract_entities_spacy(self, text):\n",
    "        doc = self.spacy_nlp(text)\n",
    "        return [{'text': ent.text, 'label': ent.label_, 'start': ent.start_char, 'end': ent.end_char, 'source': 'spacy'} for ent in doc.ents]\n",
    "\n",
    "    def extract_entities_transformer(self, text):\n",
    "        if not self.transformer_ner: return []\n",
    "        try:\n",
    "            # Chunking for long texts (max 512 tokens)\n",
    "            max_length = 500 # Keep a buffer\n",
    "            chunks = [text[i:i+max_length] for i in range(0, len(text), max_length)]\n",
    "            all_entities = []\n",
    "            offset = 0\n",
    "            for chunk in chunks:\n",
    "                entities = self.transformer_ner(chunk)\n",
    "                for ent in entities:\n",
    "                    all_entities.append({'text': ent['word'], 'label': ent['entity_group'], 'start': offset + ent['start'], 'end': offset + ent['end'], 'score': ent['score'], 'source': 'transformer'})\n",
    "                offset += len(chunk)\n",
    "            return all_entities\n",
    "        except Exception as e:\n",
    "            print(f\"     ⚠ Transformer NER error: {e}\")\n",
    "            return []\n",
    "\n",
    "    def extract_financial_patterns(self, text):\n",
    "        entities = []\n",
    "        # Risk patterns (dynamic from KB)\n",
    "        risk_keys = '|'.join(re.escape(k) for k in self.kb.risk_types.keys())\n",
    "        risk_pattern = rf'({risk_keys})'\n",
    "        for match in re.finditer(risk_pattern, text, re.IGNORECASE):\n",
    "            entities.append({'text': match.group(1), 'label': 'RISK_MENTION', 'start': match.start(), 'end': match.end(), 'source': 'pattern'})\n",
    "        # Monetary patterns\n",
    "        monetary_pattern = r'(\\$\\s*\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?|\\d{1,3}(?:,\\d{3})+(?!\\d))'\n",
    "        for match in re.finditer(monetary_pattern, text):\n",
    "            if ',' in match.group(1) or '$' in match.group(1):\n",
    "                entities.append({'text': match.group(1), 'label': 'MONETARY_VALUE', 'start': match.start(), 'end': match.end(), 'source': 'pattern'})\n",
    "        return entities\n",
    "\n",
    "    def normalize_and_link(self, entities):\n",
    "        normalized = []\n",
    "        seen_spans = set()\n",
    "        for ent in sorted(entities, key=lambda x: x.get('score', 1.0), reverse=True):\n",
    "            span = (ent['start'], ent['end'])\n",
    "            if span in seen_spans: continue\n",
    "            seen_spans.add(span)\n",
    "            \n",
    "            ent_text, ent_label = ent['text'].strip(), ent['label']\n",
    "            result = {'original_text': ent_text, 'normalized_text': None, 'entity_type': ent_label, 'linked_id': None, 'confidence': ent.get('score', 1.0), 'source': ent.get('source', 'unknown'), 'span': span}\n",
    "\n",
    "            if ent_label in ['ORG', 'ORGANIZATION']:\n",
    "                ticker = self.kb.normalize_company(ent_text)\n",
    "                if ticker:\n",
    "                    result['normalized_text'] = ticker\n",
    "                    result['entity_type'] = 'COMPANY'\n",
    "                    result['linked_id'] = f\"TICKER:{ticker}\"\n",
    "            elif ent_label == 'CARDINAL' and ',' in ent_text:\n",
    "                result['entity_type'] = 'MONETARY_VALUE'\n",
    "                result['normalized_text'] = ent_text.replace(',', '')\n",
    "            elif ent_label in ['MONEY', 'MONETARY_VALUE']:\n",
    "                result['entity_type'] = 'MONETARY_VALUE'\n",
    "                numbers = re.findall(r'\\d+(?:,\\d{3})*(?:\\.\\d+)?', ent_text.replace('$', ''))\n",
    "                if numbers: result['normalized_text'] = numbers[0].replace(',', '')\n",
    "            elif ent_label == 'RISK_MENTION':\n",
    "                risk_type = self.kb.normalize_risk(ent_text)\n",
    "                if risk_type:\n",
    "                    result['normalized_text'] = risk_type\n",
    "                    result['entity_type'] = 'RISK_TYPE'\n",
    "                    result['linked_id'] = f\"RISK:{risk_type}\"\n",
    "            \n",
    "            metric = self.kb.normalize_metric(ent_text)\n",
    "            if metric:\n",
    "                result['entity_type'] = 'FINANCIAL_METRIC'\n",
    "                result['normalized_text'] = metric\n",
    "                result['linked_id'] = f\"METRIC:{metric}\"\n",
    "            \n",
    "            normalized.append(result)\n",
    "        return normalized\n",
    "\n",
    "    def extract_and_normalize(self, text):\n",
    "        spacy_entities = self.extract_entities_spacy(text)\n",
    "        transformer_entities = self.extract_entities_transformer(text)\n",
    "        pattern_entities = self.extract_financial_patterns(text)\n",
    "        all_entities = spacy_entities + transformer_entities + pattern_entities\n",
    "        normalized = self.normalize_and_link(all_entities)\n",
    "        return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bd6fa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# PART 6: ENHANCED RAG QUERY ENGINE\n",
    "# ===========================================================================\n",
    "\n",
    "class EnhancedRAGEngine:\n",
    "    \"\"\"\n",
    "    This is the new ENHANCED RAG query engine.\n",
    "    It uses the NER components to automatically detect tickers\n",
    "    in the question and filter the search.\n",
    "    \"\"\"\n",
    "    def __init__(self, document_processor: DocumentProcessor, qdrant_manager: QdrantManager):\n",
    "        print(\"\\n Initializing Enhanced RAG Query Engine...\")\n",
    "\n",
    "        # 1. Get components from main pipeline\n",
    "        self.embedding_model = document_processor.model\n",
    "        print(\"   ✓ Using existing embedding model\")\n",
    "        self.qdrant_manager = qdrant_manager\n",
    "        print(\"   ✓ Using existing QdrantManager for search\")\n",
    "\n",
    "        # 2. Build NER components\n",
    "        self.kb = FinancialKnowledgeBase()\n",
    "        self.extractor = FinancialEntityExtractor(self.kb)\n",
    "        \n",
    "        # 3. Initialize LLM and Prompt\n",
    "        self.llm = ChatOpenAI(model=Config.LLM_MODEL, api_key=Config.OPENAI_API_KEY, temperature=0)\n",
    "        template = \"\"\"You are a helpful financial analyst assistant. Your role is to answer questions about SEC 10-Q filings based ONLY on the provided context.\n",
    "- Base your answer strictly on the provided context from SEC filings\n",
    "- Cite specific sections (e.g., \"According to Item 1A...\") when referencing information\n",
    "- If the answer is not in the context, clearly state that\n",
    "\n",
    "Context:\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        self.prompt = ChatPromptTemplate.from_template(template)\n",
    "        print(\"   ✓ Enhanced RAG Engine ready.\")\n",
    "\n",
    "    def _format_context(self, search_results: List[Dict]) -> str:\n",
    "        context_str = \"\"\n",
    "        for i, result in enumerate(search_results, 1):\n",
    "            payload = result.get('payload', {})\n",
    "            text = payload.get('text', 'No text found')\n",
    "            item = payload.get('item', 'N/A')\n",
    "            ticker = payload.get('ticker', 'N/A')\n",
    "            context_str += f\"Source {i} ({ticker} - {item}):\\n\\\"{text}\\\"\\n\\n\"\n",
    "        return context_str.strip()\n",
    "\n",
    "    def query(self, question: str, ticker_filter: str = None):\n",
    "        print(f\"\\n Processing query with Enhanced Engine: '{question}'\")\n",
    "\n",
    "        # 1. Manually embed the query (same as before)\n",
    "        print(\"   → Manually embedding query...\")\n",
    "        query_vector = self.embedding_model.encode(question).tolist()\n",
    "\n",
    "        # 2. Use NER to find tickers, unless user provides a manual override.\n",
    "        tickers_to_filter = []\n",
    "        if ticker_filter:\n",
    "            print(f\"   → Using manual ticker override: {ticker_filter}\")\n",
    "            tickers_to_filter = [ticker_filter.upper()]\n",
    "        else:\n",
    "            print(\"   → Running NER on query to find companies...\")\n",
    "            normalized_entities = self.extractor.extract_and_normalize(question)\n",
    "            tickers_from_ner = sorted(list(set(\n",
    "                ent['normalized_text'] \n",
    "                for ent in normalized_entities \n",
    "                if ent['entity_type'] == 'COMPANY' and ent['normalized_text']\n",
    "            )))\n",
    "            if tickers_from_ner:\n",
    "                print(f\"   → NER detected and will filter for: {tickers_from_ner}\")\n",
    "                tickers_to_filter = tickers_from_ner\n",
    "            else:\n",
    "                print(\"   → NER found no companies, searching all documents.\")\n",
    "        \n",
    "        # 3. Manually search Qdrant (using the new multi-ticker list)\n",
    "        print(\"   → Manually searching Qdrant...\")\n",
    "        search_results = self.qdrant_manager.search(\n",
    "            query_vector=query_vector,\n",
    "            limit=Config.TOP_K,\n",
    "            ticker_list=tickers_to_filter\n",
    "        )\n",
    "\n",
    "        if not search_results:\n",
    "            return {'answer': 'No relevant context was found in the documents to answer this question.', 'sources': []}\n",
    "\n",
    "        # 4. Manually format the prompt\n",
    "        print(\"   → Formatting context and building prompt...\")\n",
    "        formatted_context = self._format_context(search_results)\n",
    "        final_prompt_message = self.prompt.format_messages(context=formatted_context, input=question)\n",
    "\n",
    "        # 5. Manually invoke the LLM\n",
    "        print(\"   → Sending prompt to LLM...\")\n",
    "        llm_response = self.llm.invoke(final_prompt_message)\n",
    "        answer = llm_response.content\n",
    "\n",
    "        # 6. Format sources\n",
    "        sources = [{'ticker': r['payload'].get('ticker'), 'company': r['payload'].get('company_name'), 'item': r['payload'].get('item'), 'part': r['payload'].get('part'), 'filing_date': r['payload'].get('filing_date'), 'score': r['score']} for r in search_results]\n",
    "\n",
    "        return {'answer': answer, 'sources': sources}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8ae4082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# PART 7: MAIN PIPELINE ORCHESTRATOR\n",
    "# ===========================================================================\n",
    "\n",
    "class SECFilingRAGPipeline:\n",
    "    \"\"\"Main pipeline orchestrator\"\"\"\n",
    "    def __init__(self):\n",
    "        print(\"=\" * 70)\n",
    "        print(\"ENHANCED SEC 10-Q FILING RAG SYSTEM (RAG + NER)\")\n",
    "        print(\"=\" * 70)\n",
    "        self.loader = SECDocumentLoader()\n",
    "        self.processor = DocumentProcessor()\n",
    "        self.qdrant_manager = QdrantManager()\n",
    "\n",
    "        # Initialize the new query engine immediately\n",
    "        self.query_engine = EnhancedRAGEngine(\n",
    "            document_processor=self.processor,\n",
    "            qdrant_manager=self.qdrant_manager\n",
    "        )\n",
    "\n",
    "    def load_and_index_filings(self, tickers: List[str] = Config.TICKERS, num_filings_per_ticker: int = 1):\n",
    "        \"\"\"\n",
    "        MODIFIED: This function now only checks if the Golden Set exists.\n",
    "        It does NOT perform indexing.\n",
    "        \"\"\"\n",
    "        print(f\"\\\\n{'=' * 70}\")\n",
    "        print(f\"CHECKING DATABASE: {Config.COLLECTION_NAME}\")\n",
    "        print(f\"{'=' * 70}\")\n",
    "        \n",
    "        try:\n",
    "            # We use num_filings_per_ticker to check if we need to index,\n",
    "            # but for this test, we just check existence.\n",
    "            count = self.qdrant_manager.client.count(Config.COLLECTION_NAME, exact=True)\n",
    "            if count.count > 0:\n",
    "                print(f\"✓ Collection '{Config.COLLECTION_NAME}' found with {count.count} docs. Proceeding to query.\")\n",
    "            else:\n",
    "                print(f\"✗ ERROR: Collection '{Config.COLLECTION_NAME}' is empty or not found.\")\n",
    "                print(\"Please run '0_build_database.ipynb' first.\")\n",
    "                raise Exception(\"Database not found.\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ ERROR: Could not connect to or find collection '{Config.COLLECTION_NAME}'.\")\n",
    "            print(\"Please run '0_build_database.ipynb' first.\")\n",
    "            raise e\n",
    "        \n",
    "        print(f\"{'=' * 70}\\\\n\")\n",
    "\n",
    "    def query(self, question: str, ticker_filter: str = None):\n",
    "        \"\"\"Query the indexed filings using the ENHANCED engine\"\"\"\n",
    "        # The query engine is already initialized, so just use it\n",
    "        result = self.query_engine.query(question, ticker_filter)\n",
    "\n",
    "        # Print results\n",
    "        print(f\"\\n{'=' * 70}\\nANSWER\\n{'=' * 70}\")\n",
    "        print(f\"\\n{result['answer']}\\n\")\n",
    "        print(f\"{'=' * 70}\\nSOURCES ({len(result['sources'])} chunks)\\n{'=' * 70}\")\n",
    "        for i, source in enumerate(result['sources'], 1):\n",
    "            print(f\"\\n{i}. {source['company']} ({source['ticker']}) - {source['item']}\")\n",
    "            print(f\"   Filing Date: {source['filing_date']}\")\n",
    "            print(f\"   Relevance Score: {source['score']:.4f}\")\n",
    "        print(f\"\\n{'=' * 70}\\n\")\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4aad4aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ENHANCED SEC 10-Q FILING RAG SYSTEM (RAG + NER)\n",
      "======================================================================\n",
      "\n",
      " Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "   ✓ Model loaded (dimension: 384)\n",
      "   ✓ Initialized RecursiveCharacterTextSplitter\n",
      "\n",
      "Connecting to Qdrant Cloud...\n",
      "   ✓ Connected to Qdrant\n",
      "\n",
      " Initializing Enhanced RAG Query Engine...\n",
      "   ✓ Using existing embedding model\n",
      "   ✓ Using existing QdrantManager for search\n",
      "   → Initializing Financial Knowledge Base...\n",
      "     ✓ Built company map with 34397 entries\n",
      "   → Loading NER models...\n",
      "     ✓ Loaded spaCy model: en_core_web_lg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ✓ Loaded Transformer model: dslim/bert-base-NER\n",
      "   ✓ Enhanced RAG Engine ready.\n",
      "Starting the indexing process... This may take a long time.\n",
      "\\n======================================================================\n",
      "CHECKING DATABASE: sec_filings_10q_GOLDEN_BENCHMARK\n",
      "======================================================================\n",
      "✓ Collection 'sec_filings_10q_GOLDEN_BENCHMARK' found with 12807 docs. Proceeding to query.\n",
      "======================================================================\\n\n",
      "Indexing complete. Proceeding to queries.\n"
     ]
    }
   ],
   "source": [
    "# ===========================================================================\n",
    "# PART 8: USAGE EXAMPLE (Saving all results to one file)\n",
    "# ===========================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # =======================================================================\n",
    "    # 1. INITIALIZE PIPELINE\n",
    "    # =======================================================================\n",
    "    pipeline = SECFilingRAGPipeline()\n",
    "\n",
    "    # =======================================================================\n",
    "    # 2. LOAD AND INDEX FILINGS\n",
    "    # =======================================================================\n",
    "    # WARNING: This step can take a long time (10-20 minutes).\n",
    "    # If you have already run this successfully, you can comment out\n",
    "    # the line below to skip straight to querying.\n",
    "\n",
    "    print(\"Starting the indexing process... This may take a long time.\")\n",
    "    pipeline.load_and_index_filings(num_filings_per_ticker=4)\n",
    "    print(\"Indexing complete. Proceeding to queries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60e5e7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nRunning 6 queries with (Model 3) RAG + NER...\n",
      "\\n--- Running Query: What are the main risk factors mentioned by each companies? ---\n",
      "\n",
      " Processing query with Enhanced Engine: 'What are the main risk factors mentioned by each companies?'\n",
      "   → Manually embedding query...\n",
      "   → Running NER on query to find companies...\n",
      "   → NER found no companies, searching all documents.\n",
      "   → Manually searching Qdrant...\n",
      "   → Formatting context and building prompt...\n",
      "   → Sending prompt to LLM...\n",
      "\n",
      "======================================================================\n",
      "ANSWER\n",
      "======================================================================\n",
      "\n",
      "Based on the provided context, the main risk factors mentioned by each company are as follows:\n",
      "\n",
      "**AMD:**\n",
      "According to Item 1A, the main risk factors for AMD include:\n",
      "- The highly competitive and rapidly evolving markets in which their products are sold.\n",
      "- The cyclical nature of the semiconductor industry, which has experienced severe downturns.\n",
      "\n",
      "**Apple (AAPL):**\n",
      "According to Item 1A, the risk factors for Apple include:\n",
      "- Various factors that can affect the company's business, reputation, results of operations, financial condition, and stock price. However, the specific risk factors are not detailed in the provided context.\n",
      "\n",
      "**Amazon (AMZN):**\n",
      "According to Item 1A, the main risk factors for Amazon include:\n",
      "- The difficulty of implementing appropriate controls, procedures, and policies at acquired companies.\n",
      "- Risks associated with businesses they acquire or invest in, which may differ from or be more significant than those faced by their other businesses.\n",
      "- Potential unknown liabilities associated with acquisitions or investments.\n",
      "- Additional risks for foreign transactions, including integration across different cultures and languages, and economic, political, and regulatory risks associated with specific countries.\n",
      "\n",
      "======================================================================\n",
      "SOURCES (5 chunks)\n",
      "======================================================================\n",
      "\n",
      "1. ADVANCED MICRO DEVICES INC (AMD) - ITEM 1A\n",
      "   Filing Date: 2025-11-05\n",
      "   Relevance Score: 0.6942\n",
      "\n",
      "2. ADVANCED MICRO DEVICES INC (AMD) - ITEM 1A\n",
      "   Filing Date: 2025-08-06\n",
      "   Relevance Score: 0.6942\n",
      "\n",
      "3. Apple Inc. (AAPL) - ITEM 1A\n",
      "   Filing Date: 2025-01-31\n",
      "   Relevance Score: 0.6650\n",
      "\n",
      "4. AMAZON COM INC (AMZN) - ITEM 1A\n",
      "   Filing Date: 2025-08-01\n",
      "   Relevance Score: 0.6644\n",
      "\n",
      "5. AMAZON COM INC (AMZN) - ITEM 1A\n",
      "   Filing Date: 2025-05-02\n",
      "   Relevance Score: 0.6644\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\\n--- Running Query: What risks did Apple disclose in their latest 10-Q? ---\n",
      "\n",
      " Processing query with Enhanced Engine: 'What risks did Apple disclose in their latest 10-Q?'\n",
      "   → Manually embedding query...\n",
      "   → Running NER on query to find companies...\n",
      "   → NER detected and will filter for: ['AAPL']\n",
      "   → Manually searching Qdrant...\n",
      "   → Formatting context and building prompt...\n",
      "   → Sending prompt to LLM...\n",
      "\n",
      "======================================================================\n",
      "ANSWER\n",
      "======================================================================\n",
      "\n",
      "According to the provided context, Apple disclosed that macroeconomic conditions, including inflation, interest rates, and currency fluctuations, have directly and indirectly impacted, and could in the future materially impact, the Company’s results of operations and financial condition (Source 2, AAPL - ITEM 2). Additionally, events that can disrupt the supply and manufacturing chain, increase costs, and negatively impact consumer spending and demand were also mentioned as risks (Source 4, AAPL - ITEM 1A). Furthermore, the Company noted that future gross margins could be subject to volatility and downward pressure due to various factors discussed under \"Risk Factors\" (Source 3, AAPL - ITEM 2).\n",
      "\n",
      "======================================================================\n",
      "SOURCES (5 chunks)\n",
      "======================================================================\n",
      "\n",
      "1. Apple Inc. (AAPL) - ITEM 1A\n",
      "   Filing Date: 2024-08-02\n",
      "   Relevance Score: 0.7313\n",
      "\n",
      "2. Apple Inc. (AAPL) - ITEM 2\n",
      "   Filing Date: 2025-05-02\n",
      "   Relevance Score: 0.6400\n",
      "\n",
      "3. Apple Inc. (AAPL) - ITEM 2\n",
      "   Filing Date: 2025-05-02\n",
      "   Relevance Score: 0.6190\n",
      "\n",
      "4. Apple Inc. (AAPL) - ITEM 1A\n",
      "   Filing Date: 2025-05-02\n",
      "   Relevance Score: 0.6020\n",
      "\n",
      "5. Apple Inc. (AAPL) - ITEM 1\n",
      "   Filing Date: 2024-08-02\n",
      "   Relevance Score: 0.5975\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\\n--- Running Query: Compare the revenue trends of NVIDIA and AMD ---\n",
      "\n",
      " Processing query with Enhanced Engine: 'Compare the revenue trends of NVIDIA and AMD'\n",
      "   → Manually embedding query...\n",
      "   → Running NER on query to find companies...\n",
      "   → NER detected and will filter for: ['AMD', 'NVDA']\n",
      "   → Manually searching Qdrant...\n",
      "   → Formatting context and building prompt...\n",
      "   → Sending prompt to LLM...\n",
      "\n",
      "======================================================================\n",
      "ANSWER\n",
      "======================================================================\n",
      "\n",
      "Based on the provided context, we can observe the following revenue trends for NVIDIA and AMD:\n",
      "\n",
      "**NVIDIA:**\n",
      "- One customer represented approximately 17% and 13% of total revenue for the second quarter and first half of fiscal year 2024, respectively, and another customer represented 12% of total revenue for the third quarter of fiscal year 2024. These revenues were attributable to the Compute & Networking segment (Source 1 and Source 2).\n",
      "- The cost of revenue for NVIDIA was recorded as $543 million and $473 million for the third quarter, and $1.3 billion and $734 million for the first nine months of fiscal years 2025 and 2024, respectively (Source 3).\n",
      "- During the second quarter of fiscal years 2025 and 2024, NVIDIA recorded $563 million and $232 million, respectively, in cost of revenue (Source 4).\n",
      "\n",
      "**AMD:**\n",
      "- For the three months ended March 29, 2025, AMD reported net revenue of $7.4 billion, which was a 36% increase compared to the prior year period. This increase was driven by growth in the Data Center segment, particularly from AMD EPYC™ CPU and AMD Instinct™ GPU sales, and an increase in the Client and Gaming segment revenue due to strong demand for the latest “Zen 5” AMD Ryzen™ processors. However, there was a decrease in semi-custom revenue, and Embedded segment revenue decreased due to mixed demand in end markets (Source 5).\n",
      "\n",
      "In summary, NVIDIA's revenue trends show significant contributions from specific large customers in the Compute & Networking segment, while AMD experienced a substantial increase in overall net revenue driven by growth in its Data Center and Client and Gaming segments.\n",
      "\n",
      "======================================================================\n",
      "SOURCES (5 chunks)\n",
      "======================================================================\n",
      "\n",
      "1. NVIDIA CORP (NVDA) - ITEM 1\n",
      "   Filing Date: 2024-08-28\n",
      "   Relevance Score: 0.6962\n",
      "\n",
      "2. NVIDIA CORP (NVDA) - ITEM 1\n",
      "   Filing Date: 2024-11-20\n",
      "   Relevance Score: 0.6546\n",
      "\n",
      "3. NVIDIA CORP (NVDA) - ITEM 1\n",
      "   Filing Date: 2024-11-20\n",
      "   Relevance Score: 0.6414\n",
      "\n",
      "4. NVIDIA CORP (NVDA) - ITEM 1\n",
      "   Filing Date: 2024-08-28\n",
      "   Relevance Score: 0.6392\n",
      "\n",
      "5. ADVANCED MICRO DEVICES INC (AMD) - ITEM 2\n",
      "   Filing Date: 2025-05-07\n",
      "   Relevance Score: 0.6338\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\\n--- Running Query: What was Tesla's R&D spending in the latest quarter? ---\n",
      "\n",
      " Processing query with Enhanced Engine: 'What was Tesla's R&D spending in the latest quarter?'\n",
      "   → Manually embedding query...\n",
      "   → Running NER on query to find companies...\n",
      "   → NER found no companies, searching all documents.\n",
      "   → Manually searching Qdrant...\n",
      "   → Formatting context and building prompt...\n",
      "   → Sending prompt to LLM...\n",
      "\n",
      "======================================================================\n",
      "ANSWER\n",
      "======================================================================\n",
      "\n",
      "The specific amount of Tesla's R&D spending in the latest quarter is not provided in the context. The context only provides information on the percentage increase and the factors contributing to the change in R&D expenses.\n",
      "\n",
      "======================================================================\n",
      "SOURCES (5 chunks)\n",
      "======================================================================\n",
      "\n",
      "1. Tesla, Inc. (TSLA) - ITEM 2\n",
      "   Filing Date: 2024-10-24\n",
      "   Relevance Score: 0.5818\n",
      "\n",
      "2. Tesla, Inc. (TSLA) - ITEM 2\n",
      "   Filing Date: 2025-10-23\n",
      "   Relevance Score: 0.5687\n",
      "\n",
      "3. Tesla, Inc. (TSLA) - ITEM 2\n",
      "   Filing Date: 2025-07-24\n",
      "   Relevance Score: 0.5555\n",
      "\n",
      "4. Tesla, Inc. (TSLA) - ITEM 2\n",
      "   Filing Date: 2024-10-24\n",
      "   Relevance Score: 0.5433\n",
      "\n",
      "5. Alphabet Inc. (GOOGL) - ITEM 2\n",
      "   Filing Date: 2025-04-25\n",
      "   Relevance Score: 0.5425\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\\n--- Running Query: How has Microsoft's operating income changed over the last year? ---\n",
      "\n",
      " Processing query with Enhanced Engine: 'How has Microsoft's operating income changed over the last year?'\n",
      "   → Manually embedding query...\n",
      "   → Running NER on query to find companies...\n",
      "   → NER detected and will filter for: ['MSFT']\n",
      "   → Manually searching Qdrant...\n",
      "   → Formatting context and building prompt...\n",
      "   → Sending prompt to LLM...\n",
      "\n",
      "======================================================================\n",
      "ANSWER\n",
      "======================================================================\n",
      "\n",
      "The provided context does not include specific information about changes in Microsoft's operating income over the last year. The context only contains reviews of interim financial information and states that no material modifications are needed for conformity with accounting principles. For details on operating income changes, you would need to refer to specific financial statements or sections of the 10-Q that discuss operating results, which are not included in the provided context.\n",
      "\n",
      "======================================================================\n",
      "SOURCES (5 chunks)\n",
      "======================================================================\n",
      "\n",
      "1. MICROSOFT CORP (MSFT) - ITEM 1\n",
      "   Filing Date: 2025-01-29\n",
      "   Relevance Score: 0.4972\n",
      "\n",
      "2. MICROSOFT CORP (MSFT) - ITEM 1\n",
      "   Filing Date: 2025-04-30\n",
      "   Relevance Score: 0.4947\n",
      "\n",
      "3. MICROSOFT CORP (MSFT) - ITEM 1\n",
      "   Filing Date: 2024-10-30\n",
      "   Relevance Score: 0.4469\n",
      "\n",
      "4. MICROSOFT CORP (MSFT) - ITEM 1\n",
      "   Filing Date: 2025-10-29\n",
      "   Relevance Score: 0.4465\n",
      "\n",
      "5. MICROSOFT CORP (MSFT) - ITEM 1\n",
      "   Filing Date: 2025-01-29\n",
      "   Relevance Score: 0.3949\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\\n--- Running Query: What was the gross profit margin for all companies? ---\n",
      "\n",
      " Processing query with Enhanced Engine: 'What was the gross profit margin for all companies?'\n",
      "   → Manually embedding query...\n",
      "   → Running NER on query to find companies...\n",
      "   → NER found no companies, searching all documents.\n",
      "   → Manually searching Qdrant...\n",
      "   → Formatting context and building prompt...\n",
      "   → Sending prompt to LLM...\n",
      "\n",
      "======================================================================\n",
      "ANSWER\n",
      "======================================================================\n",
      "\n",
      "Based on the provided context:\n",
      "\n",
      "- For Tesla (TSLA), the gross profit margins reported in the different sources are:\n",
      "  - Source 1: 18.0% and 19.8%\n",
      "  - Source 2: 19.8% and 17.9%\n",
      "  - Source 3: 17.2% and 18.0%\n",
      "\n",
      "- For NVIDIA (NVDA), according to Source 4, the gross profit margins were:\n",
      "  - 75.1% for the second quarter of fiscal year 2025\n",
      "  - 76.6% for the first half of fiscal year 2025\n",
      "\n",
      "- For Oracle (ORCL), the context does not provide specific gross profit margin figures. It mentions that the margins reported reflect only the direct controllable costs and do not include various other expenses (Source 5).\n",
      "\n",
      "Therefore, the gross profit margins for Tesla and NVIDIA are provided, but specific figures for Oracle are not available in the context.\n",
      "\n",
      "======================================================================\n",
      "SOURCES (5 chunks)\n",
      "======================================================================\n",
      "\n",
      "1. Tesla, Inc. (TSLA) - ITEM 2\n",
      "   Filing Date: 2025-10-23\n",
      "   Relevance Score: 0.6704\n",
      "\n",
      "2. Tesla, Inc. (TSLA) - ITEM 2\n",
      "   Filing Date: 2024-10-24\n",
      "   Relevance Score: 0.6659\n",
      "\n",
      "3. Tesla, Inc. (TSLA) - ITEM 2\n",
      "   Filing Date: 2025-07-24\n",
      "   Relevance Score: 0.6528\n",
      "\n",
      "4. NVIDIA CORP (NVDA) - ITEM 2\n",
      "   Filing Date: 2024-08-28\n",
      "   Relevance Score: 0.6454\n",
      "\n",
      "5. ORACLE CORP (ORCL) - ITEM 1\n",
      "   Filing Date: 2024-09-10\n",
      "   Relevance Score: 0.6310\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\\n--- All queries complete. Saving all results to: data\\results_model_3_ner.json ---\n",
      "✓ Successfully saved all answers.\n",
      "\\nPipeline run finished.\n"
     ]
    }
   ],
   "source": [
    "# =======================================================================\n",
    "# DEFINE QUESTIONS AND SAVE ALL ANSWERS TO ONE FILE\n",
    "# =======================================================================\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Define the output folder\n",
    "DATA_FOLDER = \"data\"\n",
    "if not os.path.exists(DATA_FOLDER):\n",
    "    os.makedirs(DATA_FOLDER)\n",
    "    print(f\"Created data folder: {DATA_FOLDER}\")\n",
    "\n",
    "# --- Standardized Query Set ---\n",
    "queries_to_run = [\n",
    "    {\"question\": \"What are the main risk factors mentioned by each companies?\", \"ticker_filter\": None},\n",
    "    {\"question\": \"What risks did Apple disclose in their latest 10-Q?\", \"ticker_filter\": None},\n",
    "    {\"question\": \"Compare the revenue trends of NVIDIA and AMD\", \"ticker_filter\": None},\n",
    "    {\"question\": \"What was Tesla's R&D spending in the latest quarter?\", \"ticker_filter\": None},\n",
    "    {\"question\": \"How has Microsoft's operating income changed over the last year?\", \"ticker_filter\": None},\n",
    "    {\"question\": \"What was the gross profit margin for all companies?\", \"ticker_filter\": None},\n",
    "]\n",
    "\n",
    "print(f\"\\\\nRunning {len(queries_to_run)} queries with (Model 3) RAG + NER...\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for query in queries_to_run:\n",
    "    print(f\"\\\\n--- Running Query: {query['question']} ---\")\n",
    "    \n",
    "    result = pipeline.query(\n",
    "        question=query[\"question\"],\n",
    "        ticker_filter=query[\"ticker_filter\"]\n",
    "    )\n",
    "    \n",
    "    query_output = {\n",
    "        \"question\": query[\"question\"],\n",
    "        \"ticker_filter_manual\": query[\"ticker_filter\"],\n",
    "        \"response\": result\n",
    "    }\n",
    "    all_results.append(query_output)\n",
    "\n",
    "# --- Standardized Save Path ---\n",
    "save_path = os.path.join(DATA_FOLDER, \"results_model_3_ner.json\")\n",
    "print(f\"\\\\n--- All queries complete. Saving all results to: {save_path} ---\")\n",
    "\n",
    "try:\n",
    "    with open(save_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_results, f, indent=4)\n",
    "    print(f\"✓ Successfully saved all answers.\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Failed to save results: {e}\")\n",
    "\n",
    "print(\"\\\\nPipeline run finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
