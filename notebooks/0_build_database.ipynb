{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66e9721e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chan Thong Fong\\OneDrive - National University of Singapore\\Documents\\Y4S1\\DSA4213\\DSA4213-SEC-Fillings-Chatbot\\venv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import uuid\n",
    "import time\n",
    "import requests\n",
    "import gc\n",
    "import json\n",
    "import unicodedata\n",
    "from typing import List, Dict, Tuple, Any, Optional\n",
    "from collections import defaultdict\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "# External libraries\n",
    "from dotenv import load_dotenv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient, models\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd93a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# 1. CONFIGURATION\n",
    "# ===========================================================================\n",
    "class Config:\n",
    "    QDRANT_URL = os.getenv(\"QDRANT_URL\")\n",
    "    QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")\n",
    "    # Use the User-Agent from your working script\n",
    "    SEC_HEADERS = {'User-Agent': 'YourAppName youremail@example.com'}\n",
    "    CIK_MAP_URL = 'https://www.sec.gov/files/company_tickers.json'\n",
    "    EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    \n",
    "    COLLECTION_NAME = \"sec_filings_10q_GOLDEN_BENCHMARK\" \n",
    "    \n",
    "    VECTOR_SIZE = 384\n",
    "    CHUNK_SIZE = 800\n",
    "    CHUNK_OVERLAP = 200\n",
    "    TICKERS = ['NVDA', 'AAPL', 'MSFT', 'AMZN', 'META', 'GOOGL', 'TSLA', 'ORCL', 'JPM', 'AMD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c49281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# 2. DOCUMENT LOADER\n",
    "# ===========================================================================\n",
    "class SECDocumentLoader:\n",
    "    \"\"\"\n",
    "    Enhanced document loader with robust table parsing and validation\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_recent_10q_metadata(ticker: str, num_filings: int = 4) -> List[Dict[str, str]]:\n",
    "        \"\"\"\n",
    "        Fetches the metadata (links, dates, etc.) for the latest N 10-Q filings.\n",
    "        Does NOT fetch the actual HTML content.\n",
    "\n",
    "        Args:\n",
    "            ticker: The company ticker (e.g., 'AAPL')\n",
    "            num_filings: The number of recent 10-Q filings to fetch\n",
    "\n",
    "        Returns:\n",
    "            List of metadata dictionaries\n",
    "        \"\"\"\n",
    "        print(f\"  â†’ Fetching CIK for ticker: {ticker}...\")\n",
    "\n",
    "        # Get CIK mapping\n",
    "        response = requests.get(Config.CIK_MAP_URL, headers=Config.SEC_HEADERS)\n",
    "        response.raise_for_status()\n",
    "        company_data = response.json()\n",
    "\n",
    "        # Find CIK\n",
    "        cik = None\n",
    "        company_name = None\n",
    "        for company in company_data.values():\n",
    "            if company['ticker'] == ticker.upper():\n",
    "                cik = str(company['cik_str']).zfill(10)\n",
    "                company_name = company['title']\n",
    "                break\n",
    "\n",
    "        if not cik:\n",
    "            raise ValueError(f\"Ticker '{ticker}' not found in SEC CIK mapping.\")\n",
    "\n",
    "        print(f\"  â†’ Found CIK: {cik} ({company_name})\")\n",
    "\n",
    "        # Fetch submission history\n",
    "        submissions_url = f\"https://data.sec.gov/submissions/CIK{cik}.json\"\n",
    "        time.sleep(0.1)  # Rate limiting\n",
    "        response = requests.get(submissions_url, headers=Config.SEC_HEADERS)\n",
    "        response.raise_for_status()\n",
    "        submissions = response.json()\n",
    "\n",
    "        # Find latest N 10-Q filings metadata\n",
    "        filings_metadata = []\n",
    "        for i, form in enumerate(submissions['filings']['recent']['form']):\n",
    "            if form == '10-Q':\n",
    "                accession_number = submissions['filings']['recent']['accessionNumber'][i]\n",
    "                primary_document = submissions['filings']['recent']['primaryDocument'][i]\n",
    "                filing_date = submissions['filings']['recent']['filingDate'][i]\n",
    "\n",
    "                accession_number_clean = accession_number.replace('-', '')\n",
    "\n",
    "                # Construct the filing URL\n",
    "                filing_url = (\n",
    "                    f\"https://www.sec.gov/Archives/edgar/data/{cik}/\"\n",
    "                    f\"{accession_number_clean}/{primary_document}\"\n",
    "                )\n",
    "\n",
    "                metadata = {\n",
    "                    'ticker': ticker.upper(),\n",
    "                    'company_name': company_name,\n",
    "                    'filing_date': filing_date,\n",
    "                    'cik': cik,\n",
    "                    'filing_url': filing_url\n",
    "                }\n",
    "                filings_metadata.append(metadata)\n",
    "\n",
    "                if len(filings_metadata) >= num_filings:\n",
    "                    break\n",
    "\n",
    "        if not filings_metadata:\n",
    "            raise ValueError(f\"No recent 10-Q filings found for ticker '{ticker}'.\")\n",
    "\n",
    "        print(f\"  â†’ Found {len(filings_metadata)} recent 10-Q filing metadata entries.\")\n",
    "        return filings_metadata\n",
    "\n",
    "    @staticmethod\n",
    "    def get_filing_html(filing_url: str) -> str:\n",
    "        \"\"\"Fetches the HTML content for a single filing URL.\"\"\"\n",
    "        time.sleep(0.1)  # Rate limiting\n",
    "        response = requests.get(filing_url, headers=Config.SEC_HEADERS)\n",
    "        response.raise_for_status()\n",
    "        return response.text\n",
    "    \n",
    "    @staticmethod\n",
    "    def _normalize_header_text(text: str) -> str | None:\n",
    "        \"\"\"Normalizes header text to standard format\"\"\"\n",
    "        text = text.strip().upper()\n",
    "        part_match = re.search(r'^\\s*(PART\\s+I{1,2})', text)\n",
    "        if part_match:\n",
    "            return re.sub(r'\\s+', ' ', part_match.group(1))\n",
    "        item_match = re.search(r'^\\s*(ITEM\\s+\\d[A-Z]?)', text)\n",
    "        if item_match:\n",
    "            return re.sub(r'\\s+', ' ', item_match.group(1))\n",
    "        return None\n",
    "\n",
    "    @staticmethod\n",
    "    def _validate_markdown_table(markdown_str: str) -> bool:\n",
    "        \"\"\"\n",
    "        Tests if a markdown table can be successfully parsed into a DataFrame\n",
    "        \"\"\"\n",
    "        try:\n",
    "            lines = [line.strip() for line in markdown_str.strip().split('\\n') if line.strip()]\n",
    "            \n",
    "            if len(lines) < 3:\n",
    "                return False\n",
    "            \n",
    "            # Find separator\n",
    "            separator_index = -1\n",
    "            for i, line in enumerate(lines):\n",
    "                if '|' in line and '---' in line:\n",
    "                    cleaned = line.replace('|', '').replace('-', '').replace(' ', '')\n",
    "                    if len(cleaned) < 5:\n",
    "                        separator_index = i\n",
    "                        break\n",
    "            \n",
    "            if separator_index == -1 or separator_index == 0:\n",
    "                return False\n",
    "            \n",
    "            # Check header\n",
    "            header_line = lines[separator_index - 1]\n",
    "            header = [h.strip() for h in header_line.strip('|').split('|')]\n",
    "            \n",
    "            if not header or len(header) < 2:\n",
    "                return False\n",
    "            \n",
    "            # Check at least one data row exists\n",
    "            if separator_index + 1 >= len(lines):\n",
    "                return False\n",
    "            \n",
    "            # Try to parse with pandas\n",
    "            import pandas as pd\n",
    "            df = pd.read_csv(pd.io.common.StringIO(\n",
    "                '\\n'.join([line.strip('|') for line in lines if '|' in line and '---' not in line])\n",
    "            ), sep='|', skipinitialspace=True)\n",
    "            \n",
    "            return not df.empty and df.shape[1] >= 2\n",
    "            \n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_html_table(table_tag: BeautifulSoup) -> str:\n",
    "        \"\"\"\n",
    "        Converts HTML table to validated Markdown format\n",
    "        --- ENHANCED VERSION with validation ---\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Skip nested tables\n",
    "            if table_tag.find_parent('table'):\n",
    "                return \"\"\n",
    "            \n",
    "            markdown_rows = []\n",
    "            max_cols = 0\n",
    "            \n",
    "            # First pass: collect all rows and find max column count\n",
    "            for tr in table_tag.find_all('tr', recursive=False):\n",
    "                cells = []\n",
    "                for cell in tr.find_all(['td', 'th'], recursive=False):\n",
    "                    # Clean text\n",
    "                    text = \" \".join(cell.get_text(strip=True).split())\n",
    "                    # Handle colspan\n",
    "                    colspan = int(cell.get('colspan', 1))\n",
    "                    cells.extend([text] + [''] * (colspan - 1))\n",
    "                \n",
    "                if cells and any(c for c in cells):  # Skip empty rows\n",
    "                    markdown_rows.append(cells)\n",
    "                    max_cols = max(max_cols, len(cells))\n",
    "            \n",
    "            if not markdown_rows or max_cols < 2:\n",
    "                return \"\"\n",
    "            \n",
    "            # Normalize all rows to have same column count\n",
    "            for row in markdown_rows:\n",
    "                while len(row) < max_cols:\n",
    "                    row.append(\"\")\n",
    "                if len(row) > max_cols:\n",
    "                    row[:] = row[:max_cols]\n",
    "            \n",
    "            # Build markdown\n",
    "            md_output = []\n",
    "            \n",
    "            # Header (use first row or create generic)\n",
    "            header = markdown_rows[0] if markdown_rows else [f\"Col{i}\" for i in range(max_cols)]\n",
    "            header = [h if h else f\"Column_{i}\" for i, h in enumerate(header)]\n",
    "            \n",
    "            md_output.append(\"| \" + \" | \".join(header) + \" |\")\n",
    "            md_output.append(\"| \" + \" | \".join(['---'] * len(header)) + \" |\")\n",
    "            \n",
    "            # Data rows\n",
    "            for row in markdown_rows[1:]:\n",
    "                while len(row) < len(header):\n",
    "                    row.append(\"\")\n",
    "                row = row[:len(header)]\n",
    "                md_output.append(\"| \" + \" | \".join(row) + \" |\")\n",
    "            \n",
    "            result = \"\\n\" + \"\\n\".join(md_output) + \"\\n\"\n",
    "            \n",
    "            # Validate the table before returning\n",
    "            if not SECDocumentLoader._validate_markdown_table(result):\n",
    "                return \"\"\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  [Table Parser Error] {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    @classmethod\n",
    "    def parse_10q(cls, html_content: str) -> dict:\n",
    "        \"\"\"\n",
    "        Parses 10-Q HTML with enhanced table handling\n",
    "        \"\"\"\n",
    "        print(\"  â†’ Parsing HTML content...\")\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        potential_headers = soup.find_all(['p', 'b', 'strong', 'div'])\n",
    "        doc_headers = []\n",
    "        \n",
    "        for header in potential_headers:\n",
    "            text = header.get_text(strip=True)\n",
    "            if len(text) > 100:\n",
    "                continue\n",
    "            \n",
    "            normalized_key = cls._normalize_header_text(text)\n",
    "            if normalized_key:\n",
    "                if not header.find_parent('a'):\n",
    "                    doc_headers.append({'tag': header, 'key': normalized_key})\n",
    "\n",
    "        if not doc_headers:\n",
    "            print(\"  âš  Warning: Could not find any standard Part/Item headers.\")\n",
    "            return {}\n",
    "\n",
    "        parsed_data = defaultdict(lambda: defaultdict(str))\n",
    "        current_part_key = None\n",
    "\n",
    "        for i, header_info in enumerate(doc_headers):\n",
    "            current_key = header_info['key']\n",
    "            \n",
    "            if 'PART' in current_key:\n",
    "                current_part_key = current_key\n",
    "                continue\n",
    "            \n",
    "            if 'ITEM' in current_key:\n",
    "                if not current_part_key:\n",
    "                    current_part_key = \"PART I\"\n",
    "                \n",
    "                start_node = header_info['tag']\n",
    "                end_node = doc_headers[i + 1]['tag'] if i + 1 < len(doc_headers) else None\n",
    "                \n",
    "                content_parts = []\n",
    "                element = start_node.next_element\n",
    "                \n",
    "                while element and element != end_node:\n",
    "                    if isinstance(element, NavigableString):\n",
    "                        if not element.find_parent('table'):\n",
    "                            text = element.strip()\n",
    "                            if text:\n",
    "                                content_parts.append(text)\n",
    "                    elif element.name == 'table':\n",
    "                        if not element.find_parent('table'):\n",
    "                            table_markdown = cls._parse_html_table(element)\n",
    "                            if table_markdown:  # Only add if validation passed\n",
    "                                content_parts.append(table_markdown)\n",
    "                    \n",
    "                    element = element.next_element\n",
    "\n",
    "                full_content = \"\\n\".join(content_parts)\n",
    "                clean_content = re.sub(r'\\n{3,}', '\\n\\n', full_content).strip()\n",
    "                parsed_data[current_part_key][current_key] = clean_content\n",
    "\n",
    "        print(\"  â†’ Parsing complete.\")\n",
    "        return {part: dict(items) for part, items in parsed_data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a882e8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# 3. DOCUMENT PROCESSOR\n",
    "# ===========================================================================\n",
    "class DocumentProcessor:\n",
    "    \"\"\"\n",
    "    Enhanced processor that handles tables specially to prevent chunking\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        print(f\"\\n Loading embedding model: {embedding_model_name}\")\n",
    "        self.model = SentenceTransformer(embedding_model_name)\n",
    "        print(f\"   âœ“ Model loaded (dimension: {self.model.get_sentence_embedding_dimension()})\")\n",
    "        \n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=800,\n",
    "            chunk_overlap=200,\n",
    "            length_function=len,\n",
    "            add_start_index=False\n",
    "        )\n",
    "        print(f\"   âœ“ Initialized RecursiveCharacterTextSplitter\")\n",
    "    \n",
    "\n",
    "    def generate_document_chunks(self, parsed_data: Dict, metadata: Dict, \n",
    "                                 embed_batch_size: int = 512):\n",
    "        \"\"\"\n",
    "        Process documents (treats all content as text)\n",
    "        \"\"\"\n",
    "        all_docs = []\n",
    "        \n",
    "        for part, items in parsed_data.items():\n",
    "            for item, content in items.items():\n",
    "                if not content:\n",
    "                    continue\n",
    "\n",
    "                doc_metadata = {\n",
    "                    'ticker': metadata['ticker'],\n",
    "                    'company_name': metadata['company_name'],\n",
    "                    'filing_date': metadata['filing_date'],\n",
    "                    'filing_url': metadata['filing_url'],\n",
    "                    'part': part,\n",
    "                    'item': item,\n",
    "                }\n",
    "                \n",
    "                doc = Document(page_content=content, metadata=doc_metadata)\n",
    "                all_docs.append(doc)\n",
    "\n",
    "        if not all_docs:\n",
    "            return\n",
    "        \n",
    "        # Process all documents (tables will be chunked just like text)\n",
    "        print(f\"     â†’ Splitting {len(all_docs)} text documents into chunks...\")\n",
    "        chunked_docs = self.text_splitter.split_documents(all_docs)\n",
    "        print(f\"     â†’ Generated {len(chunked_docs)} total chunks\")\n",
    "        \n",
    "        # Batch embedding\n",
    "        text_batch = []\n",
    "        metadata_batch = []\n",
    "        \n",
    "        for chunk in chunked_docs:\n",
    "            text_batch.append(chunk.page_content)\n",
    "            metadata_batch.append(chunk.metadata)\n",
    "            \n",
    "            if len(text_batch) >= embed_batch_size:\n",
    "                embeddings = self.model.encode(text_batch, show_progress_bar=False)\n",
    "                \n",
    "                for txt, emb, meta in zip(text_batch, embeddings, metadata_batch):\n",
    "                    payload = {'text': txt, **meta}\n",
    "                    yield PointStruct(\n",
    "                        id=str(uuid.uuid4()),\n",
    "                        vector=emb.tolist(),\n",
    "                        payload=payload\n",
    "                    )\n",
    "                \n",
    "                text_batch = []\n",
    "                metadata_batch = []\n",
    "        \n",
    "        # Process remaining\n",
    "        if text_batch:\n",
    "            embeddings = self.model.encode(text_batch, show_progress_bar=False)\n",
    "            \n",
    "            for txt, emb, meta in zip(text_batch, embeddings, metadata_batch):\n",
    "                payload = {'text': txt, **meta}\n",
    "                yield PointStruct(\n",
    "                    id=str(uuid.uuid4()),\n",
    "                    vector=emb.tolist(),\n",
    "                    payload=payload\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ca595d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# 4. QDRANT MANAGER\n",
    "# ===========================================================================\n",
    "class QdrantManager:\n",
    "    def __init__(self):\n",
    "        print(f\"\\nConnecting to Qdrant Cloud...\")\n",
    "        self.client = QdrantClient(url=Config.QDRANT_URL, api_key=Config.QDRANT_API_KEY)\n",
    "        print(f\"   âœ“ Connected to Qdrant\")\n",
    "        \n",
    "    def create_collection(self, collection_name: str = Config.COLLECTION_NAME, vector_size: int = Config.VECTOR_SIZE):\n",
    "        print(f\"\\n Setting up collection: {collection_name}\")\n",
    "        try:\n",
    "            collections = self.client.get_collections().collections\n",
    "            exists = any(col.name == collection_name for col in collections)\n",
    "            if exists:\n",
    "                print(f\"   âš  Collection '{collection_name}' exists, recreating...\")\n",
    "                self.client.delete_collection(collection_name)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not check collections: {e}. Assuming it does not exist.\")\n",
    "            \n",
    "        self.client.create_collection(collection_name=collection_name, vectors_config=models.VectorParams(size=vector_size, distance=models.Distance.COSINE))\n",
    "        print(f\"   âœ“ Collection created\")\n",
    "        \n",
    "        for field in [\"ticker\", \"item\"]:\n",
    "            print(f\"   â†’ Creating payload index for '{field}' (Keyword)...\")\n",
    "            self.client.create_payload_index(collection_name=collection_name, field_name=field, field_schema=models.PayloadSchemaType.KEYWORD)\n",
    "            \n",
    "        print(f\"   âœ“ Payload indexes created.\")\n",
    "\n",
    "    def upsert_documents(self, points_generator, collection_name: str = Config.COLLECTION_NAME, batch_size: int = 512) -> int:\n",
    "        \n",
    "        print(f\" Uploading chunks to Qdrant in batches of {batch_size}...\"); \n",
    "        batch, count = [], 0\n",
    "        \n",
    "        for point in points_generator:\n",
    "            batch.append(point)\n",
    "            \n",
    "            if len(batch) >= batch_size:\n",
    "                self.client.upsert(\n",
    "                    collection_name=collection_name, \n",
    "                    points=batch, \n",
    "                    wait=True \n",
    "                )\n",
    "                count += len(batch)\n",
    "                print(f\"     â†’ Uploaded {count} chunks...\")\n",
    "                batch = []\n",
    "                \n",
    "        if batch: \n",
    "            self.client.upsert(\n",
    "                collection_name=collection_name, \n",
    "                points=batch, \n",
    "                wait=True\n",
    "            )\n",
    "            count += len(batch)\n",
    "            \n",
    "        print(f\"  âœ“ All chunks uploaded. Total: {count}\"); \n",
    "        return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eccf649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# 5. MAIN INGESTION PIPELINE\n",
    "# ===========================================================================\n",
    "class SECFilingRAGPipeline:\n",
    "    def __init__(self):\n",
    "        print(\"=\" * 70); print(\"SEC 10-Q RAG SYSTEM (Golden Database Builder)\"); print(\"=\" * 70)\n",
    "        self.loader = SECDocumentLoader()\n",
    "        self.processor = DocumentProcessor()\n",
    "        self.qdrant_manager = QdrantManager()\n",
    "\n",
    "    def load_and_index_filings(self, tickers: List[str] = Config.TICKERS, num_filings_per_ticker: int = 4):\n",
    "        print(f\"\\n{'=' * 70}\\nLOADING & INDEXING PHASE\\n{'=' * 70}\")\n",
    "        print(f\"\\nProcessing {len(tickers)} companies: {', '.join(tickers)}\")\n",
    "        print(f\"(Fetching {num_filings_per_ticker} filings per company)\\n\")\n",
    "        \n",
    "        self.qdrant_manager.create_collection()\n",
    "        \n",
    "        try:\n",
    "            count = self.qdrant_manager.client.count(Config.COLLECTION_NAME, exact=True)\n",
    "            if count.count > 0:\n",
    "                print(f\"Collection '{Config.COLLECTION_NAME}' already has {count.count} docs. SKIPPING INDEXING.\")\n",
    "                print(\"To re-index, please delete the collection in your Qdrant dashboard.\")\n",
    "                print(f\"{'=' * 70}\\n\"); return\n",
    "        except Exception: \n",
    "            print(\"Collection is new. Proceeding with indexing.\")\n",
    "\n",
    "        successful_tickers, failed_tickers = [], []; total_chunks_indexed = 0\n",
    "        for idx, ticker in enumerate( tickers, 1):\n",
    "            print(f\"\\n[{idx}/{len(tickers)}] Processing {ticker}\"); print(\"-\" * 70)\n",
    "            ticker_chunks_count, num_filings_processed = 0, 0\n",
    "            try:\n",
    "                # 1. Get METADATA (list of URLs, dates, etc.)\n",
    "                filings_metadata_list = self.loader.get_recent_10q_metadata(ticker, num_filings=num_filings_per_ticker)\n",
    "                \n",
    "                for filing_metadata in filings_metadata_list:\n",
    "                    try:\n",
    "                        # 2. Get the HTML for ONE filing\n",
    "                        print(f\"  â†’ Downloading filing from: {filing_metadata['filing_date']}...\"); \n",
    "                        html_content = self.loader.get_filing_html(filing_metadata['filing_url'])\n",
    "                        \n",
    "                        # 3. Parse ONE filing\n",
    "                        parsed_data = self.loader.parse_10q(html_content)\n",
    "                        if not parsed_data: \n",
    "                            print(f\"  âš  Warning: No data parsed for {ticker} on {filing_metadata['filing_date']}\"); \n",
    "                            del html_content; gc.collect(); continue\n",
    "                        \n",
    "                        # 4. Chunk and embed\n",
    "                        print(f\"  â†’ Creating chunks and embeddings generator...\")\n",
    "                        chunks_generator = self.processor.generate_document_chunks(parsed_data, filing_metadata)\n",
    "                        \n",
    "                        # 5. Upload\n",
    "                        num_uploaded = self.qdrant_manager.upsert_documents(chunks_generator)\n",
    "                        if num_uploaded > 0:\n",
    "                            ticker_chunks_count += num_uploaded; total_chunks_indexed += num_uploaded; num_filings_processed += 1\n",
    "                        \n",
    "                        print(f\"  â†’ Cleaning up memory...\"); del html_content; del parsed_data; del chunks_generator; gc.collect(); print(f\"  âœ“ Memory cleaned.\")\n",
    "                    \n",
    "                    except Exception as e: \n",
    "                        print(f\"  âœ— Error processing filing for {ticker} ({filing_metadata.get('filing_date', 'unknown')}): {e}\"); \n",
    "                        gc.collect()\n",
    "                \n",
    "                if ticker_chunks_count > 0: \n",
    "                    successful_tickers.append(ticker); print(f\"  âœ“ Finished {ticker}. Chunks: {ticker_chunks_count}\")\n",
    "                else: \n",
    "                    failed_tickers.append(ticker); print(f\"  âš  No chunks created for {ticker}\")\n",
    "            \n",
    "            except Exception as e: \n",
    "                print(f\"  âœ— Error processing {ticker} (metadata fetch failed): {e}\"); \n",
    "                failed_tickers.append(ticker)\n",
    "            \n",
    "            if idx < len(tickers): time.sleep(0.2)\n",
    "            \n",
    "        print(f\"\\n{'=' * 70}\\nINDEXING COMPLETE\\n{'=' * 70}\")\n",
    "        print(f\"âœ“ Success: {len(successful_tickers)} companies ({', '.join(successful_tickers)})\")\n",
    "        if failed_tickers: print(f\"âœ— Failed: {len(failed_tickers)} companies ({', '.join(failed_tickers)})\")\n",
    "        print(f\"\\n Total chunks indexed: {total_chunks_indexed}\\n{'=' * 70}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9765152b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SEC 10-Q RAG SYSTEM (Golden Database Builder)\n",
      "======================================================================\n",
      "\n",
      "ðŸ“¦ Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "   âœ“ Model loaded (dimension: 384)\n",
      "   âœ“ Initialized RecursiveCharacterTextSplitter\n",
      "\n",
      "Connecting to Qdrant Cloud...\n",
      "   âœ“ Connected to Qdrant\n",
      "\n",
      "======================================================================\n",
      "LOADING & INDEXING PHASE\n",
      "======================================================================\n",
      "\n",
      "Processing 10 companies: NVDA, AAPL, MSFT, AMZN, META, GOOGL, TSLA, ORCL, JPM, AMD\n",
      "(Fetching 4 filings per company)\n",
      "\n",
      "\n",
      " Setting up collection: sec_filings_10q_GOLDEN_BENCHMARK\n",
      "   âš  Collection 'sec_filings_10q_GOLDEN_BENCHMARK' exists, recreating...\n",
      "   âœ“ Collection created\n",
      "   â†’ Creating payload index for 'ticker' (Keyword)...\n",
      "   â†’ Creating payload index for 'item' (Keyword)...\n",
      "   âœ“ Payload indexes created.\n",
      "\n",
      "[1/10] Processing NVDA\n",
      "----------------------------------------------------------------------\n",
      "  â†’ Fetching CIK for ticker: NVDA...\n",
      "  â†’ Found CIK: 0001045810 (NVIDIA CORP)\n",
      "  â†’ Found 4 recent 10-Q filing metadata entries.\n",
      "  â†’ Downloading filing from: 2025-08-27...\n",
      "  â†’ Parsing HTML content...\n",
      "  â†’ Parsing complete.\n",
      "  â†’ Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 512...\n",
      "     â†’ Splitting 9 text documents into chunks...\n",
      "     â†’ Generated 329 total chunks\n",
      "  âœ“ All chunks uploaded. Total: 329\n",
      "  â†’ Cleaning up memory...\n",
      "  âœ“ Memory cleaned.\n",
      "  â†’ Downloading filing from: 2025-05-28...\n",
      "  â†’ Parsing HTML content...\n",
      "  â†’ Parsing complete.\n",
      "  â†’ Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 512...\n",
      "     â†’ Splitting 9 text documents into chunks...\n",
      "     â†’ Generated 322 total chunks\n",
      "  âœ“ All chunks uploaded. Total: 322\n",
      "  â†’ Cleaning up memory...\n",
      "  âœ“ Memory cleaned.\n",
      "  â†’ Downloading filing from: 2024-11-20...\n",
      "  â†’ Parsing HTML content...\n",
      "  â†’ Parsing complete.\n",
      "  â†’ Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 512...\n",
      "     â†’ Splitting 9 text documents into chunks...\n",
      "     â†’ Generated 336 total chunks\n",
      "  âœ“ All chunks uploaded. Total: 336\n",
      "  â†’ Cleaning up memory...\n",
      "  âœ“ Memory cleaned.\n",
      "  â†’ Downloading filing from: 2024-08-28...\n",
      "  â†’ Parsing HTML content...\n",
      "  â†’ Parsing complete.\n",
      "  â†’ Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 512...\n",
      "     â†’ Splitting 9 text documents into chunks...\n",
      "     â†’ Generated 355 total chunks\n",
      "  âœ“ All chunks uploaded. Total: 355\n",
      "  â†’ Cleaning up memory...\n",
      "  âœ“ Memory cleaned.\n",
      "  âœ“ Finished NVDA. Chunks: 1342\n",
      "\n",
      "[2/10] Processing AAPL\n",
      "----------------------------------------------------------------------\n",
      "  â†’ Fetching CIK for ticker: AAPL...\n",
      "  â†’ Found CIK: 0000320193 (Apple Inc.)\n",
      "  â†’ Found 4 recent 10-Q filing metadata entries.\n",
      "  â†’ Downloading filing from: 2025-08-01...\n",
      "  â†’ Parsing HTML content...\n",
      "  â†’ Parsing complete.\n",
      "  â†’ Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 512...\n",
      "     â†’ Splitting 11 text documents into chunks...\n",
      "     â†’ Generated 166 total chunks\n",
      "  âœ“ All chunks uploaded. Total: 166\n",
      "  â†’ Cleaning up memory...\n",
      "  âœ“ Memory cleaned.\n",
      "  â†’ Downloading filing from: 2025-05-02...\n",
      "  â†’ Parsing HTML content...\n",
      "  â†’ Parsing complete.\n",
      "  â†’ Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 512...\n",
      "     â†’ Splitting 11 text documents into chunks...\n",
      "     â†’ Generated 185 total chunks\n",
      "  âœ“ All chunks uploaded. Total: 185\n",
      "  â†’ Cleaning up memory...\n",
      "  âœ“ Memory cleaned.\n",
      "  â†’ Downloading filing from: 2025-01-31...\n",
      "  â†’ Parsing HTML content...\n",
      "  â†’ Parsing complete.\n",
      "  â†’ Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 512...\n",
      "     â†’ Splitting 11 text documents into chunks...\n",
      "     â†’ Generated 171 total chunks\n",
      "  âœ“ All chunks uploaded. Total: 171\n",
      "  â†’ Cleaning up memory...\n",
      "  âœ“ Memory cleaned.\n",
      "  â†’ Downloading filing from: 2024-08-02...\n",
      "  â†’ Parsing HTML content...\n",
      "  â†’ Parsing complete.\n",
      "  â†’ Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 512...\n",
      "     â†’ Splitting 11 text documents into chunks...\n",
      "     â†’ Generated 162 total chunks\n",
      "  âœ“ All chunks uploaded. Total: 162\n",
      "  â†’ Cleaning up memory...\n",
      "  âœ“ Memory cleaned.\n",
      "  âœ“ Finished AAPL. Chunks: 684\n",
      "\n",
      "[3/10] Processing MSFT\n",
      "----------------------------------------------------------------------\n",
      "  â†’ Fetching CIK for ticker: MSFT...\n",
      "  â†’ Found CIK: 0000789019 (MICROSOFT CORP)\n",
      "  â†’ Found 4 recent 10-Q filing metadata entries.\n",
      "  â†’ Downloading filing from: 2025-10-29...\n",
      "  â†’ Parsing HTML content...\n",
      "  â†’ Parsing complete.\n",
      "  â†’ Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 512...\n",
      "     â†’ Splitting 9 text documents into chunks...\n",
      "     â†’ Generated 35 total chunks\n",
      "  âœ“ All chunks uploaded. Total: 35\n",
      "  â†’ Cleaning up memory...\n",
      "  âœ“ Memory cleaned.\n",
      "  â†’ Downloading filing from: 2025-04-30...\n",
      "  â†’ Parsing HTML content...\n",
      "  â†’ Parsing complete.\n",
      "  â†’ Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 512...\n",
      "     â†’ Splitting 9 text documents into chunks...\n",
      "     â†’ Generated 46 total chunks\n",
      "  âœ“ All chunks uploaded. Total: 46\n",
      "  â†’ Cleaning up memory...\n",
      "  âœ“ Memory cleaned.\n",
      "  â†’ Downloading filing from: 2025-01-29...\n",
      "  â†’ Parsing HTML content...\n",
      "  â†’ Parsing complete.\n",
      "  â†’ Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 512...\n",
      "     â†’ Splitting 9 text documents into chunks...\n",
      "     â†’ Generated 40 total chunks\n",
      "  âœ“ All chunks uploaded. Total: 40\n",
      "  â†’ Cleaning up memory...\n",
      "  âœ“ Memory cleaned.\n",
      "  â†’ Downloading filing from: 2024-10-30...\n",
      "  â†’ Parsing HTML content...\n",
      "  â†’ Parsing complete.\n",
      "  â†’ Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 512...\n",
      "     â†’ Splitting 9 text documents into chunks...\n",
      "     â†’ Generated 41 total chunks\n",
      "  âœ“ All chunks uploaded. Total: 41\n",
      "  â†’ Cleaning up memory...\n",
      "  âœ“ Memory cleaned.\n",
      "  âœ“ Finished MSFT. Chunks: 162\n",
      "\n",
      "[4/10] Processing AMZN\n",
      "----------------------------------------------------------------------\n",
      "  â†’ Fetching CIK for ticker: AMZN...\n",
      "  â†’ Found CIK: 0001018724 (AMAZON COM INC)\n",
      "  â†’ Found 4 recent 10-Q filing metadata entries.\n",
      "  â†’ Downloading filing from: 2025-10-31...\n",
      "  â†’ Parsing HTML content...\n",
      "  â†’ Parsing complete.\n",
      "  â†’ Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 512...\n",
      "     â†’ Splitting 11 text documents into chunks...\n",
      "     â†’ Generated 415 total chunks\n",
      "  âœ“ All chunks uploaded. Total: 415\n",
      "  â†’ Cleaning up memory...\n",
      "  âœ“ Memory cleaned.\n",
      "  â†’ Downloading filing from: 2025-08-01...\n",
      "  â†’ Parsing HTML content...\n",
      "  â†’ Parsing complete.\n",
      "  â†’ Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 512...\n",
      "     â†’ Splitting 11 text documents into chunks...\n",
      "     â†’ Generated 402 total chunks\n",
      "  âœ“ All chunks uploaded. Total: 402\n",
      "  â†’ Cleaning up memory...\n",
      "  âœ“ Memory cleaned.\n",
      "  â†’ Downloading filing from: 2025-05-02...\n",
      "  â†’ Parsing HTML content...\n",
      "  â†’ Parsing complete.\n",
      "  â†’ Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 512...\n",
      "     â†’ Splitting 11 text documents into chunks...\n",
      "     â†’ Generated 379 total chunks\n",
      "  âœ“ All chunks uploaded. Total: 379\n",
      "  â†’ Cleaning up memory...\n",
      "  âœ“ Memory cleaned.\n",
      "  â†’ Downloading filing from: 2024-11-01...\n",
      "  â†’ Parsing HTML content...\n",
      "  â†’ Parsing complete.\n",
      "  â†’ Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 512...\n",
      "     â†’ Splitting 11 text documents into chunks...\n",
      "     â†’ Generated 417 total chunks\n",
      "  âœ“ All chunks uploaded. Total: 417\n",
      "  â†’ Cleaning up memory...\n",
      "  âœ“ Memory cleaned.\n",
      "  âœ“ Finished AMZN. Chunks: 1613\n",
      "\n",
      "[5/10] Processing META\n",
      "----------------------------------------------------------------------\n",
      "  â†’ Fetching CIK for ticker: META...\n",
      "  â†’ Found CIK: 0001326801 (Meta Platforms, Inc.)\n",
      "  â†’ Found 4 recent 10-Q filing metadata entries.\n",
      "  â†’ Downloading filing from: 2025-10-30...\n",
      "  â†’ Parsing HTML content...\n",
      "  â†’ Parsing complete.\n",
      "  â†’ Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 512...\n",
      "     â†’ Splitting 9 text documents into chunks...\n",
      "     â†’ Generated 782 total chunks\n",
      "     â†’ Uploaded 512 chunks...\n",
      "  âœ“ All chunks uploaded. Total: 782\n",
      "  â†’ Cleaning up memory...\n",
      "  âœ“ Memory cleaned.\n",
      "  â†’ Downloading filing from: 2025-07-31...\n",
      "  â†’ Parsing HTML content...\n",
      "  â†’ Parsing complete.\n",
      "  â†’ Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 512...\n",
      "     â†’ Splitting 9 text documents into chunks...\n",
      "     â†’ Generated 748 total chunks\n",
      "     â†’ Uploaded 512 chunks...\n",
      "  âœ“ All chunks uploaded. Total: 748\n",
      "  â†’ Cleaning up memory...\n",
      "  âœ“ Memory cleaned.\n",
      "  â†’ Downloading filing from: 2025-05-01...\n",
      "  â†’ Parsing HTML content...\n",
      "  â†’ Parsing complete.\n",
      "  â†’ Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 512...\n",
      "     â†’ Splitting 9 text documents into chunks...\n",
      "     â†’ Generated 717 total chunks\n",
      "     â†’ Uploaded 512 chunks...\n",
      "  âœ“ All chunks uploaded. Total: 717\n",
      "  â†’ Cleaning up memory...\n",
      "  âœ“ Memory cleaned.\n",
      "  â†’ Downloading filing from: 2024-10-31...\n",
      "  â†’ Parsing HTML content...\n",
      "  â†’ Parsing complete.\n",
      "  â†’ Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 512...\n",
      "     â†’ Splitting 9 text documents into chunks...\n",
      "     â†’ Generated 746 total chunks\n",
      "     â†’ Uploaded 512 chunks...\n",
      "  âœ“ All chunks uploaded. Total: 746\n",
      "  â†’ Cleaning up memory...\n",
      "  âœ“ Memory cleaned.\n",
      "  âœ“ Finished META. Chunks: 2993\n",
      "\n",
      "[6/10] Processing GOOGL\n",
      "----------------------------------------------------------------------\n",
      "  â†’ Fetching CIK for ticker: GOOGL...\n",
      "  â†’ Found CIK: 0001652044 (Alphabet Inc.)\n",
      "  â†’ Found 4 recent 10-Q filing metadata entries.\n",
      "  â†’ Downloading filing from: 2025-10-30...\n",
      "  â†’ Parsing HTML content...\n",
      "  â†’ Parsing complete.\n",
      "  â†’ Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 512...\n",
      "     â†’ Splitting 9 text documents into chunks...\n",
      "     â†’ Generated 411 total chunks\n",
      "  âœ“ All chunks uploaded. Total: 411\n",
      "  â†’ Cleaning up memory...\n",
      "  âœ“ Memory cleaned.\n",
      "  â†’ Downloading filing from: 2025-07-24...\n",
      "  â†’ Parsing HTML content...\n",
      "  â†’ Parsing complete.\n",
      "  â†’ Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 512...\n",
      "     â†’ Splitting 9 text documents into chunks...\n",
      "     â†’ Generated 414 total chunks\n",
      "  âœ“ All chunks uploaded. Total: 414\n",
      "  â†’ Cleaning up memory...\n",
      "  âœ“ Memory cleaned.\n",
      "  â†’ Downloading filing from: 2025-04-25...\n",
      "  â†’ Parsing HTML content...\n",
      "  â†’ Parsing complete.\n",
      "  â†’ Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 512...\n",
      "     â†’ Splitting 9 text documents into chunks...\n",
      "     â†’ Generated 363 total chunks\n",
      "  âœ“ All chunks uploaded. Total: 363\n",
      "  â†’ Cleaning up memory...\n",
      "  âœ“ Memory cleaned.\n",
      "  â†’ Downloading filing from: 2024-10-30...\n",
      "  â†’ Parsing HTML content...\n",
      "  â†’ Parsing complete.\n",
      "  â†’ Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 512...\n",
      "     â†’ Splitting 9 text documents into chunks...\n",
      "     â†’ Generated 407 total chunks\n",
      "  âœ“ All chunks uploaded. Total: 407\n",
      "  â†’ Cleaning up memory...\n",
      "  âœ“ Memory cleaned.\n",
      "  âœ“ Finished GOOGL. Chunks: 1595\n",
      "\n",
      "[7/10] Processing TSLA\n",
      "----------------------------------------------------------------------\n",
      "  â†’ Fetching CIK for ticker: TSLA...\n",
      "  â†’ Found CIK: 0001318605 (Tesla, Inc.)\n",
      "  â†’ Found 4 recent 10-Q filing metadata entries.\n",
      "  â†’ Downloading filing from: 2025-10-23...\n",
      "  â†’ Parsing HTML content...\n",
      "  â†’ Parsing complete.\n",
      "  â†’ Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 512...\n",
      "     â†’ Splitting 11 text documents into chunks...\n",
      "     â†’ Generated 337 total chunks\n",
      "  âœ“ All chunks uploaded. Total: 337\n",
      "  â†’ Cleaning up memory...\n",
      "  âœ“ Memory cleaned.\n",
      "  â†’ Downloading filing from: 2025-07-24...\n",
      "  â†’ Parsing HTML content...\n",
      "  â†’ Parsing complete.\n",
      "  â†’ Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 512...\n",
      "     â†’ Splitting 11 text documents into chunks...\n",
      "     â†’ Generated 315 total chunks\n",
      "  âœ“ All chunks uploaded. Total: 315\n",
      "  â†’ Cleaning up memory...\n",
      "  âœ“ Memory cleaned.\n",
      "  â†’ Downloading filing from: 2025-04-23...\n",
      "  â†’ Parsing HTML content...\n",
      "  â†’ Parsing complete.\n",
      "  â†’ Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 512...\n",
      "     â†’ Splitting 11 text documents into chunks...\n",
      "     â†’ Generated 291 total chunks\n",
      "  âœ“ All chunks uploaded. Total: 291\n",
      "  â†’ Cleaning up memory...\n",
      "  âœ“ Memory cleaned.\n",
      "  â†’ Downloading filing from: 2024-10-24...\n",
      "  â†’ Parsing HTML content...\n",
      "  â†’ Parsing complete.\n",
      "  â†’ Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 512...\n",
      "     â†’ Splitting 11 text documents into chunks...\n",
      "     â†’ Generated 314 total chunks\n",
      "  âœ“ All chunks uploaded. Total: 314\n",
      "  â†’ Cleaning up memory...\n",
      "  âœ“ Memory cleaned.\n",
      "  âœ“ Finished TSLA. Chunks: 1257\n",
      "\n",
      "[8/10] Processing ORCL\n",
      "----------------------------------------------------------------------\n",
      "  â†’ Fetching CIK for ticker: ORCL...\n",
      "  â†’ Found CIK: 0001341439 (ORACLE CORP)\n",
      "  â†’ Found 4 recent 10-Q filing metadata entries.\n",
      "  â†’ Downloading filing from: 2025-09-10...\n",
      "  â†’ Parsing HTML content...\n",
      "  â†’ Parsing complete.\n",
      "  â†’ Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 512...\n",
      "     â†’ Splitting 9 text documents into chunks...\n",
      "     â†’ Generated 284 total chunks\n",
      "  âœ“ All chunks uploaded. Total: 284\n",
      "  â†’ Cleaning up memory...\n",
      "  âœ“ Memory cleaned.\n",
      "  â†’ Downloading filing from: 2025-03-11...\n",
      "  â†’ Parsing HTML content...\n",
      "  â†’ Parsing complete.\n",
      "  â†’ Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 512...\n",
      "     â†’ Splitting 9 text documents into chunks...\n",
      "     â†’ Generated 331 total chunks\n",
      "  âœ“ All chunks uploaded. Total: 331\n",
      "  â†’ Cleaning up memory...\n",
      "  âœ“ Memory cleaned.\n",
      "  â†’ Downloading filing from: 2024-12-10...\n",
      "  â†’ Parsing HTML content...\n",
      "  â†’ Parsing complete.\n",
      "  â†’ Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 512...\n",
      "     â†’ Splitting 9 text documents into chunks...\n",
      "     â†’ Generated 333 total chunks\n",
      "  âœ“ All chunks uploaded. Total: 333\n",
      "  â†’ Cleaning up memory...\n",
      "  âœ“ Memory cleaned.\n",
      "  â†’ Downloading filing from: 2024-09-10...\n",
      "  â†’ Parsing HTML content...\n",
      "  â†’ Parsing complete.\n",
      "  â†’ Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 512...\n",
      "     â†’ Splitting 8 text documents into chunks...\n",
      "     â†’ Generated 283 total chunks\n",
      "  âœ“ All chunks uploaded. Total: 283\n",
      "  â†’ Cleaning up memory...\n",
      "  âœ“ Memory cleaned.\n",
      "  âœ“ Finished ORCL. Chunks: 1231\n",
      "\n",
      "[9/10] Processing JPM\n",
      "----------------------------------------------------------------------\n",
      "  â†’ Fetching CIK for ticker: JPM...\n",
      "  â†’ Found CIK: 0000019617 (JPMORGAN CHASE & CO)\n",
      "  â†’ Found 3 recent 10-Q filing metadata entries.\n",
      "  â†’ Downloading filing from: 2025-11-04...\n",
      "  â†’ Parsing HTML content...\n",
      "  â†’ Parsing complete.\n",
      "  â†’ Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 512...\n",
      "     â†’ Splitting 8 text documents into chunks...\n",
      "     â†’ Generated 34 total chunks\n",
      "  âœ“ All chunks uploaded. Total: 34\n",
      "  â†’ Cleaning up memory...\n",
      "  âœ“ Memory cleaned.\n",
      "  â†’ Downloading filing from: 2025-08-05...\n",
      "  â†’ Parsing HTML content...\n",
      "  â†’ Parsing complete.\n",
      "  â†’ Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 512...\n",
      "     â†’ Splitting 8 text documents into chunks...\n",
      "     â†’ Generated 26 total chunks\n",
      "  âœ“ All chunks uploaded. Total: 26\n",
      "  â†’ Cleaning up memory...\n",
      "  âœ“ Memory cleaned.\n",
      "  â†’ Downloading filing from: 2025-05-01...\n",
      "  â†’ Parsing HTML content...\n",
      "  â†’ Parsing complete.\n",
      "  â†’ Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 512...\n",
      "     â†’ Splitting 8 text documents into chunks...\n",
      "     â†’ Generated 29 total chunks\n",
      "  âœ“ All chunks uploaded. Total: 29\n",
      "  â†’ Cleaning up memory...\n",
      "  âœ“ Memory cleaned.\n",
      "  âœ“ Finished JPM. Chunks: 89\n",
      "\n",
      "[10/10] Processing AMD\n",
      "----------------------------------------------------------------------\n",
      "  â†’ Fetching CIK for ticker: AMD...\n",
      "  â†’ Found CIK: 0000002488 (ADVANCED MICRO DEVICES INC)\n",
      "  â†’ Found 4 recent 10-Q filing metadata entries.\n",
      "  â†’ Downloading filing from: 2025-11-05...\n",
      "  â†’ Parsing HTML content...\n",
      "  â†’ Parsing complete.\n",
      "  â†’ Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 512...\n",
      "     â†’ Splitting 9 text documents into chunks...\n",
      "     â†’ Generated 483 total chunks\n",
      "  âœ“ All chunks uploaded. Total: 483\n",
      "  â†’ Cleaning up memory...\n",
      "  âœ“ Memory cleaned.\n",
      "  â†’ Downloading filing from: 2025-08-06...\n",
      "  â†’ Parsing HTML content...\n",
      "  â†’ Parsing complete.\n",
      "  â†’ Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 512...\n",
      "     â†’ Splitting 9 text documents into chunks...\n",
      "     â†’ Generated 476 total chunks\n",
      "  âœ“ All chunks uploaded. Total: 476\n",
      "  â†’ Cleaning up memory...\n",
      "  âœ“ Memory cleaned.\n",
      "  â†’ Downloading filing from: 2025-05-07...\n",
      "  â†’ Parsing HTML content...\n",
      "  â†’ Parsing complete.\n",
      "  â†’ Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 512...\n",
      "     â†’ Splitting 9 text documents into chunks...\n",
      "     â†’ Generated 440 total chunks\n",
      "  âœ“ All chunks uploaded. Total: 440\n",
      "  â†’ Cleaning up memory...\n",
      "  âœ“ Memory cleaned.\n",
      "  â†’ Downloading filing from: 2024-10-30...\n",
      "  â†’ Parsing HTML content...\n",
      "  â†’ Parsing complete.\n",
      "  â†’ Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 512...\n",
      "     â†’ Splitting 9 text documents into chunks...\n",
      "     â†’ Generated 442 total chunks\n",
      "  âœ“ All chunks uploaded. Total: 442\n",
      "  â†’ Cleaning up memory...\n",
      "  âœ“ Memory cleaned.\n",
      "  âœ“ Finished AMD. Chunks: 1841\n",
      "\n",
      "======================================================================\n",
      "INDEXING COMPLETE\n",
      "======================================================================\n",
      "âœ“ Success: 10 companies (NVDA, AAPL, MSFT, AMZN, META, GOOGL, TSLA, ORCL, JPM, AMD)\n",
      "\n",
      " Total chunks indexed: 12807\n",
      "======================================================================\n",
      "\n",
      "Database build complete.\n"
     ]
    }
   ],
   "source": [
    "# ===========================================================================\n",
    "# 6. RUN THE INGESTION\n",
    "# ===========================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    pipeline = SECFilingRAGPipeline()\n",
    "    pipeline.load_and_index_filings(num_filings_per_ticker=4)\n",
    "    print(\"Database build complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
