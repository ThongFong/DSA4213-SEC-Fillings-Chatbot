{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dedd3c94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import uuid\n",
    "import time\n",
    "import requests\n",
    "import gc\n",
    "import json\n",
    "import unicodedata\n",
    "import pathlib\n",
    "from typing import List, Dict, Tuple, Any, Optional\n",
    "from collections import defaultdict\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "# External libraries\n",
    "from dotenv import load_dotenv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient, models\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
    "from openai import OpenAI\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Imports for QueryProcessor\n",
    "import spacy\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49fcfb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# PART 1: CONFIGURATION & SETUP\n",
    "# ===========================================================================\n",
    "# (From base_RAG.ipynb)\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Configuration class for API keys and model settings\"\"\"\n",
    "    QDRANT_URL = os.getenv(\"QDRANT_URL\")\n",
    "    QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")\n",
    "    OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "    SEC_HEADERS = {'User-Agent': 'EnhancedRAG-Project research@example.com'}\n",
    "    CIK_MAP_URL = 'https://www.sec.gov/files/company_tickers.json'\n",
    "\n",
    "    # Model Configuration\n",
    "    EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"  # 384 dimensions\n",
    "    LLM_MODEL = \"gpt-4o\"\n",
    "    FLAN_T5_MODEL_NAME = \"google/flan-t5-small\"\n",
    "    SPACY_MODEL = \"en_core_web_sm\" # Use the small, fast model\n",
    "\n",
    "    # Collection Configuration\n",
    "    # We use the same collection as base_RAG, so no re-indexing is needed\n",
    "    COLLECTION_NAME = \"sec_filings_10q\" \n",
    "    VECTOR_SIZE = 384\n",
    "    CHUNK_SIZE = 800\n",
    "    CHUNK_OVERLAP = 200\n",
    "    TOP_K = 5\n",
    "    TICKERS = ['NVDA', 'AAPL', 'MSFT', 'AMZN', 'META', 'GOOGL', 'TSLA', 'ORCL', 'JPM', 'AMD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "177764c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# PART 2: DOCUMENT LOADING\n",
    "# ===========================================================================\n",
    "# (From base_RAG.ipynb)\n",
    "\n",
    "class SECDocumentLoader:\n",
    "    \"\"\"Handles fetching and parsing of SEC 10-Q filings\"\"\"\n",
    "    @staticmethod\n",
    "    def get_recent_10q_metadata(ticker: str, num_filings: int = 4) -> List[Dict[str, str]]:\n",
    "        print(f\"  → Fetching CIK for ticker: {ticker}...\")\n",
    "        response = requests.get(Config.CIK_MAP_URL, headers=Config.SEC_HEADERS)\n",
    "        response.raise_for_status()\n",
    "        company_data = response.json()\n",
    "\n",
    "        cik = None\n",
    "        company_name = None\n",
    "        for company in company_data.values():\n",
    "            if company['ticker'] == ticker.upper():\n",
    "                cik = str(company['cik_str']).zfill(10)\n",
    "                company_name = company['title']\n",
    "                break\n",
    "        if not cik:\n",
    "            raise ValueError(f\"Ticker '{ticker}' not found in SEC CIK mapping.\")\n",
    "        print(f\"  → Found CIK: {cik} ({company_name})\")\n",
    "\n",
    "        submissions_url = f\"https://data.sec.gov/submissions/CIK{cik}.json\"\n",
    "        time.sleep(0.1)\n",
    "        response = requests.get(submissions_url, headers=Config.SEC_HEADERS)\n",
    "        response.raise_for_status()\n",
    "        submissions = response.json()\n",
    "\n",
    "        filings_metadata = []\n",
    "        for i, form in enumerate(submissions['filings']['recent']['form']):\n",
    "            if form == '10-Q':\n",
    "                accession_number = submissions['filings']['recent']['accessionNumber'][i]\n",
    "                primary_document = submissions['filings']['recent']['primaryDocument'][i]\n",
    "                filing_date = submissions['filings']['recent']['filingDate'][i]\n",
    "                accession_number_clean = accession_number.replace('-', '')\n",
    "                filing_url = (\n",
    "                    f\"https://www.sec.gov/Archives/edgar/data/{cik}/\"\n",
    "                    f\"{accession_number_clean}/{primary_document}\"\n",
    "                )\n",
    "                metadata = {\n",
    "                    'ticker': ticker.upper(),\n",
    "                    'company_name': company_name,\n",
    "                    'filing_date': filing_date,\n",
    "                    'cik': cik,\n",
    "                    'filing_url': filing_url\n",
    "                }\n",
    "                filings_metadata.append(metadata)\n",
    "                if len(filings_metadata) >= num_filings:\n",
    "                    break\n",
    "        if not filings_metadata:\n",
    "            raise ValueError(f\"No recent 10-Q filings found for ticker '{ticker}'.\")\n",
    "        print(f\"  → Found {len(filings_metadata)} recent 10-Q filing metadata entries.\")\n",
    "        return filings_metadata\n",
    "\n",
    "    @staticmethod\n",
    "    def get_filing_html(filing_url: str) -> str:\n",
    "        time.sleep(0.1)\n",
    "        response = requests.get(filing_url, headers=Config.SEC_HEADERS)\n",
    "        response.raise_for_status()\n",
    "        return response.text\n",
    "\n",
    "    @staticmethod\n",
    "    def _normalize_header_text(text: str) -> str:\n",
    "        text = text.strip().upper()\n",
    "        part_match = re.search(r'^\\s*(PART\\s+I{1,2})', text)\n",
    "        if part_match:\n",
    "            return re.sub(r'\\s+', ' ', part_match.group(1))\n",
    "        item_match = re.search(r'^\\s*(ITEM\\s+\\d[A-Z]?)', text)\n",
    "        if item_match:\n",
    "            return re.sub(r'\\s+', ' ', item_match.group(1))\n",
    "        return None\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_html_table(table_tag) -> str:\n",
    "        markdown_rows = []\n",
    "        for tr in table_tag.find_all('tr'):\n",
    "            cells = [\" \".join(cell.get_text(strip=True).split())\n",
    "                    for cell in tr.find_all(['td', 'th'])]\n",
    "            if any(cells):\n",
    "                markdown_rows.append(cells)\n",
    "        if not markdown_rows: return \"\"\n",
    "        md_output = []\n",
    "        header = markdown_rows[0]\n",
    "        md_output.append(\"| \" + \" | \".join(header) + \" |\")\n",
    "        md_output.append(\"| \" + \" | \".join(['---'] * len(header)) + \" |\")\n",
    "        for row in markdown_rows[1:]:\n",
    "            while len(row) < len(header): row.append(\"\")\n",
    "            row = row[:len(header)]\n",
    "            md_output.append(\"| \" + \" | \".join(row) + \" |\")\n",
    "        return \"\\n\" + \"\\n\".join(md_output) + \"\\n\"\n",
    "\n",
    "    @classmethod\n",
    "    def parse_10q(cls, html_content: str) -> Dict:\n",
    "        soup = BeautifulSoup(html_content, 'lxml')\n",
    "        potential_headers = soup.find_all(['p', 'b', 'strong', 'div'])\n",
    "        doc_headers = []\n",
    "        for header in potential_headers:\n",
    "            text = header.get_text(strip=True)\n",
    "            if len(text) > 100: continue\n",
    "            normalized_key = cls._normalize_header_text(text)\n",
    "            if normalized_key and not header.find_parent('a'):\n",
    "                doc_headers.append({'tag': header, 'key': normalized_key})\n",
    "        if not doc_headers: return {}\n",
    "        parsed_data = defaultdict(lambda: defaultdict(str))\n",
    "        current_part_key = None\n",
    "        for i, header_info in enumerate(doc_headers):\n",
    "            current_key = header_info['key']\n",
    "            if 'PART' in current_key:\n",
    "                current_part_key = current_key\n",
    "                continue\n",
    "            if 'ITEM' in current_key:\n",
    "                if not current_part_key: current_part_key = \"PART I\"\n",
    "                start_node = header_info['tag']\n",
    "                end_node = doc_headers[i + 1]['tag'] if i + 1 < len(doc_headers) else None\n",
    "                content_parts = []\n",
    "                element = start_node.next_element\n",
    "                while element and element != end_node:\n",
    "                    if isinstance(element, NavigableString):\n",
    "                        if not element.find_parent('table'):\n",
    "                            text = element.strip()\n",
    "                            if text: content_parts.append(text)\n",
    "                    elif element.name == 'table':\n",
    "                        if not element.find_parent('table'):\n",
    "                            table_markdown = cls._parse_html_table(element)\n",
    "                            if table_markdown: content_parts.append(table_markdown)\n",
    "                    element = element.next_element\n",
    "                full_content = \"\\n\".join(content_parts)\n",
    "                clean_content = re.sub(r'\\n{3,}', '\\n\\n', full_content).strip()\n",
    "                parsed_data[current_part_key][current_key] = clean_content\n",
    "        return {part: dict(items) for part, items in parsed_data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "255f0dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# PART 3: TEXT CHUNKING & EMBEDDING\n",
    "# ===========================================================================\n",
    "# (From base_RAG.ipynb)\n",
    "\n",
    "class DocumentProcessor:\n",
    "    def __init__(self, embedding_model_name: str = Config.EMBEDDING_MODEL):\n",
    "        print(f\"\\n Loading embedding model: {embedding_model_name}\")\n",
    "        self.model = SentenceTransformer(embedding_model_name)\n",
    "        print(f\"   ✓ Model loaded (dimension: {self.model.get_sentence_embedding_dimension()})\")\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=Config.CHUNK_SIZE,\n",
    "            chunk_overlap=Config.CHUNK_OVERLAP,\n",
    "            length_function=len,\n",
    "            add_start_index=False,\n",
    "        )\n",
    "        print(f\"   ✓ Initialized RecursiveCharacterTextSplitter\")\n",
    "\n",
    "    def generate_document_chunks(self, parsed_data: Dict, metadata: Dict,\n",
    "                                 embed_batch_size: int = 1024):\n",
    "        all_docs = []\n",
    "        for part, items in parsed_data.items():\n",
    "            for item, content in items.items():\n",
    "                if not content: continue\n",
    "                doc_metadata = {\n",
    "                    'ticker': metadata['ticker'],\n",
    "                    'company_name': metadata['company_name'],\n",
    "                    'filing_date': metadata['filing_date'],\n",
    "                    'filing_url': metadata['filing_url'],\n",
    "                    'part': part,\n",
    "                    'item': item\n",
    "                }\n",
    "                doc = Document(page_content=content, metadata=doc_metadata)\n",
    "                all_docs.append(doc)\n",
    "        if not all_docs: return\n",
    "        print(f\"     → Splitting {len(all_docs)} high-level 'Items' into smaller chunks...\")\n",
    "        chunked_docs = self.text_splitter.split_documents(all_docs)\n",
    "        print(f\"     → Generated {len(chunked_docs)} chunks\")\n",
    "        text_batch, metadata_batch = [], []\n",
    "        for chunk in chunked_docs:\n",
    "            text_batch.append(chunk.page_content)\n",
    "            metadata_batch.append(chunk.metadata)\n",
    "            if len(text_batch) >= embed_batch_size:\n",
    "                embeddings = self.model.encode(text_batch, show_progress_bar=False)\n",
    "                for txt, emb, meta in zip(text_batch, embeddings, metadata_batch):\n",
    "                    payload = {'text': txt, **meta}\n",
    "                    yield PointStruct(id=str(uuid.uuid4()), vector=emb.tolist(), payload=payload)\n",
    "                text_batch, metadata_batch = [], []\n",
    "        if text_batch:\n",
    "            embeddings = self.model.encode(text_batch, show_progress_bar=False)\n",
    "            for txt, emb, meta in zip(text_batch, embeddings, metadata_batch):\n",
    "                payload = {'text': txt, **meta}\n",
    "                yield PointStruct(id=str(uuid.uuid4()), vector=emb.tolist(), payload=payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a628846c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# PART 4: QDRANT VECTOR DATABASE\n",
    "# ===========================================================================\n",
    "# (From base_RAG.ipynb)\n",
    "\n",
    "class QdrantManager:\n",
    "    def __init__(self):\n",
    "        print(f\"\\nConnecting to Qdrant Cloud...\")\n",
    "        self.client = QdrantClient(url=Config.QDRANT_URL, api_key=Config.QDRANT_API_KEY)\n",
    "        print(f\"   ✓ Connected to Qdrant\")\n",
    "\n",
    "    def create_collection(self, collection_name: str = Config.COLLECTION_NAME,\n",
    "                         vector_size: int = Config.VECTOR_SIZE):\n",
    "        print(f\"\\n Setting up collection: {collection_name}\")\n",
    "        collections = self.client.get_collections().collections\n",
    "        exists = any(col.name == collection_name for col in collections)\n",
    "        if exists:\n",
    "            # If it exists, we assume it's already indexed and skip recreation\n",
    "            print(f\"   ✓ Collection '{collection_name}' already exists. Skipping creation.\")\n",
    "            return\n",
    "        \n",
    "        self.client.create_collection(\n",
    "            collection_name=collection_name,\n",
    "            vectors_config=models.VectorParams(size=vector_size, distance=models.Distance.COSINE)\n",
    "        )\n",
    "        print(f\"   ✓ Collection created\")\n",
    "        print(f\"   → Creating payload index for 'ticker'...\")\n",
    "        self.client.create_payload_index(\n",
    "            collection_name=collection_name,\n",
    "            field_name=\"ticker\",\n",
    "            field_schema=models.PayloadSchemaType.KEYWORD\n",
    "        )\n",
    "        print(f\"   → Creating payload index for 'item'...\")\n",
    "        self.client.create_payload_index(\n",
    "            collection_name=collection_name,\n",
    "            field_name=\"item\",\n",
    "            field_schema=models.PayloadSchemaType.KEYWORD\n",
    "        )\n",
    "        print(f\"   ✓ Payload indexes created.\")\n",
    "\n",
    "    def upsert_documents(self, points_generator,\n",
    "                        collection_name: str = Config.COLLECTION_NAME,\n",
    "                        batch_size: int = 2048) -> int:\n",
    "        print(f\" Uploading chunks to Qdrant in batches of {batch_size}...\")\n",
    "        batch, count = [], 0\n",
    "        for point in points_generator:\n",
    "            batch.append(point)\n",
    "            if len(batch) >= batch_size:\n",
    "                self.client.upsert(collection_name=collection_name, points=batch, wait=False)\n",
    "                count += len(batch)\n",
    "                print(f\"     → Uploaded {count} chunks so far...\")\n",
    "                batch = []\n",
    "        if batch:\n",
    "            self.client.upsert(collection_name=collection_name, points=batch, wait=False)\n",
    "            count += len(batch)\n",
    "        print(f\"  ✓ All chunks uploaded for this document. Total: {count}\")\n",
    "        return count\n",
    "\n",
    "    def search(self, query_vector: List[float],\n",
    "              collection_name: str = Config.COLLECTION_NAME,\n",
    "              limit: int = Config.TOP_K,\n",
    "              filter_dict: Dict = None) -> List[Dict]:\n",
    "        \n",
    "        qdrant_filter = None\n",
    "        if filter_dict:\n",
    "            # MODIFIED: Allow multi-value filters (e.g., list of tickers)\n",
    "            must_conditions = []\n",
    "            for key, value in filter_dict.items():\n",
    "                if isinstance(value, list):\n",
    "                    must_conditions.append(models.FieldCondition(\n",
    "                        key=key, \n",
    "                        match=models.MatchAny(any=value)\n",
    "                    ))\n",
    "                else:\n",
    "                    must_conditions.append(models.FieldCondition(\n",
    "                        key=key, \n",
    "                        match=models.MatchValue(value=value)\n",
    "                    ))\n",
    "            qdrant_filter = models.Filter(must=must_conditions)\n",
    "\n",
    "        results = self.client.search(\n",
    "            collection_name=collection_name,\n",
    "            query_vector=query_vector,\n",
    "            limit=limit,\n",
    "            query_filter=qdrant_filter,\n",
    "            with_payload=True\n",
    "        )\n",
    "        return [{'score': result.score, 'payload': result.payload} for result in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ae41a9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded spaCy model for NER\n",
      "✓ Loaded SentenceTransformer model 'sentence-transformers/all-MiniLM-L6-v2'\n",
      "✓ Loaded Flan-T5 model 'google/flan-t5-small' on cpu\n"
     ]
    }
   ],
   "source": [
    "# ===========================================================================\n",
    "# PART 5: QUERY PROCESSOR MODULE\n",
    "# ===========================================================================\n",
    "# (Full QueryProcessor logic from query_processors.ipynb)\n",
    "\n",
    "USE_SBERT = True\n",
    "USE_FLAN_T5 = True\n",
    "\n",
    "def normalize(text: str) -> str:\n",
    "    text = unicodedata.normalize(\"NFKC\", text).strip()\n",
    "    return re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "TOKEN_RE = re.compile(r\"[A-Za-z0-9]+(?:'[A-Za-z0-9]+)?|[&$%.\\-]+\")\n",
    "def simple_tokenize(text: str) -> List[str]:\n",
    "    toks = TOKEN_RE.findall(text)\n",
    "    cleaned = []\n",
    "    for t in toks:\n",
    "        if t.lower() == \"'s\": continue\n",
    "        if t.endswith(\"'s\"): t = t[:-2]\n",
    "        cleaned.append(t)\n",
    "    return cleaned\n",
    "\n",
    "DOMAIN_SYNONYMS = {\n",
    "    \"risk\": [\"risk factor\",\"risk factors\",\"uncertainty\",\"exposure\",\"threat\"],\n",
    "    \"cyber\": [\"cybersecurity\",\"information security\",\"infosec\",\"data breach\",\"security incident\"],\n",
    "    \"performance\": [\"revenue\",\"growth\",\"margin\",\"profit\",\"loss\",\"guidance\",\"results\"],\n",
    "    \"strategy\": [\"roadmap\",\"plan\",\"initiative\",\"expansion\",\"capex\",\"restructuring\",\"acquisition\"],\n",
    "    \"md&a\": [\"management discussion\",\"md&a\",\"results of operations\"],\n",
    "}\n",
    "\n",
    "COMPANY_TICKERS = {\"tesla\":\"TSLA\", \"apple\":\"AAPL\", \"microsoft\":\"MSFT\", \"nvidia\":\"NVDA\", \"google\":\"GOOGL\", \"alphabet\":\"GOOGL\", \"meta\":\"META\", \"amazon\":\"AMZN\", \"amd\":\"AMD\", \"oracle\":\"ORCL\", \"jpmorgan\":\"JPM\", \"jpm\":\"JPM\"}\n",
    "\n",
    "def keyword_expand(tokens: List[str]) -> List[str]:\n",
    "    ex = []\n",
    "    for t in tokens:\n",
    "        t0 = t.strip(\".-\").lower()\n",
    "        ex.extend(DOMAIN_SYNONYMS.get(t0, []))\n",
    "    seen, out = set(), []\n",
    "    for w in ex:\n",
    "        if w not in seen: seen.add(w); out.append(w)\n",
    "    return out\n",
    "\n",
    "def build_keywords(tokens: List[str], expansions: List[str]) -> List[str]:\n",
    "    kept = []\n",
    "    for t in tokens + expansions:\n",
    "        t = t.lower()\n",
    "        if re.search(r\"[a-z0-9]\", t) and t not in kept:\n",
    "            kept.append(t)\n",
    "    return kept\n",
    "\n",
    "try:\n",
    "    _nlp = spacy.load(Config.SPACY_MODEL)\n",
    "    print(\"✓ Loaded spaCy model for NER\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Failed to load spaCy model '{Config.SPACY_MODEL}'. NER will be limited.\")\n",
    "    print(\"  Run: python -m spacy download en_core_web_lg\")\n",
    "    _nlp = None\n",
    "\n",
    "def extract_entities(raw_text: str) -> dict:\n",
    "    out = {}; low = raw_text.lower()\n",
    "    q = re.findall(r\"\\b(q[1-4])\\s*([12][0-9]{3})\\b\", low)\n",
    "    if q: out[\"quarter\"] = [f\"{p.upper()} {y}\" for p, y in q]\n",
    "    years = re.findall(r\"\\b(20[0-4][0-9]|19[0-9]{2})\\b\", raw_text)\n",
    "    if years: out[\"year\"] = sorted(set(years))\n",
    "    companies = set()\n",
    "    if _nlp is not None:\n",
    "        doc = _nlp(raw_text)\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == \"ORG\": companies.add(ent.text.strip())\n",
    "    low_raw = raw_text.lower()\n",
    "    for name in COMPANY_TICKERS:\n",
    "        if name in low_raw: companies.add(name.title())\n",
    "    if companies: out[\"company\"] = sorted(companies)\n",
    "    tickers = set(COMPANY_TICKERS.get(c.lower(),\"\") for c in companies if COMPANY_TICKERS.get(c.lower()))\n",
    "    tickers.update(re.findall(r\"\\$([A-Z]{1,5})\\b\", raw_text))\n",
    "    tickers.update(re.findall(r\"\\(([A-Z]{1,5})\\)\", raw_text))\n",
    "    tickers.update(re.findall(r\"\\b(?:NASDAQ|NYSE)\\s*:\\s*([A-Z]{1,5})\\b\", raw_text))\n",
    "    tickers = {t for t in tickers if t}\n",
    "    if tickers: out[\"ticker\"] = sorted(tickers)\n",
    "    return out\n",
    "\n",
    "try:\n",
    "    _sbert = SentenceTransformer(Config.EMBEDDING_MODEL) if USE_SBERT else None\n",
    "    print(f\"✓ Loaded SentenceTransformer model '{Config.EMBEDDING_MODEL}'\")\n",
    "except Exception: _sbert = None; print(f\"✗ Failed to load SentenceTransformer\")\n",
    "\n",
    "def sbert_embed(text: str) -> Optional[List[float]]:\n",
    "    if _sbert is None: return None\n",
    "    v = _sbert.encode([text], normalize_embeddings=True)[0]\n",
    "    return v.tolist()\n",
    "\n",
    "try:\n",
    "    _flan_device = \"cuda\" if USE_FLAN_T5 and torch.cuda.is_available() else \"cpu\"\n",
    "    _flan_tok = AutoTokenizer.from_pretrained(Config.FLAN_T5_MODEL_NAME) if USE_FLAN_T5 else None\n",
    "    _flan_mdl = AutoModelForSeq2SeqLM.from_pretrained(Config.FLAN_T5_MODEL_NAME).to(_flan_device).eval() if USE_FLAN_T5 else None\n",
    "    print(f\"✓ Loaded Flan-T5 model '{Config.FLAN_T5_MODEL_NAME}' on {_flan_device}\")\n",
    "except Exception as e:\n",
    "    _flan_tok = _flan_mdl = None; _flan_device = \"cpu\"; print(f\"✗ Failed to load Flan-T5 model: {e}\")\n",
    "\n",
    "def t5_paraphrases_safe(q: str, num_return: int = 3, max_new_tokens: int = 48) -> List[str]:\n",
    "    if not (USE_FLAN_T5 and _flan_tok is not None and _flan_mdl is not None): return []\n",
    "    prompt = f\"Rewrite the query into multiple short paraphrases without adding facts or numbers. Keep meaning.\\nQuery: {q}\"\n",
    "    x = _flan_tok(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    x = {k: v.to(_flan_device) for k, v in x.items()}\n",
    "    with torch.no_grad():\n",
    "        out = _flan_mdl.generate(**x, do_sample=True, top_k=50, top_p=0.92, temperature=0.9, num_return_sequences=num_return, max_new_tokens=max_new_tokens, repetition_penalty=1.1, no_repeat_ngram_size=3)\n",
    "    paras = _flan_tok.batch_decode(out, skip_special_tokens=True)\n",
    "    base = re.sub(r\"\\W+\",\" \", q).strip().lower()\n",
    "    seen, kept = set(), []\n",
    "    for p in paras:\n",
    "        p2 = normalize(p); p2_cmp = re.sub(r\"\\W+\",\" \", p2).strip().lower()\n",
    "        if p2_cmp == base: continue\n",
    "        if p2 and p2 not in seen: seen.add(p2); kept.append(p2)\n",
    "    return kept[:num_return]\n",
    "\n",
    "INTENT_LABELS = [\"risk\",\"performance\",\"strategy\"]\n",
    "X_train = [\"What new risk factors?\", \"Cybersecurity breach Tesla\", \"Explain Apple revenue growth\", \"Compare Microsoft profit guidance\", \"Outline Nvidia expansion strategy\", \"What restructuring plan?\"]\n",
    "y_train = [\"risk\",\"risk\",\"performance\",\"performance\",\"strategy\",\"strategy\"]\n",
    "_intent_clf = Pipeline([(\"tfidf\", TfidfVectorizer(ngram_range=(1,2), min_df=1)), (\"lr\", LogisticRegression(max_iter=300, class_weight=\"balanced\", multi_class=\"ovr\"))]).fit(X_train, y_train)\n",
    "RISK_KW = {\"risk\",\"risk factor\",\"risk factors\",\"uncertainty\",\"cyber\",\"cybersecurity\",\"breach\",\"litigation\",\"security\"}\n",
    "PERF_KW = {\"revenue\",\"growth\",\"margin\",\"profit\",\"loss\",\"guidance\",\"results\",\"compare\",\"last quarter\",\"quarterly\"}\n",
    "STRAT_KW= {\"strategy\",\"plan\",\"roadmap\",\"expansion\",\"acquisition\",\"restructuring\",\"capex\",\"data center\",\"data centers\"}\n",
    "def _kw_score(t: str, kws: set[str]) -> int: return sum(1 for k in kws if k in t)\n",
    "def classify_intent(text: str) -> Tuple[str, float]:\n",
    "    tx = normalize(text); proba = _intent_clf.predict_proba([tx])[0].tolist()\n",
    "    k_r = _kw_score(tx, RISK_KW); k_p = _kw_score(tx, PERF_KW); k_s = _kw_score(tx, STRAT_KW)\n",
    "    k_sum = max(1, (k_r + k_p + k_s)); priors = [k_r/k_sum, k_p/k_sum, k_s/k_sum]\n",
    "    alpha, beta = 0.6, 0.4; blended = [alpha*proba[i] + beta*priors[i] for i in range(3)]\n",
    "    s = sum(blended) or 1.0; blended = [b/s for b in blended]\n",
    "    idx = max(range(3), key=lambda i: blended[i])\n",
    "    return INTENT_LABELS[idx], float(blended[idx])\n",
    "\n",
    "def expand_query(query: str) -> dict:\n",
    "    norm = normalize(query); toks = simple_tokenize(norm); lex_ex = keyword_expand(toks)\n",
    "    paras = t5_paraphrases_safe(norm, num_return=3, max_new_tokens=48) if USE_FLAN_T5 else []\n",
    "    para_tokens = list(dict.fromkeys(t for p in paras for t in simple_tokenize(p)))\n",
    "    para_ex = keyword_expand(para_tokens) if para_tokens else []\n",
    "    expansions = list(dict.fromkeys(w for lst in (lex_ex, para_ex) for w in lst))\n",
    "    return {\"normalized\": norm, \"tokens\": toks, \"expansions\": expansions, \"paraphrases\": paras, \"keywords\": build_keywords(toks, expansions)}\n",
    "\n",
    "@dataclass\n",
    "class QueryProcessorConfig:\n",
    "    labels: List[str] = field(default_factory=lambda: [\"risk\",\"performance\",\"strategy\"])\n",
    "\n",
    "class QueryProcessor:\n",
    "    def __init__(self, config: QueryProcessorConfig = QueryProcessorConfig()):\n",
    "        self.config = config; print(\"✓ QueryProcessor initialized\")\n",
    "    def process(self, query: str) -> Dict[str, Any]:\n",
    "        raw = query; ex = expand_query(query); ents = extract_entities(raw)\n",
    "        label, conf = classify_intent(ex[\"normalized\"])\n",
    "        emb = sbert_embed(ex[\"normalized\"]) if USE_SBERT else None\n",
    "        return {\"normalized\": ex[\"normalized\"], \"label\": label, \"confidence\": conf, \"expansions\": ex[\"expansions\"], \"paraphrases\": ex[\"paraphrases\"], \"keywords\": ex[\"keywords\"], \"entities\": ents, \"filters\": ents.copy(), \"embedding\": emb}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2f4e1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# PART 6: RAG QUERY ENGINE (MODIFIED)\n",
    "# ===========================================================================\n",
    "# (From base_RAG.ipynb, modified to use QueryProcessor)\n",
    "\n",
    "class ManualRAGEngine:\n",
    "    def __init__(self, document_processor: DocumentProcessor, \n",
    "                 qdrant_manager: QdrantManager, \n",
    "                 query_processor: QueryProcessor):\n",
    "        print(\"\\n Initializing Manual RAG Engine (with Query Processor)...\")\n",
    "        \n",
    "        # 1. Get components\n",
    "        self.embedding_model = document_processor.model\n",
    "        print(\"   ✓ Using existing embedding model from DocumentProcessor\")\n",
    "        self.qdrant_manager = qdrant_manager\n",
    "        print(\"   ✓ Using existing QdrantManager for search\")\n",
    "        self.query_processor = query_processor\n",
    "        print(\"   ✓ QueryProcessor attached\")\n",
    "\n",
    "        # 2. Initialize LLM\n",
    "        self.llm = ChatOpenAI(model=Config.LLM_MODEL, api_key=Config.OPENAI_API_KEY, temperature=0)\n",
    "        print(\"   ✓ Initialized ChatOpenAI LLM\")\n",
    "\n",
    "        # 3. Create prompt template\n",
    "        template = \"\"\"You are a helpful financial analyst assistant. Your role is to answer questions about SEC 10-Q filings based ONLY on the provided context.\n",
    "- Base your answer strictly on the provided context from SEC filings\n",
    "- Cite specific sections (e.g., \"According to Item 1A...\") when referencing information\n",
    "- If the answer is not in the context, clearly state that\n",
    "\n",
    "Context:\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        self.prompt = ChatPromptTemplate.from_template(template)\n",
    "        print(\"   ✓ Manual RAG Engine ready.\")\n",
    "\n",
    "    def _format_context(self, search_results: List[Dict]) -> str:\n",
    "        context_str = \"\"\n",
    "        for i, result in enumerate(search_results, 1):\n",
    "            payload = result.get('payload', {})\n",
    "            text = payload.get('text', 'No text found')\n",
    "            item = payload.get('item', 'N/A')\n",
    "            ticker = payload.get('ticker', 'N/A')\n",
    "            context_str += f\"Source {i} ({ticker} - {item}):\\n\\\"{text}\\\"\\n\\n\"\n",
    "        return context_str.strip()\n",
    "\n",
    "    def query(self, question: str, ticker_filter: str = None):\n",
    "        print(f\"\\n Processing query with Manual Engine: '{question}'\")\n",
    "        \n",
    "        # 1. --- NEW: Use QueryProcessor ---\n",
    "        print(\"   → Processing query (NER, Intent, Embedding)...\")\n",
    "        q_obj = self.query_processor.process(question)\n",
    "        \n",
    "        query_vector = q_obj[\"embedding\"]\n",
    "        filter_dict = {}\n",
    "\n",
    "        # Use filters generated by the processor\n",
    "        if \"ticker\" in q_obj[\"filters\"]:\n",
    "            filter_dict[\"ticker\"] = q_obj[\"filters\"][\"ticker\"]\n",
    "            print(f\"   → Auto-detected filter: {filter_dict}\")\n",
    "        \n",
    "        # --- Allow manual override ---\n",
    "        if ticker_filter:\n",
    "            print(f\"   → MANUAL OVERRIDE: Applying filter: {ticker_filter}\")\n",
    "            filter_dict = {\"ticker\": ticker_filter.upper()}\n",
    "        \n",
    "        if not filter_dict:\n",
    "            print(\"   → No filters applied. Searching all documents.\")\n",
    "\n",
    "        # 2. Manually search Qdrant\n",
    "        print(\"   → Manually searching Qdrant...\")\n",
    "        search_results = self.qdrant_manager.search(\n",
    "            query_vector=query_vector,\n",
    "            limit=Config.TOP_K,\n",
    "            filter_dict=filter_dict if filter_dict else None\n",
    "        )\n",
    "\n",
    "        if not search_results:\n",
    "            return {'answer': 'No relevant context was found in the documents to answer this question.', 'sources': []}\n",
    "\n",
    "        # 3. Format prompt\n",
    "        print(\"   → Formatting context and building prompt...\")\n",
    "        formatted_context = self._format_context(search_results)\n",
    "        final_prompt_message = self.prompt.format_messages(\n",
    "            context=formatted_context,\n",
    "            input=question\n",
    "        )\n",
    "\n",
    "        # 4. Invoke LLM\n",
    "        print(\"   → Sending prompt to LLM...\")\n",
    "        llm_response = self.llm.invoke(final_prompt_message)\n",
    "        answer = llm_response.content\n",
    "\n",
    "        # 5. Format sources\n",
    "        sources = []\n",
    "        for i, result in enumerate(search_results, 1):\n",
    "            sources.append({\n",
    "                'ticker': result['payload'].get('ticker'),\n",
    "                'company': result['payload'].get('company_name'),\n",
    "                'item': result['payload'].get('item'),\n",
    "                'part': result['payload'].get('part'),\n",
    "                'filing_date': result['payload'].get('filing_date'),\n",
    "                'score': result['score']\n",
    "            })\n",
    "\n",
    "        return {'answer': answer, 'sources': sources}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a78a3c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# PART 6: RAG QUERY ENGINE (MODEL 4)\n",
    "# ===========================================================================\n",
    "# This engine is modified to *only* use Pre-Retrieval.\n",
    "# It uses the QueryProcessor for its embedding, but *not* for filtering.\n",
    "\n",
    "class PreRetrievalRAGEngine:\n",
    "    def __init__(self, document_processor: DocumentProcessor, \n",
    "                 qdrant_manager: QdrantManager, \n",
    "                 query_processor: QueryProcessor):\n",
    "        print(\"\\n Initializing Pre-Retrieval RAG Engine (Model 4)...\")\n",
    "        self.embedding_model = document_processor.model\n",
    "        print(\"   ✓ Using existing embedding model (for fallback)\")\n",
    "        self.qdrant_manager = qdrant_manager\n",
    "        print(\"   ✓ Using existing QdrantManager for search\")\n",
    "        self.query_processor = query_processor\n",
    "        print(\"   ✓ QueryProcessor attached (for embedding generation)\")\n",
    "\n",
    "        self.llm = ChatOpenAI(model=Config.LLM_MODEL, api_key=Config.OPENAI_API_KEY, temperature=0)\n",
    "        print(\"   ✓ Initialized ChatOpenAI LLM\")\n",
    "        template = \"\"\"You are a helpful financial analyst assistant...\n",
    "Context:\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "Question: {input}\n",
    "Answer:\"\"\"\n",
    "        self.prompt = ChatPromptTemplate.from_template(template.replace(\"...\",\"\").strip())\n",
    "        print(\"   ✓ Pre-Retrieval RAG Engine ready.\")\n",
    "\n",
    "    def _format_context(self, search_results: List[Dict]) -> str:\n",
    "        context_str = \"\"\n",
    "        for i, result in enumerate(search_results, 1):\n",
    "            payload = result.get('payload', {})\n",
    "            context_str += f\"Source {i} ({payload.get('ticker','N/A')} - {payload.get('item','N/A')}):\\n\\\"{payload.get('text','No text')}\\\"\\n\\n\"\n",
    "        return context_str.strip()\n",
    "\n",
    "    def query(self, question: str, ticker_filter: str = None):\n",
    "        print(f\"\\n Processing query with Pre-Retrieval Engine: '{question}'\")\n",
    "        \n",
    "        # 1. --- Use QueryProcessor for PRE-RETRIEVAL ---\n",
    "        print(\"   → Processing query (Normalization, Expansion, Embedding)...\")\n",
    "        q_obj = self.query_processor.process(question)\n",
    "        \n",
    "        # This is the \"Pre-Retrieval\" step: using the processed embedding\n",
    "        query_vector = q_obj[\"embedding\"]\n",
    "        if query_vector is None:\n",
    "            print(\"   → Warning: QP embedding failed, using base model embedding.\")\n",
    "            query_vector = self.embedding_model.encode(question).tolist()\n",
    "\n",
    "        # 2. --- FILTERING IS DISABLED FOR THIS MODEL ---\n",
    "        # We ignore q_obj[\"filters\"] to isolate the Pre-Retrieval effect.\n",
    "        filter_dict = None\n",
    "        if ticker_filter:\n",
    "            print(f\"   → MANUAL OVERRIDE: Applying filter: {ticker_filter}\")\n",
    "            filter_dict = {\"ticker\": ticker_filter.upper()}\n",
    "        else:\n",
    "            print(\"   → No filters applied (NER filtering is OFF for this model).\")\n",
    "\n",
    "        # 3. Manually search Qdrant\n",
    "        print(\"   → Manually searching Qdrant...\")\n",
    "        search_results = self.qdrant_manager.search(\n",
    "            query_vector=query_vector,\n",
    "            limit=Config.TOP_K,\n",
    "            filter_dict=filter_dict\n",
    "        )\n",
    "        if not search_results:\n",
    "            return {'answer': 'No relevant context found.', 'sources': []}\n",
    "\n",
    "        # 4. Format prompt\n",
    "        print(\"   → Formatting context and building prompt...\")\n",
    "        formatted_context = self._format_context(search_results)\n",
    "        final_prompt_message = self.prompt.format_messages(context=formatted_context, input=question)\n",
    "\n",
    "        # 5. Invoke LLM\n",
    "        print(\"   → Sending prompt to LLM...\")\n",
    "        llm_response = self.llm.invoke(final_prompt_message)\n",
    "        answer = llm_response.content\n",
    "\n",
    "        # 6. Format sources\n",
    "        sources = [{'ticker': r['payload'].get('ticker'), 'company': r['payload'].get('company_name'), 'item': r['payload'].get('item'), 'part': r['payload'].get('part'), 'filing_date': r['payload'].get('filing_date'), 'score': r['score']} for r in search_results]\n",
    "        return {'answer': answer, 'sources': sources}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "058ffd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# PART 7: MAIN PIPELINE ORCHESTRATOR\n",
    "# ===========================================================================\n",
    "class SECFilingRAGPipeline:\n",
    "    def __init__(self):\n",
    "        print(\"=\" * 70); print(\"SEC 10-Q RAG SYSTEM (Model 4: Pre-Retrieval)\"); print(\"=\" * 70)\n",
    "        self.loader = SECDocumentLoader()\n",
    "        self.processor = DocumentProcessor()\n",
    "        self.qdrant_manager = QdrantManager()\n",
    "        self.query_processor = QueryProcessor()\n",
    "        self.query_engine = None\n",
    "\n",
    "    def load_and_index_filings(self, tickers: List[str] = Config.TICKERS, num_filings_per_ticker: int = 4):\n",
    "        print(f\"\\n{'=' * 70}\\nLOADING & INDEXING PHASE\\n{'=' * 70}\")\n",
    "        print(f\"\\nProcessing {len(tickers)} companies: {', '.join(tickers)}\")\n",
    "        print(f\"(Fetching {num_filings_per_ticker} filings per company)\\n\")\n",
    "        \n",
    "        self.qdrant_manager.create_collection() # Will skip if collection exists\n",
    "        \n",
    "        try: # Check if collection is empty\n",
    "            count = self.qdrant_manager.client.count(Config.COLLECTION_NAME, exact=True)\n",
    "            if count.count > 0:\n",
    "                print(f\"Collection '{Config.COLLECTION_NAME}' already has {count.count} docs. Skipping indexing.\")\n",
    "                print(f\"{'=' * 70}\\n\"); return\n",
    "        except Exception: pass # Collection might not exist, proceed.\n",
    "\n",
    "        successful_tickers, failed_tickers = [], []; total_chunks_indexed = 0\n",
    "        for idx, ticker in enumerate( tickers, 1):\n",
    "            print(f\"\\n[{idx}/{len(tickers)}] Processing {ticker}\"); print(\"-\" * 70)\n",
    "            ticker_chunks_count, num_filings_processed = 0, 0\n",
    "            try:\n",
    "                filings_metadata_list = self.loader.get_recent_10q_metadata(ticker, num_filings=num_filings_per_ticker)\n",
    "                for filing_metadata in filings_metadata_list:\n",
    "                    try:\n",
    "                        print(f\"  → Downloading filing from: {filing_metadata['filing_date']}...\"); html_content = self.loader.get_filing_html(filing_metadata['filing_url'])\n",
    "                        print(f\"  → Parsing 10-Q structure...\"); parsed_data = self.loader.parse_10q(html_content)\n",
    "                        if not parsed_data: print(f\"  ⚠ Warning: No data parsed for {ticker}\"); del html_content; gc.collect(); continue\n",
    "                        print(f\"  → Creating chunks and embeddings generator...\")\n",
    "                        chunks_generator = self.processor.generate_document_chunks(parsed_data, filing_metadata)\n",
    "                        num_uploaded = self.qdrant_manager.upsert_documents(chunks_generator)\n",
    "                        if num_uploaded > 0:\n",
    "                            ticker_chunks_count += num_uploaded; total_chunks_indexed += num_uploaded; num_filings_processed += 1\n",
    "                        print(f\"  → Cleaning up memory...\"); del html_content; del parsed_data; del chunks_generator; gc.collect(); print(f\"  ✓ Memory cleaned.\")\n",
    "                    except Exception as e: print(f\"  ✗ Error processing filing for {ticker}: {e}\"); gc.collect()\n",
    "                if ticker_chunks_count > 0: successful_tickers.append(ticker); print(f\"  ✓ Finished {ticker}. Chunks: {ticker_chunks_count}\")\n",
    "                else: failed_tickers.append(ticker); print(f\"  ⚠ No chunks created for {ticker}\")\n",
    "            except Exception as e: print(f\"  ✗ Error processing {ticker}: {e}\"); failed_tickers.append(ticker)\n",
    "            if idx < len(tickers): time.sleep(0.2)\n",
    "        print(f\"\\n{'=' * 70}\\nINDEXING COMPLETE\\n{'=' * 70}\")\n",
    "        print(f\"✓ Success: {len(successful_tickers)} companies ({', '.join(successful_tickers)})\")\n",
    "        if failed_tickers: print(f\"✗ Failed: {len(failed_tickers)} companies ({', '.join(failed_tickers)})\")\n",
    "        print(f\"\\n Total chunks indexed: {total_chunks_indexed}\\n{'=' * 70}\\n\")\n",
    "\n",
    "    def query(self, question: str, ticker_filter: str = None):\n",
    "        if self.query_engine is None:\n",
    "            self.query_engine = PreRetrievalRAGEngine(\n",
    "                document_processor=self.processor,\n",
    "                qdrant_manager=self.qdrant_manager,\n",
    "                query_processor=self.query_processor\n",
    "            )\n",
    "        result = self.query_engine.query(question, ticker_filter)\n",
    "        print(f\"\\n{'=' * 70}\\nANSWER\\n{'=' * 70}\\n\\n{result['answer']}\\n\")\n",
    "        print(f\"{'=' * 70}\\nSOURCES ({len(result['sources'])})\\n{'=' * 70}\")\n",
    "        for i, source in enumerate(result['sources'], 1):\n",
    "            print(f\"\\n{i}. {source['company']} ({source['ticker']}) - {source['item']}\")\n",
    "            print(f\"   Filing Date: {source['filing_date']}\")\n",
    "            print(f\"   Relevance Score: {source['score']:.4f}\")\n",
    "        print(f\"\\n{'=' * 70}\\n\")\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "29447490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SEC 10-Q RAG SYSTEM (Model 4: Pre-Retrieval)\n",
      "======================================================================\n",
      "\n",
      " Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✓ Model loaded (dimension: 384)\n",
      "   ✓ Initialized RecursiveCharacterTextSplitter\n",
      "\n",
      "Connecting to Qdrant Cloud...\n",
      "   ✓ Connected to Qdrant\n",
      "✓ QueryProcessor initialized\n",
      "Checking if indexing is needed...\n",
      "\n",
      "======================================================================\n",
      "LOADING & INDEXING PHASE\n",
      "======================================================================\n",
      "\n",
      "Processing 10 companies: NVDA, AAPL, MSFT, AMZN, META, GOOGL, TSLA, ORCL, JPM, AMD\n",
      "(Fetching 4 filings per company)\n",
      "\n",
      "\n",
      " Setting up collection: sec_filings_10q\n",
      "   ✓ Collection 'sec_filings_10q' already exists. Skipping creation.\n",
      "Collection 'sec_filings_10q' already has 9109 docs. Skipping indexing.\n",
      "======================================================================\n",
      "\n",
      "Indexing check complete. Proceeding to queries.\n"
     ]
    }
   ],
   "source": [
    "# ===========================================================================\n",
    "# PART 8: USAGE EXAMPLE (Saving all results to one file)\n",
    "# ===========================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # =======================================================================\n",
    "    # 1. INITIALIZE PIPELINE\n",
    "    # =======================================================================\n",
    "    pipeline = SECFilingRAGPipeline()\n",
    "\n",
    "    # =======================================================================\n",
    "    # 2. LOAD AND INDEX FILINGS\n",
    "    # =======================================================================\n",
    "    # This will check if the 'sec_filings_10q' collection has data.\n",
    "    # If you already ran base_RAG.ipynb, it will skip re-indexing.\n",
    "    \n",
    "    print(\"Checking if indexing is needed...\")\n",
    "    pipeline.load_and_index_filings(num_filings_per_ticker=4)\n",
    "    print(\"Indexing check complete. Proceeding to queries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "971148c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running 6 queries with (Model 4) Pre-Retrieval RAG...\n",
      "\n",
      "--- Running Query: What are the main risk factors mentioned by each companies? ---\n",
      "\n",
      " Processing query with Pre-Retrieval Engine: 'What are the main risk factors mentioned by each companies?'\n",
      "   → Processing query (Normalization, Expansion, Embedding)...\n",
      "   → No filters applied (NER filtering is OFF for this model).\n",
      "   → Manually searching Qdrant...\n",
      "   → Formatting context and building prompt...\n",
      "   → Sending prompt to LLM...\n",
      "\n",
      "======================================================================\n",
      "ANSWER\n",
      "======================================================================\n",
      "\n",
      "Based on the provided context, the main risk factors mentioned by each company are as follows:\n",
      "\n",
      "**AMD (Advanced Micro Devices):**\n",
      "1. The markets in which AMD's products are sold are highly competitive and rapidly evolving.\n",
      "2. The semiconductor industry is highly cyclical and has experienced severe downturns.\n",
      "3. Intel Corporation's dominance of the microprocessor market and its aggressive business practices may limit AMD's ability to compete effectively on a level playing field.\n",
      "\n",
      "**AAPL (Apple Inc.):**\n",
      "1. The company's business, reputation, results of operations, financial condition, and stock price can be affected by a number of factors, whether currently known or unknown. Specific risk factors are detailed in their 2024 Form 10-K under the heading \"Risk Factors.\" However, no specific risk factors are detailed in the provided context.\n",
      "\n",
      "======================================================================\n",
      "SOURCES (5)\n",
      "======================================================================\n",
      "\n",
      "1. ADVANCED MICRO DEVICES INC (AMD) - ITEM 1A\n",
      "   Filing Date: 2025-08-06\n",
      "   Relevance Score: 0.6942\n",
      "\n",
      "2. ADVANCED MICRO DEVICES INC (AMD) - ITEM 1A\n",
      "   Filing Date: 2025-11-05\n",
      "   Relevance Score: 0.6942\n",
      "\n",
      "3. Apple Inc. (AAPL) - ITEM 1A\n",
      "   Filing Date: 2025-01-31\n",
      "   Relevance Score: 0.6650\n",
      "\n",
      "4. ADVANCED MICRO DEVICES INC (AMD) - ITEM 1A\n",
      "   Filing Date: 2025-05-07\n",
      "   Relevance Score: 0.6457\n",
      "\n",
      "5. ADVANCED MICRO DEVICES INC (AMD) - ITEM 1A\n",
      "   Filing Date: 2024-10-30\n",
      "   Relevance Score: 0.6457\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "--- Running Query: What risks did Apple disclose in their latest 10-Q? ---\n",
      "\n",
      " Processing query with Pre-Retrieval Engine: 'What risks did Apple disclose in their latest 10-Q?'\n",
      "   → Processing query (Normalization, Expansion, Embedding)...\n",
      "   → No filters applied (NER filtering is OFF for this model).\n",
      "   → Manually searching Qdrant...\n",
      "   → Formatting context and building prompt...\n",
      "   → Sending prompt to LLM...\n",
      "\n",
      "======================================================================\n",
      "ANSWER\n",
      "======================================================================\n",
      "\n",
      "In their latest 10-Q, Apple disclosed several risks, including:\n",
      "\n",
      "1. **Macroeconomic Conditions**: Apple noted that macroeconomic conditions such as inflation, interest rates, and currency fluctuations have impacted and could continue to materially impact the company's results of operations and financial condition.\n",
      "\n",
      "2. **Supply Chain and Manufacturing Risks**: Events that disrupt Apple's ability to manufacture and deliver products, create delays and inefficiencies in the supply and manufacturing chain, result in slowdowns and outages to service offerings, increase costs, and negatively impact consumer spending and demand in affected areas.\n",
      "\n",
      "3. **Gross Margin Volatility**: The company acknowledged that future gross margins could be subject to volatility and downward pressure due to various factors.\n",
      "\n",
      "These risks are part of the broader set of risk factors that Apple regularly updates and discusses in their filings, as referenced in their 2024 Form 10-K and subsequent 10-Q filings.\n",
      "\n",
      "======================================================================\n",
      "SOURCES (5)\n",
      "======================================================================\n",
      "\n",
      "1. Apple Inc. (AAPL) - ITEM 1A\n",
      "   Filing Date: 2024-08-02\n",
      "   Relevance Score: 0.7313\n",
      "\n",
      "2. Apple Inc. (AAPL) - ITEM 2\n",
      "   Filing Date: 2025-05-02\n",
      "   Relevance Score: 0.6400\n",
      "\n",
      "3. Apple Inc. (AAPL) - ITEM 2\n",
      "   Filing Date: 2025-05-02\n",
      "   Relevance Score: 0.6190\n",
      "\n",
      "4. Apple Inc. (AAPL) - ITEM 1A\n",
      "   Filing Date: 2025-05-02\n",
      "   Relevance Score: 0.6020\n",
      "\n",
      "5. Apple Inc. (AAPL) - ITEM 1\n",
      "   Filing Date: 2024-08-02\n",
      "   Relevance Score: 0.5975\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "--- Running Query: Compare the revenue trends of NVIDIA and AMD ---\n",
      "\n",
      " Processing query with Pre-Retrieval Engine: 'Compare the revenue trends of NVIDIA and AMD'\n",
      "   → Processing query (Normalization, Expansion, Embedding)...\n",
      "   → No filters applied (NER filtering is OFF for this model).\n",
      "   → Manually searching Qdrant...\n",
      "   → Formatting context and building prompt...\n",
      "   → Sending prompt to LLM...\n",
      "\n",
      "======================================================================\n",
      "ANSWER\n",
      "======================================================================\n",
      "\n",
      "Based on the provided context, we can observe the following revenue trends for NVIDIA and AMD:\n",
      "\n",
      "**NVIDIA:**\n",
      "1. **Revenue Concentration:** NVIDIA's revenue is significantly concentrated, with two direct customers accounting for 12% and 11% of total revenue for the third quarter and the first nine months of fiscal year 2024, respectively. This indicates a reliance on a few key customers, particularly in the Compute & Networking segment.\n",
      "2. **Cost of Revenue:** NVIDIA's cost of revenue for the third quarter of fiscal years 2025 and 2024 was $543 million and $473 million, respectively. For the first nine months, it was $1.3 billion and $734 million, respectively. This suggests an increase in cost of revenue year-over-year, which could imply higher sales or increased costs associated with production or services.\n",
      "\n",
      "**AMD:**\n",
      "1. **Revenue Growth:** AMD experienced a 36% increase in net revenue for both the three months ended March 29, 2025, and September 27, 2025, compared to the prior year periods. This growth was driven by increased demand in the Data Center segment, particularly for AMD EPYC™ CPUs and AMD Instinct™ GPUs, as well as strong demand in the Client and Gaming segment for AMD Ryzen™ processors and Radeon™ gaming GPUs.\n",
      "2. **Segment Performance:** The Client and Gaming segment showed strong performance due to demand for new processors and gaming products, while the Embedded segment saw a decrease due to mixed demand in end markets.\n",
      "3. **Gross Margin:** AMD's gross margin increased from 50% to 52% for the three months ended September 27, 2025, indicating improved profitability, likely due to a favorable product mix.\n",
      "\n",
      "**Comparison:**\n",
      "- **Growth Rate:** AMD shows a robust revenue growth rate of 36% year-over-year for the periods mentioned, driven by strong demand across multiple segments. In contrast, NVIDIA's revenue details are more focused on customer concentration and cost of revenue, without specific growth rates provided.\n",
      "- **Profitability:** AMD's increase in gross margin suggests improved profitability, whereas NVIDIA's data does not provide specific gross margin figures, focusing instead on cost of revenue.\n",
      "- **Market Focus:** Both companies are experiencing growth in data center and gaming-related products, but AMD's detailed segment performance highlights specific product lines contributing to growth, such as the AMD EPYC™ and Ryzen™ processors.\n",
      "\n",
      "Overall, AMD appears to be experiencing significant revenue growth and improved profitability, while NVIDIA's data highlights customer concentration and increased costs, without specific growth metrics.\n",
      "\n",
      "======================================================================\n",
      "SOURCES (5)\n",
      "======================================================================\n",
      "\n",
      "1. NVIDIA CORP (NVDA) - ITEM 1\n",
      "   Filing Date: 2024-11-20\n",
      "   Relevance Score: 0.6546\n",
      "\n",
      "2. NVIDIA CORP (NVDA) - ITEM 1\n",
      "   Filing Date: 2024-11-20\n",
      "   Relevance Score: 0.6414\n",
      "\n",
      "3. NVIDIA CORP (NVDA) - ITEM 1\n",
      "   Filing Date: 2024-08-28\n",
      "   Relevance Score: 0.6392\n",
      "\n",
      "4. ADVANCED MICRO DEVICES INC (AMD) - ITEM 2\n",
      "   Filing Date: 2025-05-07\n",
      "   Relevance Score: 0.6338\n",
      "\n",
      "5. ADVANCED MICRO DEVICES INC (AMD) - ITEM 2\n",
      "   Filing Date: 2025-11-05\n",
      "   Relevance Score: 0.6258\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "--- Running Query: What was Tesla's R&D spending in the latest quarter? ---\n",
      "\n",
      " Processing query with Pre-Retrieval Engine: 'What was Tesla's R&D spending in the latest quarter?'\n",
      "   → Processing query (Normalization, Expansion, Embedding)...\n",
      "   → No filters applied (NER filtering is OFF for this model).\n",
      "   → Manually searching Qdrant...\n",
      "   → Formatting context and building prompt...\n",
      "   → Sending prompt to LLM...\n",
      "\n",
      "======================================================================\n",
      "ANSWER\n",
      "======================================================================\n",
      "\n",
      "In the latest quarter, which is the three months ended September 30, 2025, Tesla's R&D expenses increased by $591 million, or 57%, compared to the same period in 2024. However, the exact amount of R&D spending for the latest quarter is not directly provided in the context. To determine the exact spending, one would need the R&D expenses for the three months ended September 30, 2024, and then add the $591 million increase to that figure.\n",
      "\n",
      "======================================================================\n",
      "SOURCES (5)\n",
      "======================================================================\n",
      "\n",
      "1. Tesla, Inc. (TSLA) - ITEM 2\n",
      "   Filing Date: 2024-10-24\n",
      "   Relevance Score: 0.5818\n",
      "\n",
      "2. Tesla, Inc. (TSLA) - ITEM 2\n",
      "   Filing Date: 2025-10-23\n",
      "   Relevance Score: 0.5687\n",
      "\n",
      "3. Tesla, Inc. (TSLA) - ITEM 2\n",
      "   Filing Date: 2025-07-24\n",
      "   Relevance Score: 0.5555\n",
      "\n",
      "4. Tesla, Inc. (TSLA) - ITEM 2\n",
      "   Filing Date: 2024-10-24\n",
      "   Relevance Score: 0.5433\n",
      "\n",
      "5. Alphabet Inc. (GOOGL) - ITEM 2\n",
      "   Filing Date: 2025-10-30\n",
      "   Relevance Score: 0.5300\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "--- Running Query: How has Microsoft's operating income changed over the last year? ---\n",
      "\n",
      " Processing query with Pre-Retrieval Engine: 'How has Microsoft's operating income changed over the last year?'\n",
      "   → Processing query (Normalization, Expansion, Embedding)...\n",
      "   → No filters applied (NER filtering is OFF for this model).\n",
      "   → Manually searching Qdrant...\n",
      "   → Formatting context and building prompt...\n",
      "   → Sending prompt to LLM...\n",
      "\n",
      "======================================================================\n",
      "ANSWER\n",
      "======================================================================\n",
      "\n",
      "The provided context does not contain any information about Microsoft's operating income. The context only includes financial data for Google (GOOGL) and AMD. To determine how Microsoft's operating income has changed over the last year, you would need to refer to Microsoft's financial statements or reports.\n",
      "\n",
      "======================================================================\n",
      "SOURCES (5)\n",
      "======================================================================\n",
      "\n",
      "1. Alphabet Inc. (GOOGL) - ITEM 2\n",
      "   Filing Date: 2025-10-30\n",
      "   Relevance Score: 0.5671\n",
      "\n",
      "2. ADVANCED MICRO DEVICES INC (AMD) - ITEM 2\n",
      "   Filing Date: 2024-10-30\n",
      "   Relevance Score: 0.5590\n",
      "\n",
      "3. ADVANCED MICRO DEVICES INC (AMD) - ITEM 2\n",
      "   Filing Date: 2025-05-07\n",
      "   Relevance Score: 0.5587\n",
      "\n",
      "4. Alphabet Inc. (GOOGL) - ITEM 2\n",
      "   Filing Date: 2024-10-30\n",
      "   Relevance Score: 0.5536\n",
      "\n",
      "5. ADVANCED MICRO DEVICES INC (AMD) - ITEM 2\n",
      "   Filing Date: 2025-11-05\n",
      "   Relevance Score: 0.5526\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "--- Running Query: What was the gross profit margin for all companies? ---\n",
      "\n",
      " Processing query with Pre-Retrieval Engine: 'What was the gross profit margin for all companies?'\n",
      "   → Processing query (Normalization, Expansion, Embedding)...\n",
      "   → No filters applied (NER filtering is OFF for this model).\n",
      "   → Manually searching Qdrant...\n",
      "   → Formatting context and building prompt...\n",
      "   → Sending prompt to LLM...\n",
      "\n",
      "======================================================================\n",
      "ANSWER\n",
      "======================================================================\n",
      "\n",
      "Based on the provided context, here are the gross profit margins for the companies mentioned:\n",
      "\n",
      "1. **NVIDIA (NVDA):**\n",
      "   - Second quarter of fiscal year 2025: 75.1%\n",
      "   - First half of fiscal year 2025: 76.6%\n",
      "   - Second quarter of fiscal year 2024: 70.1%\n",
      "   - First half of fiscal year 2024: 68.2%\n",
      "\n",
      "2. **Oracle (ORCL):**\n",
      "   - The context does not provide specific gross profit margin percentages for Oracle. The information provided focuses on the exclusion of certain costs and expenses from the reported margins, and a reconciliation to income before income taxes is mentioned but not detailed in the context.\n",
      "\n",
      "3. **Apple (AAPL):**\n",
      "   - For the three months ended June 28, 2025:\n",
      "     - Products: 34.5%\n",
      "     - Services: 75.6%\n",
      "     - Total: 46.5%\n",
      "   - For the nine months ended June 29, 2024:\n",
      "     - Products: 35.3%\n",
      "     - Services: 74.0%\n",
      "     - Total: 46.3%\n",
      "\n",
      "These figures represent the gross profit margins as reported in the context for each company.\n",
      "\n",
      "======================================================================\n",
      "SOURCES (5)\n",
      "======================================================================\n",
      "\n",
      "1. NVIDIA CORP (NVDA) - ITEM 2\n",
      "   Filing Date: 2024-08-28\n",
      "   Relevance Score: 0.6454\n",
      "\n",
      "2. ORACLE CORP (ORCL) - ITEM 1\n",
      "   Filing Date: 2024-09-10\n",
      "   Relevance Score: 0.6310\n",
      "\n",
      "3. ORACLE CORP (ORCL) - ITEM 1\n",
      "   Filing Date: 2025-03-11\n",
      "   Relevance Score: 0.6257\n",
      "\n",
      "4. ORACLE CORP (ORCL) - ITEM 1\n",
      "   Filing Date: 2024-12-10\n",
      "   Relevance Score: 0.6232\n",
      "\n",
      "5. Apple Inc. (AAPL) - ITEM 2\n",
      "   Filing Date: 2025-08-01\n",
      "   Relevance Score: 0.6031\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "--- All queries complete. Saving all results to: data\\RAG_query_processing_results.json ---\n",
      "✓ Successfully saved all answers.\n",
      "\n",
      "Pipeline run finished.\n"
     ]
    }
   ],
   "source": [
    "DATA_FOLDER = \"data\"\n",
    "if not os.path.exists(DATA_FOLDER):\n",
    "    os.makedirs(DATA_FOLDER); print(f\"Created data folder: {DATA_FOLDER}\")\n",
    "\n",
    "queries_to_run = [\n",
    "    {\"question\": \"What are the main risk factors mentioned by each companies?\", \"ticker_filter\": None},\n",
    "    {\"question\": \"What risks did Apple disclose in their latest 10-Q?\", \"ticker_filter\": None},\n",
    "    {\"question\": \"Compare the revenue trends of NVIDIA and AMD\", \"ticker_filter\": None},\n",
    "    {\"question\": \"What was Tesla's R&D spending in the latest quarter?\", \"ticker_filter\": None},\n",
    "    {\"question\": \"How has Microsoft's operating income changed over the last year?\", \"ticker_filter\": None},\n",
    "    {\"question\": \"What was the gross profit margin for all companies?\", \"ticker_filter\": None},\n",
    "]\n",
    "\n",
    "print(f\"\\nRunning {len(queries_to_run)} queries with (Model 4) Pre-Retrieval RAG...\")\n",
    "all_results = []\n",
    "for query in queries_to_run:\n",
    "    print(f\"\\n--- Running Query: {query['question']} ---\")\n",
    "    result = pipeline.query(question=query[\"question\"], ticker_filter=query[\"ticker_filter\"])\n",
    "    all_results.append({\"question\": query[\"question\"], \"ticker_filter_manual\": query[\"ticker_filter\"], \"response\": result})\n",
    "\n",
    "save_path = os.path.join(DATA_FOLDER, \"RAG_query_processing_results.json\")\n",
    "print(f\"\\n--- All queries complete. Saving all results to: {save_path} ---\")\n",
    "try:\n",
    "    with open(save_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_results, f, indent=4)\n",
    "    print(f\"✓ Successfully saved all answers.\")\n",
    "except Exception as e: print(f\"✗ Failed to save results: {e}\")\n",
    "print(\"\\nPipeline run finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSA4213-SEC-Fillings-Chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
