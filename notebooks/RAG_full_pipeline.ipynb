{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9acce990",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ngzhi\\Documents\\Year 4 Sem 1\\DSA4213\\DSA4213-SEC-Fillings-Chatbot\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import uuid\n",
    "import time\n",
    "import requests\n",
    "import gc\n",
    "import json\n",
    "import unicodedata\n",
    "import pathlib\n",
    "from typing import List, Dict, Tuple, Any, Optional\n",
    "from collections import defaultdict\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "# External libraries\n",
    "from dotenv import load_dotenv\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder # Import CrossEncoder\n",
    "from qdrant_client import QdrantClient, models\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
    "from openai import OpenAI\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Imports for QueryProcessor\n",
    "import spacy\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0152ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# PART 1: CONFIGURATION\n",
    "# ===========================================================================\n",
    "class Config:\n",
    "    QDRANT_URL = os.getenv(\"QDRANT_URL\")\n",
    "    QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")\n",
    "    OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "    SEC_HEADERS = {'User-Agent': 'FullAdvancedRAG research@example.com'}\n",
    "    CIK_MAP_URL = 'https://www.sec.gov/files/company_tickers.json'\n",
    "\n",
    "    # Model Configuration\n",
    "    EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    LLM_MODEL = \"gpt-4o\"\n",
    "    FLAN_T5_MODEL_NAME = \"google/flan-t5-small\"\n",
    "    SPACY_MODEL = \"en_core_web_lg\" # Use the large, more accurate model\n",
    "    \n",
    "    # --- Post-Retrieval Config ---\n",
    "    CROSS_ENCODER_MODEL = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "    RETRIEVAL_TOP_K = 20 # Retrieve 20 candidates\n",
    "    FINAL_TOP_K = 5      # Re-rank down to 5\n",
    "\n",
    "    # --- Collection ---\n",
    "    # !! CRITICAL: New collection name. This pipeline MUST re-index.\n",
    "    COLLECTION_NAME = \"sec_filings_10q_v_full\" \n",
    "    VECTOR_SIZE = 384\n",
    "    CHUNK_SIZE = 800\n",
    "    CHUNK_OVERLAP = 200\n",
    "    TICKERS = ['NVDA', 'AAPL', 'MSFT', 'AMZN', 'META', 'GOOGL', 'TSLA', 'ORCL', 'JPM', 'AMD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6f98d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# PART 2: DOCUMENT LOADING\n",
    "# ===========================================================================\n",
    "# (Using the table-aware parser from RAG_post_retrieval.ipynb)\n",
    "\n",
    "class SECDocumentLoader:\n",
    "    @staticmethod\n",
    "    def get_recent_10q_metadata(ticker: str, num_filings: int = 4) -> List[Dict[str, str]]:\n",
    "        print(f\"  → Fetching CIK for ticker: {ticker}...\")\n",
    "        response = requests.get(Config.CIK_MAP_URL, headers=Config.SEC_HEADERS)\n",
    "        response.raise_for_status()\n",
    "        company_data = response.json()\n",
    "        cik = None\n",
    "        company_name = None\n",
    "        for company in company_data.values():\n",
    "            if company['ticker'] == ticker.upper():\n",
    "                cik = str(company['cik_str']).zfill(10)\n",
    "                company_name = company['title']\n",
    "                break\n",
    "        if not cik: raise ValueError(f\"Ticker '{ticker}' not found\")\n",
    "        print(f\"  → Found CIK: {cik} ({company_name})\")\n",
    "        submissions_url = f\"https://data.sec.gov/submissions/CIK{cik}.json\"\n",
    "        time.sleep(0.1)\n",
    "        response = requests.get(submissions_url, headers=Config.SEC_HEADERS)\n",
    "        response.raise_for_status()\n",
    "        submissions = response.json()\n",
    "        filings_metadata = []\n",
    "        for i, form in enumerate(submissions['filings']['recent']['form']):\n",
    "            if form == '10-Q':\n",
    "                accession_number = submissions['filings']['recent']['accessionNumber'][i]\n",
    "                primary_document = submissions['filings']['recent']['primaryDocument'][i]\n",
    "                filing_date = submissions['filings']['recent']['filingDate'][i]\n",
    "                accession_number_clean = accession_number.replace('-', '')\n",
    "                filing_url = f\"https://www.sec.gov/Archives/edgar/data/{cik}/{accession_number_clean}/{primary_document}\"\n",
    "                metadata = {'ticker': ticker.upper(), 'company_name': company_name, 'filing_date': filing_date, 'cik': cik, 'filing_url': filing_url}\n",
    "                filings_metadata.append(metadata)\n",
    "                if len(filings_metadata) >= num_filings: break\n",
    "        if not filings_metadata: raise ValueError(f\"No recent 10-Q filings found for ticker '{ticker}'\")\n",
    "        print(f\"  → Found {len(filings_metadata)} recent 10-Q filing metadata entries.\")\n",
    "        return filings_metadata\n",
    "\n",
    "    @staticmethod\n",
    "    def get_filing_html(filing_url: str) -> str:\n",
    "        time.sleep(0.1); response = requests.get(filing_url, headers=Config.SEC_HEADERS); response.raise_for_status(); return response.text\n",
    "\n",
    "    @staticmethod\n",
    "    def _normalize_header_text(text: str) -> str:\n",
    "        text = text.strip().upper()\n",
    "        part_match = re.search(r'^\\s*(PART\\s+I{1,2})', text)\n",
    "        if part_match: return re.sub(r'\\s+', ' ', part_match.group(1))\n",
    "        item_match = re.search(r'^\\s*(ITEM\\s+\\d[A-Z]?)', text)\n",
    "        if item_match: return re.sub(r'\\s+', ' ', item_match.group(1))\n",
    "        return None\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_html_table(table_tag) -> str:\n",
    "        rows = []\n",
    "        for tr in table_tag.find_all('tr'):\n",
    "            cells = [\" \".join(cell.get_text(strip=True).split()) for cell in tr.find_all(['td', 'th'])]\n",
    "            if any(cells): rows.append(cells)\n",
    "        if not rows: return \"\"\n",
    "        header = rows[0]; md = [\"| \" + \" | \".join(header) + \" |\", \"| \" + \" | \".join(['---'] * len(header)) + \" |\"]\n",
    "        for row in rows[1:]:\n",
    "            while len(row) < len(header): row.append(\"\")\n",
    "            md.append(\"| \" + \" | \".join(row[:len(header)]) + \" |\")\n",
    "        return \"\\n\" + \"\\n\".join(md) + \"\\n\"\n",
    "\n",
    "    @classmethod\n",
    "    def parse_10q(cls, html_content: str) -> Dict:\n",
    "        soup = BeautifulSoup(html_content, 'lxml'); headers = []\n",
    "        for h in soup.find_all(['p', 'b', 'strong', 'div']):\n",
    "            text = h.get_text(strip=True)\n",
    "            if len(text) > 100: continue\n",
    "            key = cls._normalize_header_text(text)\n",
    "            if key and not h.find_parent('a'): headers.append({'tag': h, 'key': key})\n",
    "        if not headers: return {}\n",
    "        data = defaultdict(lambda: defaultdict(str)); part_key = None\n",
    "        for i, h_info in enumerate(headers):\n",
    "            key = h_info['key']\n",
    "            if 'PART' in key: part_key = key; continue\n",
    "            if 'ITEM' in key:\n",
    "                if not part_key: part_key = \"PART I\"\n",
    "                start_node = h_info['tag']; end_node = headers[i + 1]['tag'] if i + 1 < len(headers) else None\n",
    "                parts = []\n",
    "                for elem in start_node.next_elements:\n",
    "                    if elem == end_node: break\n",
    "                    if isinstance(elem, NavigableString) and not elem.find_parent('table'):\n",
    "                        if txt := elem.strip(): parts.append(txt)\n",
    "                    elif elem.name == 'table' and not elem.find_parent('table'):\n",
    "                        if md := cls._parse_html_table(elem): parts.append(md)\n",
    "                data[part_key][key] = re.sub(r'\\n{3,}', '\\n\\n', \"\\n\".join(parts)).strip()\n",
    "        return {p: dict(i) for p, i in data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d6e8cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# PART 3: TEXT CHUNKING & EMBEDDING\n",
    "# ===========================================================================\n",
    "# (Using the table-aware chunker from RAG_post_retrieval.ipynb)\n",
    "class DocumentProcessor:\n",
    "    def __init__(self, embedding_model_name: str = Config.EMBEDDING_MODEL):\n",
    "        print(f\"\\n Loading embedding model: {embedding_model_name}\")\n",
    "        self.model = SentenceTransformer(embedding_model_name)\n",
    "        print(f\"   ✓ Model loaded (dimension: {self.model.get_sentence_embedding_dimension()})\")\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(chunk_size=Config.CHUNK_SIZE, chunk_overlap=Config.CHUNK_OVERLAP, length_function=len, add_start_index=False)\n",
    "        print(f\"   ✓ Initialized RecursiveCharacterTextSplitter\")\n",
    "        \n",
    "    def generate_document_chunks(self, parsed_data: Dict, metadata: Dict, embed_batch_size: int = 1024):\n",
    "        all_docs = []\n",
    "        for part, items in parsed_data.items():\n",
    "            for item, content in items.items():\n",
    "                if not content: continue\n",
    "                # --- NEW: Add table detection ---\n",
    "                is_table = False\n",
    "                if content.strip().startswith(\"|\") and \"---\" in content.strip().split('\\n')[1]:\n",
    "                    is_table = True\n",
    "                # ---\n",
    "                doc_metadata = {'ticker': metadata['ticker'], 'company_name': metadata['company_name'], 'filing_date': metadata['filing_date'], 'filing_url': metadata['filing_url'], 'part': part, 'item': item, 'is_table': is_table} # Add is_table\n",
    "                all_docs.append(Document(page_content=content, metadata=doc_metadata))\n",
    "        if not all_docs: return\n",
    "        print(f\"     → Splitting {len(all_docs)} high-level 'Items' into smaller chunks...\")\n",
    "        chunked_docs = self.text_splitter.split_documents(all_docs)\n",
    "        print(f\"     → Generated {len(chunked_docs)} chunks\")\n",
    "        text_batch, metadata_batch = [], []\n",
    "        for chunk in chunked_docs:\n",
    "            text_batch.append(chunk.page_content); metadata_batch.append(chunk.metadata)\n",
    "            if len(text_batch) >= embed_batch_size:\n",
    "                embeddings = self.model.encode(text_batch, show_progress_bar=False)\n",
    "                for txt, emb, meta in zip(text_batch, embeddings, metadata_batch):\n",
    "                    yield PointStruct(id=str(uuid.uuid4()), vector=emb.tolist(), payload={'text': txt, **meta})\n",
    "                text_batch, metadata_batch = [], []\n",
    "        if text_batch:\n",
    "            embeddings = self.model.encode(text_batch, show_progress_bar=False)\n",
    "            for txt, emb, meta in zip(text_batch, embeddings, metadata_batch):\n",
    "                yield PointStruct(id=str(uuid.uuid4()), vector=emb.tolist(), payload={'text': txt, **meta})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa91cb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# PART 4: QDRANT VECTOR DATABASE\n",
    "# ===========================================================================\n",
    "# (With 'is_table' index and multi-filter search)\n",
    "class QdrantManager:\n",
    "    def __init__(self):\n",
    "        print(f\"\\nConnecting to Qdrant Cloud...\")\n",
    "        self.client = QdrantClient(url=Config.QDRANT_URL, api_key=Config.QDRANT_API_KEY)\n",
    "        print(f\"   ✓ Connected to Qdrant\")\n",
    "        \n",
    "    def create_collection(self, collection_name: str = Config.COLLECTION_NAME, vector_size: int = Config.VECTOR_SIZE):\n",
    "        print(f\"\\n Setting up collection: {collection_name}\")\n",
    "        collections = self.client.get_collections().collections\n",
    "        exists = any(col.name == collection_name for col in collections)\n",
    "        if exists:\n",
    "            print(f\"   ⚠ Collection '{collection_name}' exists, recreating...\")\n",
    "            self.client.delete_collection(collection_name)\n",
    "            \n",
    "        self.client.create_collection(collection_name=collection_name, vectors_config=models.VectorParams(size=vector_size, distance=models.Distance.COSINE))\n",
    "        print(f\"   ✓ Collection created\")\n",
    "        \n",
    "        for field in [\"ticker\", \"item\"]:\n",
    "            print(f\"   → Creating payload index for '{field}' (Keyword)...\")\n",
    "            self.client.create_payload_index(collection_name=collection_name, field_name=field, field_schema=models.PayloadSchemaType.KEYWORD)\n",
    "            \n",
    "        print(f\"   → Creating payload index for 'is_table' (Boolean)...\")\n",
    "        self.client.create_payload_index(collection_name=collection_name, field_name=\"is_table\", field_schema=models.PayloadSchemaType.BOOL)\n",
    "        print(f\"   ✓ Payload indexes created.\")\n",
    "\n",
    "    def upsert_documents(self, points_generator, collection_name: str = Config.COLLECTION_NAME, batch_size: int = 2048) -> int:\n",
    "        print(f\" Uploading chunks to Qdrant in batches of {batch_size}...\"); batch, count = [], 0\n",
    "        for point in points_generator:\n",
    "            batch.append(point)\n",
    "            if len(batch) >= batch_size:\n",
    "                self.client.upsert(collection_name=collection_name, points=batch, wait=False); count += len(batch); print(f\"     → Uploaded {count} chunks...\"); batch = []\n",
    "        if batch: self.client.upsert(collection_name=collection_name, points=batch, wait=False); count += len(batch)\n",
    "        print(f\"  ✓ All chunks uploaded. Total: {count}\"); return count\n",
    "\n",
    "    def search(self, query_vector: List[float], collection_name: str = Config.COLLECTION_NAME, limit: int = Config.RETRIEVAL_TOP_K, filter_dict: Dict = None) -> List[Dict]:\n",
    "        qdrant_filter = None\n",
    "        if filter_dict:\n",
    "            must_conditions = []\n",
    "            for key, value in filter_dict.items():\n",
    "                if isinstance(value, list): must_conditions.append(models.FieldCondition(key=key, match=models.MatchAny(any=value)))\n",
    "                elif isinstance(value, bool): must_conditions.append(models.FieldCondition(key=key, match=models.MatchValue(value=value)))\n",
    "                else: must_conditions.append(models.FieldCondition(key=key, match=models.MatchValue(value=value)))\n",
    "            qdrant_filter = models.Filter(must=must_conditions)\n",
    "            \n",
    "        results = self.client.search(collection_name=collection_name, query_vector=query_vector, limit=limit, query_filter=qdrant_filter, with_payload=True)\n",
    "        return [{'score': result.score, 'payload': result.payload} for result in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1daa59d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded spaCy model for NER\n",
      "✓ Loaded SentenceTransformer model 'sentence-transformers/all-MiniLM-L6-v2'\n",
      "✓ Loaded Flan-T5 model 'google/flan-t5-small' on cpu\n"
     ]
    }
   ],
   "source": [
    "# ===========================================================================\n",
    "# PART 5: QUERY PROCESSOR MODULE\n",
    "# ===========================================================================\n",
    "# (Full QueryProcessor logic from query_processors.ipynb, with the BUG FIX)\n",
    "\n",
    "USE_SBERT = True\n",
    "USE_FLAN_T5 = True\n",
    "\n",
    "def normalize(text: str) -> str:\n",
    "    text = unicodedata.normalize(\"NFKC\", text).strip()\n",
    "    return re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "TOKEN_RE = re.compile(r\"[A-Za-z0-9]+(?:'[A-Za-z0-9]+)?|[&$%.\\-]+\")\n",
    "def simple_tokenize(text: str) -> List[str]:\n",
    "    toks = TOKEN_RE.findall(text); cleaned = []\n",
    "    for t in toks:\n",
    "        if t.lower() == \"'s\": continue\n",
    "        if t.endswith(\"'s\"): t = t[:-2]\n",
    "        cleaned.append(t)\n",
    "    return cleaned\n",
    "\n",
    "DOMAIN_SYNONYMS = {\"risk\": [\"risk factor\",\"risk factors\",\"uncertainty\",\"exposure\",\"threat\"], \"cyber\": [\"cybersecurity\",\"information security\",\"infosec\",\"data breach\",\"security incident\"], \"performance\": [\"revenue\",\"growth\",\"margin\",\"profit\",\"loss\",\"guidance\",\"results\"], \"strategy\": [\"roadmap\",\"plan\",\"initiative\",\"expansion\",\"capex\",\"restructuring\",\"acquisition\"], \"md&a\": [\"management discussion\",\"md&a\",\"results of operations\"],}\n",
    "COMPANY_TICKERS = {\"tesla\":\"TSLA\", \"apple\":\"AAPL\", \"microsoft\":\"MSFT\", \"nvidia\":\"NVDA\", \"google\":\"GOOGL\", \"alphabet\":\"GOOGL\", \"meta\":\"META\", \"amazon\":\"AMZN\", \"amd\":\"AMD\", \"oracle\":\"ORCL\", \"jpmorgan\":\"JPM\", \"jpm\":\"JPM\"}\n",
    "\n",
    "def keyword_expand(tokens: List[str]) -> List[str]:\n",
    "    ex = []; t0s = [t.strip(\".-\").lower() for t in tokens]\n",
    "    for t0 in t0s: ex.extend(DOMAIN_SYNONYMS.get(t0, []))\n",
    "    seen, out = set(), [];\n",
    "    for w in ex:\n",
    "        if w not in seen: seen.add(w); out.append(w)\n",
    "    return out\n",
    "\n",
    "def build_keywords(tokens: List[str], expansions: List[str]) -> List[str]:\n",
    "    kept = []\n",
    "    for t in tokens + expansions:\n",
    "        t = t.lower()\n",
    "        if re.search(r\"[a-z0-9]\", t) and t not in kept: kept.append(t)\n",
    "    return kept\n",
    "\n",
    "try:\n",
    "    _nlp = spacy.load(Config.SPACY_MODEL)\n",
    "    print(\"✓ Loaded spaCy model for NER\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Failed to load spaCy model '{Config.SPACY_MODEL}'. NER will be limited.\")\n",
    "    print(\"  Run: python -m spacy download en_core_web_lg\")\n",
    "    _nlp = None\n",
    "\n",
    "def extract_entities(raw_text: str) -> dict:\n",
    "    out = {}; low = raw_text.lower()\n",
    "    q = re.findall(r\"\\b(q[1-4])\\s*([12][0-9]{3})\\b\", low)\n",
    "    if q: out[\"quarter\"] = [f\"{p.upper()} {y}\" for p, y in q]\n",
    "    years = re.findall(r\"\\b(20[0-4][0-9]|19[0-9]{2})\\b\", raw_text)\n",
    "    if years: out[\"year\"] = sorted(set(years))\n",
    "    \n",
    "    companies = set()\n",
    "    if _nlp is not None:\n",
    "        doc = _nlp(raw_text)\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == \"ORG\": companies.add(ent.text.strip())\n",
    "            \n",
    "    # --- THIS IS THE BUG FIX ---\n",
    "    low_raw = raw_text.lower() \n",
    "    # --- END OF BUG FIX ---\n",
    "    \n",
    "    for name in COMPANY_TICKERS:\n",
    "        if name in low_raw: companies.add(name.title()) # 'low_raw' is now defined\n",
    "        \n",
    "    if companies: out[\"company\"] = sorted(companies)\n",
    "    \n",
    "    tickers = set(COMPANY_TICKERS.get(c.lower(),\"\") for c in companies if COMPANY_TICKERS.get(c.lower()))\n",
    "    tickers.update(re.findall(r\"\\$([A-Z]{1,5})\\b\", raw_text))\n",
    "    tickers.update(re.findall(r\"\\(([A-Z]{1,5})\\)\", raw_text))\n",
    "    tickers.update(re.findall(r\"\\b(?:NASDAQ|NYSE)\\s*:\\s*([A-Z]{1,5})\\b\", raw_text))\n",
    "    tickers = {t for t in tickers if t}\n",
    "    if tickers: out[\"ticker\"] = sorted(tickers)\n",
    "    return out\n",
    "\n",
    "try:\n",
    "    _sbert = SentenceTransformer(Config.EMBEDDING_MODEL) if USE_SBERT else None\n",
    "    print(f\"✓ Loaded SentenceTransformer model '{Config.EMBEDDING_MODEL}'\")\n",
    "except Exception: _sbert = None; print(f\"✗ Failed to load SentenceTransformer\")\n",
    "\n",
    "def sbert_embed(text: str) -> Optional[List[float]]:\n",
    "    if _sbert is None: return None\n",
    "    v = _sbert.encode([text], normalize_embeddings=True)[0]\n",
    "    return v.tolist()\n",
    "\n",
    "try:\n",
    "    _flan_device = \"cuda\" if USE_FLAN_T5 and torch.cuda.is_available() else \"cpu\"\n",
    "    _flan_tok = AutoTokenizer.from_pretrained(Config.FLAN_T5_MODEL_NAME) if USE_FLAN_T5 else None\n",
    "    _flan_mdl = AutoModelForSeq2SeqLM.from_pretrained(Config.FLAN_T5_MODEL_NAME).to(_flan_device).eval() if USE_FLAN_T5 else None\n",
    "    print(f\"✓ Loaded Flan-T5 model '{Config.FLAN_T5_MODEL_NAME}' on {_flan_device}\")\n",
    "except Exception as e:\n",
    "    _flan_tok = _flan_mdl = None; _flan_device = \"cpu\"; print(f\"✗ Failed to load Flan-T5 model: {e}\")\n",
    "\n",
    "def t5_paraphrases_safe(q: str, num_return: int = 3, max_new_tokens: int = 48) -> List[str]:\n",
    "    if not (USE_FLAN_T5 and _flan_tok is not None and _flan_mdl is not None): return []\n",
    "    prompt = f\"Rewrite the query into multiple short paraphrases without adding facts or numbers. Keep meaning.\\nQuery: {q}\"\n",
    "    x = _flan_tok(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    x = {k: v.to(_flan_device) for k, v in x.items()}\n",
    "    with torch.no_grad():\n",
    "        out = _flan_mdl.generate(**x, do_sample=True, top_k=50, top_p=0.92, temperature=0.9, num_return_sequences=num_return, max_new_tokens=max_new_tokens, repetition_penalty=1.1, no_repeat_ngram_size=3)\n",
    "    paras = _flan_tok.batch_decode(out, skip_special_tokens=True)\n",
    "    base = re.sub(r\"\\W+\",\" \", q).strip().lower()\n",
    "    seen, kept = set(), []\n",
    "    for p in paras:\n",
    "        p2 = normalize(p); p2_cmp = re.sub(r\"\\W+\",\" \", p2).strip().lower()\n",
    "        if p2_cmp == base: continue\n",
    "        if p2 and p2 not in seen: seen.add(p2); kept.append(p2)\n",
    "    return kept[:num_return]\n",
    "\n",
    "INTENT_LABELS = [\"risk\",\"performance\",\"strategy\"]\n",
    "X_train = [\"What new risk factors?\", \"Cybersecurity breach Tesla\", \"Explain Apple revenue growth\", \"Compare Microsoft profit guidance\", \"Outline Nvidia expansion strategy\", \"What restructuring plan?\"]\n",
    "y_train = [\"risk\",\"risk\",\"performance\",\"performance\",\"strategy\",\"strategy\"]\n",
    "_intent_clf = Pipeline([(\"tfidf\", TfidfVectorizer(ngram_range=(1,2), min_df=1)), (\"lr\", LogisticRegression(max_iter=300, class_weight=\"balanced\", multi_class=\"ovr\"))]).fit(X_train, y_train)\n",
    "RISK_KW = {\"risk\",\"risk factor\",\"risk factors\",\"uncertainty\",\"cyber\",\"cybersecurity\",\"breach\",\"litigation\",\"security\"}\n",
    "PERF_KW = {\"revenue\",\"growth\",\"margin\",\"profit\",\"loss\",\"guidance\",\"results\",\"compare\",\"last quarter\",\"quarterly\"}\n",
    "STRAT_KW= {\"strategy\",\"plan\",\"roadmap\",\"expansion\",\"acquisition\",\"restructuring\",\"capex\",\"data center\",\"data centers\"}\n",
    "def _kw_score(t: str, kws: set[str]) -> int: return sum(1 for k in kws if k in t)\n",
    "def classify_intent(text: str) -> Tuple[str, float]:\n",
    "    tx = normalize(text); proba = _intent_clf.predict_proba([tx])[0].tolist()\n",
    "    k_r = _kw_score(tx, RISK_KW); k_p = _kw_score(tx, PERF_KW); k_s = _kw_score(tx, STRAT_KW)\n",
    "    k_sum = max(1, (k_r + k_p + k_s)); priors = [k_r/k_sum, k_p/k_sum, k_s/k_sum]\n",
    "    alpha, beta = 0.6, 0.4; blended = [alpha*proba[i] + beta*priors[i] for i in range(3)]\n",
    "    s = sum(blended) or 1.0; blended = [b/s for b in blended]\n",
    "    idx = max(range(3), key=lambda i: blended[i])\n",
    "    return INTENT_LABELS[idx], float(blended[idx])\n",
    "\n",
    "def expand_query(query: str) -> dict:\n",
    "    norm = normalize(query); toks = simple_tokenize(norm); lex_ex = keyword_expand(toks)\n",
    "    paras = t5_paraphrases_safe(norm, num_return=3, max_new_tokens=48) if USE_FLAN_T5 else []\n",
    "    para_tokens = list(dict.fromkeys(t for p in paras for t in simple_tokenize(p)))\n",
    "    para_ex = keyword_expand(para_tokens) if para_tokens else []\n",
    "    expansions = list(dict.fromkeys(w for lst in (lex_ex, para_ex) for w in lst))\n",
    "    return {\"normalized\": norm, \"tokens\": toks, \"expansions\": expansions, \"paraphrases\": paras, \"keywords\": build_keywords(toks, expansions)}\n",
    "\n",
    "@dataclass\n",
    "class QueryProcessorConfig:\n",
    "    labels: List[str] = field(default_factory=lambda: [\"risk\",\"performance\",\"strategy\"])\n",
    "class QueryProcessor:\n",
    "    def __init__(self, config: QueryProcessorConfig = QueryProcessorConfig()):\n",
    "        self.config = config; print(\"✓ QueryProcessor initialized\")\n",
    "    def process(self, query: str) -> Dict[str, Any]:\n",
    "        raw = query; ex = expand_query(query); ents = extract_entities(raw)\n",
    "        label, conf = classify_intent(ex[\"normalized\"])\n",
    "        emb = sbert_embed(ex[\"normalized\"]) if USE_SBERT else None\n",
    "        return {\"normalized\": ex[\"normalized\"], \"label\": label, \"confidence\": conf, \"expansions\": ex[\"expansions\"], \"paraphrases\": ex[\"paraphrases\"], \"keywords\": ex[\"keywords\"], \"entities\": ents, \"filters\": ents.copy(), \"embedding\": emb}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b674411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# PART 6: RAG QUERY ENGINE (MODEL 5)\n",
    "# ===========================================================================\n",
    "# This is the new, combined engine.\n",
    "class FullAdvancedRAGEngine:\n",
    "    def __init__(self, document_processor: DocumentProcessor, \n",
    "                 qdrant_manager: QdrantManager, \n",
    "                 query_processor: QueryProcessor):\n",
    "        print(\"\\n Initializing Full Advanced RAG Engine (Model 5)...\")\n",
    "        \n",
    "        # --- All Components ---\n",
    "        self.embedding_model = document_processor.model\n",
    "        self.qdrant_manager = qdrant_manager\n",
    "        self.query_processor = query_processor\n",
    "        print(\"   ✓ Processor, QdrantManager, and QueryProcessor attached.\")\n",
    "\n",
    "        # --- Post-Retrieval ---\n",
    "        print(f\"   → Loading Cross-Encoder: {Config.CROSS_ENCODER_MODEL}...\")\n",
    "        self.cross_encoder = CrossEncoder(Config.CROSS_ENCODER_MODEL)\n",
    "        print(\"   ✓ Cross-Encoder model loaded.\")\n",
    "        \n",
    "        # --- LLM & Prompts ---\n",
    "        self.llm = ChatOpenAI(model=Config.LLM_MODEL, api_key=Config.OPENAI_API_KEY, temperature=0)\n",
    "        print(\"   ✓ Initialized ChatOpenAI LLM\")\n",
    "        \n",
    "        # Generation Prompt\n",
    "        template = \"\"\"You are a helpful financial analyst assistant. Your role is to answer questions about SEC 10-Q filings based ONLY on the provided context.\n",
    "- Base your answer strictly on the provided context from SEC filings\n",
    "- Cite specific sections (e.g., \"According to Item 1A...\") when referencing information\n",
    "- If the answer is not in the context, clearly state that\n",
    "\n",
    "Context:\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        self.prompt = ChatPromptTemplate.from_template(template)\n",
    "        \n",
    "        # Router Prompt (from RAG_post_retrieval)\n",
    "        router_template = \"\"\"Your job is to classify a user's question.\n",
    "Does the question ask for a specific numerical value, metric, or data from a financial table?\n",
    "(e.g., \"What was revenue?\", \"How much was R&D spending?\", \"Show me the cash flow\")\n",
    "\n",
    "Answer only with 'yes' or 'no'.\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "        self.table_query_router_prompt = ChatPromptTemplate.from_template(router_template)\n",
    "        self.query_router_chain = self.table_query_router_prompt | self.llm | StrOutputParser()\n",
    "        print(\"   ✓ Full Advanced RAG Engine ready.\")\n",
    "\n",
    "    def _format_context(self, search_results: List[Dict]) -> str:\n",
    "        context_str = \"\"\n",
    "        for i, result in enumerate(search_results, 1):\n",
    "            payload = result.get('payload', {})\n",
    "            is_table = payload.get('is_table', False)\n",
    "            table_tag = \"[TABLE DATA] \" if is_table else \"\"\n",
    "            context_str += f\"Source {i} ({payload.get('ticker','N/A')} - {payload.get('item','N/A')}) {table_tag}:\\n\\\"{payload.get('text','No text')}\\\"\\n\\n\"\n",
    "        return context_str.strip()\n",
    "\n",
    "    def query(self, question: str, ticker_filter: str = None):\n",
    "        print(f\"\\n Processing query with Full Advanced Engine: '{question}'\")\n",
    "        \n",
    "        # 1. --- PRE-RETRIEVAL + NER (QueryProcessor) ---\n",
    "        print(\"   → Step 1: Processing query (NER, Intent, Embedding)...\")\n",
    "        q_obj = self.query_processor.process(question)\n",
    "        \n",
    "        query_vector = q_obj[\"embedding\"]\n",
    "        if query_vector is None:\n",
    "            print(\"   → Warning: QP embedding failed, using base model embedding.\")\n",
    "            query_vector = self.embedding_model.encode(question).tolist()\n",
    "\n",
    "        # 2. --- FILTER GENERATION (NER + ROUTER) ---\n",
    "        print(\"   → Step 2: Generating filters...\")\n",
    "        final_filter_dict = {}\n",
    "\n",
    "        # a) Get NER filters (tickers)\n",
    "        if \"ticker\" in q_obj[\"filters\"]:\n",
    "            final_filter_dict[\"ticker\"] = q_obj[\"filters\"][\"ticker\"]\n",
    "            print(f\"   → NER auto-detected filter: {final_filter_dict}\")\n",
    "        \n",
    "        # b) Get Table Router filter\n",
    "        print(\"   → Routing for table data...\")\n",
    "        try:\n",
    "            result = self.query_router_chain.invoke({\"question\": question})\n",
    "            if 'yes' in result.lower():\n",
    "                print(\"   → Query routed to TABLE search.\")\n",
    "                final_filter_dict[\"is_table\"] = True\n",
    "            else:\n",
    "                print(\"   → Query routed to GENERAL text search.\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ⚠ Error in query routing: {e}. Defaulting to general search.\")\n",
    "        \n",
    "        # c) Apply manual override\n",
    "        if ticker_filter:\n",
    "            print(f\"   → MANUAL OVERRIDE: Applying filter: {ticker_filter}\")\n",
    "            # Override all filters if a manual one is given\n",
    "            final_filter_dict = {\"ticker\": ticker_filter.upper()} \n",
    "        \n",
    "        if not final_filter_dict:\n",
    "            print(\"   → No filters applied. Searching all documents.\")\n",
    "\n",
    "        # 3. --- RETRIEVAL ---\n",
    "        print(f\"   → Step 3: Searching Qdrant (Retrieving Top {Config.RETRIEVAL_TOP_K})...\")\n",
    "        search_results = self.qdrant_manager.search(\n",
    "            query_vector=query_vector,\n",
    "            limit=Config.RETRIEVAL_TOP_K, # Retrieve 20\n",
    "            filter_dict=final_filter_dict if final_filter_dict else None\n",
    "        )\n",
    "        if not search_results:\n",
    "            return {'answer': 'No relevant context found.', 'sources': []}\n",
    "        print(f\"   → Retrieved {len(search_results)} candidates.\")\n",
    "\n",
    "        # 4. --- POST-RETRIEVAL (Re-ranking) ---\n",
    "        print(f\"   → Step 4: Re-ranking candidates with Cross-Encoder...\")\n",
    "        passages = [result['payload']['text'] for result in search_results]\n",
    "        query_passage_pairs = [(question, passage) for passage in passages]\n",
    "        \n",
    "        cross_encoder_scores = self.cross_encoder.predict(query_passage_pairs)\n",
    "        \n",
    "        scored_results = list(zip(cross_encoder_scores, search_results))\n",
    "        scored_results.sort(key=lambda x: x[0], reverse=True) # Sort by new score\n",
    "        \n",
    "        # Get the final Top-K results\n",
    "        final_sources = []\n",
    "        for (score, result) in scored_results[:Config.FINAL_TOP_K]:\n",
    "            source_data = result['payload']\n",
    "            source_data['retrieval_score'] = result['score'] # Original vector score\n",
    "            source_data['rerank_score'] = float(score) # New cross-encoder score\n",
    "            final_sources.append(source_data)\n",
    "        print(f\"   → Re-ranked. Final {len(final_sources)} sources selected.\")\n",
    "\n",
    "        # 5. --- GENERATION ---\n",
    "        print(\"   → Step 5: Formatting context and sending to LLM...\")\n",
    "        context_results = [{'score': data['rerank_score'], 'payload': data} for data in final_sources]\n",
    "        formatted_context = self._format_context(context_results)\n",
    "        final_prompt_message = self.prompt.format_messages(context=formatted_context, input=question)\n",
    "\n",
    "        llm_response = self.llm.invoke(final_prompt_message)\n",
    "        answer = llm_response.content\n",
    "\n",
    "        # 6. --- FORMAT OUTPUT ---\n",
    "        sources_output = []\n",
    "        for i, source_data in enumerate(final_sources, 1):\n",
    "            sources_output.append({\n",
    "                'ticker': source_data.get('ticker'),\n",
    "                'company': source_data.get('company_name'),\n",
    "                'item': source_data.get('item'),\n",
    "                'is_table': source_data.get('is_table'),\n",
    "                'filing_date': source_data.get('filing_date'),\n",
    "                'rerank_score': source_data['rerank_score'],\n",
    "                'retrieval_score': source_data['retrieval_score']\n",
    "            })\n",
    "        return {'answer': answer, 'sources': sources_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "366e76b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# PART 7: MAIN PIPELINE ORCHESTRATOR\n",
    "# ===========================================================================\n",
    "class SECFilingRAGPipeline:\n",
    "    def __init__(self):\n",
    "        print(\"=\" * 70); print(\"SEC 10-Q RAG SYSTEM (Model 5: Full Advanced)\"); print(\"=\" * 70)\n",
    "        self.loader = SECDocumentLoader()\n",
    "        self.processor = DocumentProcessor()\n",
    "        self.qdrant_manager = QdrantManager()\n",
    "        self.query_processor = QueryProcessor()\n",
    "        self.query_engine = None\n",
    "\n",
    "    def load_and_index_filings(self, tickers: List[str] = Config.TICKERS, num_filings_per_ticker: int = 4):\n",
    "        print(f\"\\n{'=' * 70}\\nLOADING & INDEXING PHASE\\n{'=' * 70}\")\n",
    "        print(f\"\\nProcessing {len(tickers)} companies: {', '.join(tickers)}\")\n",
    "        print(f\"(Fetching {num_filings_per_ticker} filings per company)\\n\")\n",
    "        \n",
    "        # This pipeline *MUST* index into a new collection\n",
    "        self.qdrant_manager.create_collection()\n",
    "        \n",
    "        # Check if it's already indexed (e.g., if script was interrupted)\n",
    "        try:\n",
    "            count = self.qdrant_manager.client.count(Config.COLLECTION_NAME, exact=True)\n",
    "            if count.count > 0:\n",
    "                print(f\"Collection '{Config.COLLECTION_NAME}' already has {count.count} docs. Skipping indexing.\")\n",
    "                print(f\"{'=' * 70}\\n\"); return\n",
    "        except Exception: \n",
    "            print(\"Collection is new. Proceeding with indexing.\")\n",
    "\n",
    "        successful_tickers, failed_tickers = [], []; total_chunks_indexed = 0\n",
    "        for idx, ticker in enumerate( tickers, 1):\n",
    "            print(f\"\\n[{idx}/{len(tickers)}] Processing {ticker}\"); print(\"-\" * 70)\n",
    "            ticker_chunks_count, num_filings_processed = 0, 0\n",
    "            try:\n",
    "                filings_metadata_list = self.loader.get_recent_10q_metadata(ticker, num_filings=num_filings_per_ticker)\n",
    "                for filing_metadata in filings_metadata_list:\n",
    "                    try:\n",
    "                        print(f\"  → Downloading filing from: {filing_metadata['filing_date']}...\"); html_content = self.loader.get_filing_html(filing_metadata['filing_url'])\n",
    "                        print(f\"  → Parsing 10-Q structure...\"); parsed_data = self.loader.parse_10q(html_content)\n",
    "                        if not parsed_data: print(f\"  ⚠ Warning: No data parsed for {ticker}\"); del html_content; gc.collect(); continue\n",
    "                        print(f\"  → Creating chunks and embeddings generator...\")\n",
    "                        chunks_generator = self.processor.generate_document_chunks(parsed_data, filing_metadata)\n",
    "                        num_uploaded = self.qdrant_manager.upsert_documents(chunks_generator)\n",
    "                        if num_uploaded > 0:\n",
    "                            ticker_chunks_count += num_uploaded; total_chunks_indexed += num_uploaded; num_filings_processed += 1\n",
    "                        print(f\"  → Cleaning up memory...\"); del html_content; del parsed_data; del chunks_generator; gc.collect(); print(f\"  ✓ Memory cleaned.\")\n",
    "                    except Exception as e: print(f\"  ✗ Error processing filing for {ticker}: {e}\"); gc.collect()\n",
    "                if ticker_chunks_count > 0: successful_tickers.append(ticker); print(f\"  ✓ Finished {ticker}. Chunks: {ticker_chunks_count}\")\n",
    "                else: failed_tickers.append(ticker); print(f\"  ⚠ No chunks created for {ticker}\")\n",
    "            except Exception as e: print(f\"  ✗ Error processing {ticker}: {e}\"); failed_tickers.append(ticker)\n",
    "            if idx < len(tickers): time.sleep(0.2)\n",
    "        print(f\"\\n{'=' * 70}\\nINDEXING COMPLETE\\n{'=' * 70}\")\n",
    "        print(f\"✓ Success: {len(successful_tickers)} companies ({', '.join(successful_tickers)})\")\n",
    "        if failed_tickers: print(f\"✗ Failed: {len(failed_tickers)} companies ({', '.join(failed_tickers)})\")\n",
    "        print(f\"\\n Total chunks indexed: {total_chunks_indexed}\\n{'=' * 70}\\n\")\n",
    "\n",
    "    def query(self, question: str, ticker_filter: str = None):\n",
    "        if self.query_engine is None:\n",
    "            self.query_engine = FullAdvancedRAGEngine(\n",
    "                document_processor=self.processor,\n",
    "                qdrant_manager=self.qdrant_manager,\n",
    "                query_processor=self.query_processor\n",
    "            )\n",
    "        result = self.query_engine.query(question, ticker_filter)\n",
    "        print(f\"\\n{'=' * 70}\\nANSWER\\n{'=' * 70}\\n\\n{result['answer']}\\n\")\n",
    "        print(f\"{'=' * 70}\\nSOURCES ({len(result['sources'])})\\n{'=' * 70}\")\n",
    "        for i, source in enumerate(result['sources'], 1):\n",
    "            print(f\"\\n{i}. {source['company']} ({source['ticker']}) - {source['item']}\")\n",
    "            print(f\"   Filing Date: {source['filing_date']}\")\n",
    "            print(f\"   Is Table: {source.get('is_table')}\")\n",
    "            print(f\"   Re-Rank Score: {source.get('rerank_score', 'N/A'):.4f}\")\n",
    "            print(f\"   Vector Score: {source.get('retrieval_score', 'N/A'):.4f}\")\n",
    "        print(f\"\\n{'=' * 70}\\n\")\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7b985cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SEC 10-Q RAG SYSTEM (Model 5: Full Advanced)\n",
      "======================================================================\n",
      "\n",
      " Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "   ✓ Model loaded (dimension: 384)\n",
      "   ✓ Initialized RecursiveCharacterTextSplitter\n",
      "\n",
      "Connecting to Qdrant Cloud...\n",
      "   ✓ Connected to Qdrant\n",
      "✓ QueryProcessor initialized\n",
      "Checking if indexing is needed for the 'v_full' collection...\n",
      "\n",
      "======================================================================\n",
      "LOADING & INDEXING PHASE\n",
      "======================================================================\n",
      "\n",
      "Processing 10 companies: NVDA, AAPL, MSFT, AMZN, META, GOOGL, TSLA, ORCL, JPM, AMD\n",
      "(Fetching 4 filings per company)\n",
      "\n",
      "\n",
      " Setting up collection: sec_filings_10q_v_full\n",
      "   ✓ Collection created\n",
      "   → Creating payload index for 'ticker' (Keyword)...\n",
      "   → Creating payload index for 'item' (Keyword)...\n",
      "   → Creating payload index for 'is_table' (Boolean)...\n",
      "   ✓ Payload indexes created.\n",
      "\n",
      "[1/10] Processing NVDA\n",
      "----------------------------------------------------------------------\n",
      "  → Fetching CIK for ticker: NVDA...\n",
      "  → Found CIK: 0001045810 (NVIDIA CORP)\n",
      "  → Found 4 recent 10-Q filing metadata entries.\n",
      "  → Downloading filing from: 2025-08-27...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
      "     → Generated 245 chunks\n",
      "  ✓ All chunks uploaded. Total: 245\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  → Downloading filing from: 2025-05-28...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
      "     → Generated 245 chunks\n",
      "  ✓ All chunks uploaded. Total: 245\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  → Downloading filing from: 2024-11-20...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
      "     → Generated 257 chunks\n",
      "  ✓ All chunks uploaded. Total: 257\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  → Downloading filing from: 2024-08-28...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
      "     → Generated 271 chunks\n",
      "  ✓ All chunks uploaded. Total: 271\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  ✓ Finished NVDA. Chunks: 1018\n",
      "\n",
      "[2/10] Processing AAPL\n",
      "----------------------------------------------------------------------\n",
      "  → Fetching CIK for ticker: AAPL...\n",
      "  → Found CIK: 0000320193 (Apple Inc.)\n",
      "  → Found 4 recent 10-Q filing metadata entries.\n",
      "  → Downloading filing from: 2025-08-01...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 11 high-level 'Items' into smaller chunks...\n",
      "     → Generated 116 chunks\n",
      "  ✓ All chunks uploaded. Total: 116\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  → Downloading filing from: 2025-05-02...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 11 high-level 'Items' into smaller chunks...\n",
      "     → Generated 136 chunks\n",
      "  ✓ All chunks uploaded. Total: 136\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  → Downloading filing from: 2025-01-31...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 11 high-level 'Items' into smaller chunks...\n",
      "     → Generated 123 chunks\n",
      "  ✓ All chunks uploaded. Total: 123\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  → Downloading filing from: 2024-08-02...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 11 high-level 'Items' into smaller chunks...\n",
      "     → Generated 111 chunks\n",
      "  ✓ All chunks uploaded. Total: 111\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  ✓ Finished AAPL. Chunks: 486\n",
      "\n",
      "[3/10] Processing MSFT\n",
      "----------------------------------------------------------------------\n",
      "  → Fetching CIK for ticker: MSFT...\n",
      "  → Found CIK: 0000789019 (MICROSOFT CORP)\n",
      "  → Found 4 recent 10-Q filing metadata entries.\n",
      "  → Downloading filing from: 2025-10-29...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
      "     → Generated 31 chunks\n",
      "  ✓ All chunks uploaded. Total: 31\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  → Downloading filing from: 2025-04-30...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
      "     → Generated 42 chunks\n",
      "  ✓ All chunks uploaded. Total: 42\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  → Downloading filing from: 2025-01-29...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
      "     → Generated 36 chunks\n",
      "  ✓ All chunks uploaded. Total: 36\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  → Downloading filing from: 2024-10-30...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
      "     → Generated 37 chunks\n",
      "  ✓ All chunks uploaded. Total: 37\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  ✓ Finished MSFT. Chunks: 146\n",
      "\n",
      "[4/10] Processing AMZN\n",
      "----------------------------------------------------------------------\n",
      "  → Fetching CIK for ticker: AMZN...\n",
      "  → Found CIK: 0001018724 (AMAZON COM INC)\n",
      "  → Found 4 recent 10-Q filing metadata entries.\n",
      "  → Downloading filing from: 2025-10-31...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 11 high-level 'Items' into smaller chunks...\n",
      "     → Generated 352 chunks\n",
      "  ✓ All chunks uploaded. Total: 352\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  → Downloading filing from: 2025-08-01...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 11 high-level 'Items' into smaller chunks...\n",
      "     → Generated 341 chunks\n",
      "  ✓ All chunks uploaded. Total: 341\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  → Downloading filing from: 2025-05-02...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 11 high-level 'Items' into smaller chunks...\n",
      "     → Generated 324 chunks\n",
      "  ✓ All chunks uploaded. Total: 324\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  → Downloading filing from: 2024-11-01...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 11 high-level 'Items' into smaller chunks...\n",
      "     → Generated 357 chunks\n",
      "  ✓ All chunks uploaded. Total: 357\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  ✓ Finished AMZN. Chunks: 1374\n",
      "\n",
      "[5/10] Processing META\n",
      "----------------------------------------------------------------------\n",
      "  → Fetching CIK for ticker: META...\n",
      "  → Found CIK: 0001326801 (Meta Platforms, Inc.)\n",
      "  → Found 4 recent 10-Q filing metadata entries.\n",
      "  → Downloading filing from: 2025-10-30...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
      "     → Generated 704 chunks\n",
      "  ✓ All chunks uploaded. Total: 704\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  → Downloading filing from: 2025-07-31...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
      "     → Generated 675 chunks\n",
      "  ✓ All chunks uploaded. Total: 675\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  → Downloading filing from: 2025-05-01...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
      "     → Generated 646 chunks\n",
      "  ✓ All chunks uploaded. Total: 646\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  → Downloading filing from: 2024-10-31...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
      "     → Generated 663 chunks\n",
      "  ✓ All chunks uploaded. Total: 663\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  ✓ Finished META. Chunks: 2688\n",
      "\n",
      "[6/10] Processing GOOGL\n",
      "----------------------------------------------------------------------\n",
      "  → Fetching CIK for ticker: GOOGL...\n",
      "  → Found CIK: 0001652044 (Alphabet Inc.)\n",
      "  → Found 4 recent 10-Q filing metadata entries.\n",
      "  → Downloading filing from: 2025-10-30...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
      "     → Generated 277 chunks\n",
      "  ✓ All chunks uploaded. Total: 277\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  → Downloading filing from: 2025-07-24...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
      "     → Generated 274 chunks\n",
      "  ✓ All chunks uploaded. Total: 274\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  → Downloading filing from: 2025-04-25...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
      "     → Generated 244 chunks\n",
      "  ✓ All chunks uploaded. Total: 244\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  → Downloading filing from: 2024-10-30...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
      "     → Generated 274 chunks\n",
      "  ✓ All chunks uploaded. Total: 274\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  ✓ Finished GOOGL. Chunks: 1069\n",
      "\n",
      "[7/10] Processing TSLA\n",
      "----------------------------------------------------------------------\n",
      "  → Fetching CIK for ticker: TSLA...\n",
      "  → Found CIK: 0001318605 (Tesla, Inc.)\n",
      "  → Found 4 recent 10-Q filing metadata entries.\n",
      "  → Downloading filing from: 2025-10-23...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 11 high-level 'Items' into smaller chunks...\n",
      "     → Generated 247 chunks\n",
      "  ✓ All chunks uploaded. Total: 247\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  → Downloading filing from: 2025-07-24...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 11 high-level 'Items' into smaller chunks...\n",
      "     → Generated 226 chunks\n",
      "  ✓ All chunks uploaded. Total: 226\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  → Downloading filing from: 2025-04-23...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 11 high-level 'Items' into smaller chunks...\n",
      "     → Generated 210 chunks\n",
      "  ✓ All chunks uploaded. Total: 210\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  → Downloading filing from: 2024-10-24...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 11 high-level 'Items' into smaller chunks...\n",
      "     → Generated 225 chunks\n",
      "  ✓ All chunks uploaded. Total: 225\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  ✓ Finished TSLA. Chunks: 908\n",
      "\n",
      "[8/10] Processing ORCL\n",
      "----------------------------------------------------------------------\n",
      "  → Fetching CIK for ticker: ORCL...\n",
      "  → Found CIK: 0001341439 (ORACLE CORP)\n",
      "  → Found 4 recent 10-Q filing metadata entries.\n",
      "  → Downloading filing from: 2025-09-10...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
      "     → Generated 260 chunks\n",
      "  ✓ All chunks uploaded. Total: 260\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  → Downloading filing from: 2025-03-11...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
      "     → Generated 280 chunks\n",
      "  ✓ All chunks uploaded. Total: 280\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  → Downloading filing from: 2024-12-10...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
      "     → Generated 284 chunks\n",
      "  ✓ All chunks uploaded. Total: 284\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  → Downloading filing from: 2024-09-10...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 8 high-level 'Items' into smaller chunks...\n",
      "     → Generated 258 chunks\n",
      "  ✓ All chunks uploaded. Total: 258\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  ✓ Finished ORCL. Chunks: 1082\n",
      "\n",
      "[9/10] Processing JPM\n",
      "----------------------------------------------------------------------\n",
      "  → Fetching CIK for ticker: JPM...\n",
      "  → Found CIK: 0000019617 (JPMORGAN CHASE & CO)\n",
      "  → Found 3 recent 10-Q filing metadata entries.\n",
      "  → Downloading filing from: 2025-11-04...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 8 high-level 'Items' into smaller chunks...\n",
      "     → Generated 31 chunks\n",
      "  ✓ All chunks uploaded. Total: 31\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  → Downloading filing from: 2025-08-05...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 8 high-level 'Items' into smaller chunks...\n",
      "     → Generated 23 chunks\n",
      "  ✓ All chunks uploaded. Total: 23\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  → Downloading filing from: 2025-05-01...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 8 high-level 'Items' into smaller chunks...\n",
      "     → Generated 24 chunks\n",
      "  ✓ All chunks uploaded. Total: 24\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  ✓ Finished JPM. Chunks: 78\n",
      "\n",
      "[10/10] Processing AMD\n",
      "----------------------------------------------------------------------\n",
      "  → Fetching CIK for ticker: AMD...\n",
      "  → Found CIK: 0000002488 (ADVANCED MICRO DEVICES INC)\n",
      "  → Found 4 recent 10-Q filing metadata entries.\n",
      "  → Downloading filing from: 2025-11-05...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
      "     → Generated 430 chunks\n",
      "  ✓ All chunks uploaded. Total: 430\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  → Downloading filing from: 2025-08-06...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
      "     → Generated 422 chunks\n",
      "  ✓ All chunks uploaded. Total: 422\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  → Downloading filing from: 2025-05-07...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
      "     → Generated 390 chunks\n",
      "  ✓ All chunks uploaded. Total: 390\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  → Downloading filing from: 2024-10-30...\n",
      "  → Parsing 10-Q structure...\n",
      "  → Creating chunks and embeddings generator...\n",
      " Uploading chunks to Qdrant in batches of 2048...\n",
      "     → Splitting 9 high-level 'Items' into smaller chunks...\n",
      "     → Generated 392 chunks\n",
      "  ✓ All chunks uploaded. Total: 392\n",
      "  → Cleaning up memory...\n",
      "  ✓ Memory cleaned.\n",
      "  ✓ Finished AMD. Chunks: 1634\n",
      "\n",
      "======================================================================\n",
      "INDEXING COMPLETE\n",
      "======================================================================\n",
      "✓ Success: 10 companies (NVDA, AAPL, MSFT, AMZN, META, GOOGL, TSLA, ORCL, JPM, AMD)\n",
      "\n",
      " Total chunks indexed: 10483\n",
      "======================================================================\n",
      "\n",
      "Indexing check complete. Proceeding to queries.\n"
     ]
    }
   ],
   "source": [
    "# ===========================================================================\n",
    "# PART 8: USAGE EXAMPLE (Saving all results)\n",
    "# ===========================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    pipeline = SECFilingRAGPipeline()\n",
    "    \n",
    "    # NOTE: This uses a new collection 'sec_filings_10q_v_full'\n",
    "    # You MUST run this indexing step one time.\n",
    "    print(\"Checking if indexing is needed for the 'v_full' collection...\")\n",
    "    pipeline.load_and_index_filings(num_filings_per_ticker=4)\n",
    "    print(\"Indexing check complete. Proceeding to queries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fe67ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running 6 queries with (Model 5) Full Advanced RAG...\n",
      "\n",
      "--- Running Query: What are the main risk factors mentioned by each companies? ---\n",
      "\n",
      " Initializing Full Advanced RAG Engine (Model 5)...\n",
      "   ✓ Processor, QdrantManager, and QueryProcessor attached.\n",
      "   → Loading Cross-Encoder: cross-encoder/ms-marco-MiniLM-L-6-v2...\n",
      "   ✓ Cross-Encoder model loaded.\n",
      "   ✓ Initialized ChatOpenAI LLM\n",
      "   ✓ Full Advanced RAG Engine ready.\n",
      "\n",
      " Processing query with Full Advanced Engine: 'What are the main risk factors mentioned by each companies?'\n",
      "   → Step 1: Processing query (NER, Intent, Embedding)...\n",
      "   → Step 2: Generating filters...\n",
      "   → Routing for table data...\n",
      "   → Query routed to GENERAL text search.\n",
      "   → No filters applied. Searching all documents.\n",
      "   → Step 3: Searching Qdrant (Retrieving Top 20)...\n",
      "   → Retrieved 20 candidates.\n",
      "   → Step 4: Re-ranking candidates with Cross-Encoder...\n",
      "   → Re-ranked. Final 5 sources selected.\n",
      "   → Step 5: Formatting context and sending to LLM...\n",
      "\n",
      "======================================================================\n",
      "ANSWER\n",
      "======================================================================\n",
      "\n",
      "According to the provided context:\n",
      "\n",
      "For Apple (AAPL):\n",
      "- The main risk factors are not explicitly detailed in the provided context. However, it is mentioned that there have been no material changes to the Company's risk factors since the 2024 Form 10-K, as stated in \"Item 1A. Risk Factors\" from Source 1.\n",
      "\n",
      "For AMD:\n",
      "- The main risk factors include:\n",
      "  1. Intel Corporation’s dominance of the microprocessor market and its aggressive business practices, which may limit AMD's ability to compete effectively (Source 2).\n",
      "  2. The highly competitive and rapidly evolving markets in which AMD's products are sold (Source 4).\n",
      "  3. The cyclical nature of the semiconductor industry, which has experienced severe downturns (Source 4).\n",
      "\n",
      "These risk factors are summarized in \"ITEM 1A. RISK FACTORS\" from Sources 2, 3, 4, and 5.\n",
      "\n",
      "======================================================================\n",
      "SOURCES (5)\n",
      "======================================================================\n",
      "\n",
      "1. Apple Inc. (AAPL) - ITEM 1A\n",
      "   Filing Date: 2025-01-31\n",
      "   Is Table: False\n",
      "   Re-Rank Score: 4.0526\n",
      "   Vector Score: 0.6650\n",
      "\n",
      "2. ADVANCED MICRO DEVICES INC (AMD) - ITEM 1A\n",
      "   Filing Date: 2025-05-07\n",
      "   Is Table: False\n",
      "   Re-Rank Score: 3.5313\n",
      "   Vector Score: 0.6457\n",
      "\n",
      "3. ADVANCED MICRO DEVICES INC (AMD) - ITEM 1A\n",
      "   Filing Date: 2024-10-30\n",
      "   Is Table: False\n",
      "   Re-Rank Score: 3.5313\n",
      "   Vector Score: 0.6457\n",
      "\n",
      "4. ADVANCED MICRO DEVICES INC (AMD) - ITEM 1A\n",
      "   Filing Date: 2025-08-06\n",
      "   Is Table: False\n",
      "   Re-Rank Score: 2.6218\n",
      "   Vector Score: 0.6942\n",
      "\n",
      "5. ADVANCED MICRO DEVICES INC (AMD) - ITEM 1A\n",
      "   Filing Date: 2025-11-05\n",
      "   Is Table: False\n",
      "   Re-Rank Score: 2.6218\n",
      "   Vector Score: 0.6942\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "--- Running Query: What risks did Apple disclose in their latest 10-Q? ---\n",
      "\n",
      " Processing query with Full Advanced Engine: 'What risks did Apple disclose in their latest 10-Q?'\n",
      "   → Step 1: Processing query (NER, Intent, Embedding)...\n",
      "   → Step 2: Generating filters...\n",
      "   → NER auto-detected filter: {'ticker': ['AAPL']}\n",
      "   → Routing for table data...\n",
      "   → Query routed to GENERAL text search.\n",
      "   → Step 3: Searching Qdrant (Retrieving Top 20)...\n",
      "   → Retrieved 20 candidates.\n",
      "   → Step 4: Re-ranking candidates with Cross-Encoder...\n",
      "   → Re-ranked. Final 5 sources selected.\n",
      "   → Step 5: Formatting context and sending to LLM...\n",
      "\n",
      "======================================================================\n",
      "ANSWER\n",
      "======================================================================\n",
      "\n",
      "The specific risks disclosed by Apple in their latest 10-Q are not detailed in the provided context. However, it is mentioned that there have been no material changes to the Company’s risk factors since the 2023 Form 10-K (Source 1). Additionally, it is noted that various factors can impact the Company's future gross margins, which are discussed under the heading “Risk Factors” in Part I, Item 1A of the 2024 Form 10-K and Part II, Item 1A of the current Form 10-Q (Source 2). Furthermore, certain events can disrupt Apple's operations, such as manufacturing and delivery challenges, supply chain inefficiencies, service outages, increased costs, and negative impacts on consumer spending and demand (Source 5).\n",
      "\n",
      "======================================================================\n",
      "SOURCES (5)\n",
      "======================================================================\n",
      "\n",
      "1. Apple Inc. (AAPL) - ITEM 1A\n",
      "   Filing Date: 2024-08-02\n",
      "   Is Table: False\n",
      "   Re-Rank Score: 3.0329\n",
      "   Vector Score: 0.7313\n",
      "\n",
      "2. Apple Inc. (AAPL) - ITEM 2\n",
      "   Filing Date: 2025-05-02\n",
      "   Is Table: False\n",
      "   Re-Rank Score: 2.4537\n",
      "   Vector Score: 0.6190\n",
      "\n",
      "3. Apple Inc. (AAPL) - ITEM 2\n",
      "   Filing Date: 2025-05-02\n",
      "   Is Table: False\n",
      "   Re-Rank Score: 2.0474\n",
      "   Vector Score: 0.5701\n",
      "\n",
      "4. Apple Inc. (AAPL) - ITEM 2\n",
      "   Filing Date: 2024-08-02\n",
      "   Is Table: False\n",
      "   Re-Rank Score: 2.0157\n",
      "   Vector Score: 0.5700\n",
      "\n",
      "5. Apple Inc. (AAPL) - ITEM 1A\n",
      "   Filing Date: 2025-05-02\n",
      "   Is Table: False\n",
      "   Re-Rank Score: 0.8328\n",
      "   Vector Score: 0.6020\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "--- Running Query: Compare the revenue trends of NVIDIA and AMD ---\n",
      "\n",
      " Processing query with Full Advanced Engine: 'Compare the revenue trends of NVIDIA and AMD'\n",
      "   → Step 1: Processing query (NER, Intent, Embedding)...\n",
      "   → Step 2: Generating filters...\n",
      "   → NER auto-detected filter: {'ticker': ['AMD', 'NVDA']}\n",
      "   → Routing for table data...\n",
      "   → Query routed to GENERAL text search.\n",
      "   → Step 3: Searching Qdrant (Retrieving Top 20)...\n",
      "   → Retrieved 20 candidates.\n",
      "   → Step 4: Re-ranking candidates with Cross-Encoder...\n",
      "   → Re-ranked. Final 5 sources selected.\n",
      "   → Step 5: Formatting context and sending to LLM...\n",
      "\n",
      "======================================================================\n",
      "ANSWER\n",
      "======================================================================\n",
      "\n",
      "Based on the provided context, we can observe the following revenue trends for NVIDIA and AMD:\n",
      "\n",
      "**NVIDIA:**\n",
      "- For the second quarter and first half of fiscal year 2026, NVIDIA's revenue from sales to customers outside of the United States accounted for 53% of total revenue, compared to 48% for the first quarter of fiscal year 2025 (Source 2).\n",
      "- Sales to one direct customer, Customer A, represented 16% of total revenue, indicating a significant concentration of revenue from a single customer (Source 2).\n",
      "- In the third quarter of fiscal year 2024, sales to one direct customer represented 12% of total revenue, and sales to a second direct customer represented 11% of total revenue for the first nine months of fiscal year 2024, both attributable to the Compute & Networking segment (Source 5).\n",
      "\n",
      "**AMD:**\n",
      "- For the three months ended September 27, 2025, AMD reported net revenue of $9.2 billion, a 36% increase compared to the prior year period. This increase was driven by strong demand in the Client and Gaming segment, particularly for AMD Ryzen™ processors, semi-custom game console SoCs, and Radeon™ gaming GPUs, as well as an increase in the Data Center segment due to demand for AMD EPYC™ processors and AMD Instinct™ MI350 Series GPUs (Source 3).\n",
      "- AMD's gross margin for the same period was 52%, up from 50% in the prior year period, primarily due to product mix (Source 3).\n",
      "\n",
      "In summary, both NVIDIA and AMD experienced growth in their respective revenue streams, with NVIDIA showing a significant portion of its revenue coming from international sales and specific large customers, while AMD demonstrated a substantial year-over-year increase in net revenue driven by strong demand across multiple product segments.\n",
      "\n",
      "======================================================================\n",
      "SOURCES (5)\n",
      "======================================================================\n",
      "\n",
      "1. NVIDIA CORP (NVDA) - ITEM 1\n",
      "   Filing Date: 2025-08-27\n",
      "   Is Table: False\n",
      "   Re-Rank Score: 0.8773\n",
      "   Vector Score: 0.5922\n",
      "\n",
      "2. NVIDIA CORP (NVDA) - ITEM 1\n",
      "   Filing Date: 2025-05-28\n",
      "   Is Table: False\n",
      "   Re-Rank Score: 0.6547\n",
      "   Vector Score: 0.5925\n",
      "\n",
      "3. ADVANCED MICRO DEVICES INC (AMD) - ITEM 2\n",
      "   Filing Date: 2025-11-05\n",
      "   Is Table: True\n",
      "   Re-Rank Score: 0.6101\n",
      "   Vector Score: 0.6258\n",
      "\n",
      "4. NVIDIA CORP (NVDA) - ITEM 1\n",
      "   Filing Date: 2024-08-28\n",
      "   Is Table: False\n",
      "   Re-Rank Score: 0.0571\n",
      "   Vector Score: 0.6392\n",
      "\n",
      "5. NVIDIA CORP (NVDA) - ITEM 1\n",
      "   Filing Date: 2024-11-20\n",
      "   Is Table: False\n",
      "   Re-Rank Score: 0.0267\n",
      "   Vector Score: 0.6546\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "--- Running Query: What was Tesla's R&D spending in the latest quarter? ---\n",
      "\n",
      " Processing query with Full Advanced Engine: 'What was Tesla's R&D spending in the latest quarter?'\n",
      "   → Step 1: Processing query (NER, Intent, Embedding)...\n",
      "   → Step 2: Generating filters...\n",
      "   → NER auto-detected filter: {'ticker': ['TSLA']}\n",
      "   → Routing for table data...\n",
      "   → Query routed to TABLE search.\n",
      "   → Step 3: Searching Qdrant (Retrieving Top 20)...\n",
      "\n",
      "======================================================================\n",
      "ANSWER\n",
      "======================================================================\n",
      "\n",
      "No relevant context found.\n",
      "\n",
      "======================================================================\n",
      "SOURCES (0)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "--- Running Query: How has Microsoft's operating income changed over the last year? ---\n",
      "\n",
      " Processing query with Full Advanced Engine: 'How has Microsoft's operating income changed over the last year?'\n",
      "   → Step 1: Processing query (NER, Intent, Embedding)...\n",
      "   → Step 2: Generating filters...\n",
      "   → NER auto-detected filter: {'ticker': ['MSFT']}\n",
      "   → Routing for table data...\n",
      "   → Query routed to GENERAL text search.\n",
      "   → Step 3: Searching Qdrant (Retrieving Top 20)...\n",
      "   → Retrieved 20 candidates.\n",
      "   → Step 4: Re-ranking candidates with Cross-Encoder...\n",
      "   → Re-ranked. Final 5 sources selected.\n",
      "   → Step 5: Formatting context and sending to LLM...\n",
      "\n",
      "======================================================================\n",
      "ANSWER\n",
      "======================================================================\n",
      "\n",
      "The provided context does not include specific information about changes in Microsoft's operating income over the last year. The context only contains reviews of interim financial information, confirming that no material modifications are needed for conformity with accounting principles, but it does not provide specific financial figures or changes in operating income.\n",
      "\n",
      "======================================================================\n",
      "SOURCES (5)\n",
      "======================================================================\n",
      "\n",
      "1. MICROSOFT CORP (MSFT) - ITEM 1\n",
      "   Filing Date: 2025-04-30\n",
      "   Is Table: False\n",
      "   Re-Rank Score: -2.2425\n",
      "   Vector Score: 0.4947\n",
      "\n",
      "2. MICROSOFT CORP (MSFT) - ITEM 1\n",
      "   Filing Date: 2025-01-29\n",
      "   Is Table: False\n",
      "   Re-Rank Score: -2.4004\n",
      "   Vector Score: 0.4972\n",
      "\n",
      "3. MICROSOFT CORP (MSFT) - ITEM 1\n",
      "   Filing Date: 2025-10-29\n",
      "   Is Table: False\n",
      "   Re-Rank Score: -3.8218\n",
      "   Vector Score: 0.4465\n",
      "\n",
      "4. MICROSOFT CORP (MSFT) - ITEM 1\n",
      "   Filing Date: 2024-10-30\n",
      "   Is Table: False\n",
      "   Re-Rank Score: -3.8560\n",
      "   Vector Score: 0.4469\n",
      "\n",
      "5. MICROSOFT CORP (MSFT) - ITEM 1\n",
      "   Filing Date: 2025-04-30\n",
      "   Is Table: False\n",
      "   Re-Rank Score: -8.6739\n",
      "   Vector Score: 0.3949\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "--- Running Query: What was the gross profit margin for all companies? ---\n",
      "\n",
      " Processing query with Full Advanced Engine: 'What was the gross profit margin for all companies?'\n",
      "   → Step 1: Processing query (NER, Intent, Embedding)...\n",
      "   → Step 2: Generating filters...\n",
      "   → Routing for table data...\n",
      "   → Query routed to TABLE search.\n",
      "   → Step 3: Searching Qdrant (Retrieving Top 20)...\n",
      "   → Retrieved 20 candidates.\n",
      "   → Step 4: Re-ranking candidates with Cross-Encoder...\n",
      "   → Re-ranked. Final 5 sources selected.\n",
      "   → Step 5: Formatting context and sending to LLM...\n",
      "\n",
      "======================================================================\n",
      "ANSWER\n",
      "======================================================================\n",
      "\n",
      "The context provided only includes information about AMD's gross profit margin. According to the data:\n",
      "\n",
      "- For the three months ended September 27, 2025, AMD's gross margin was 52%.\n",
      "- For the three months ended September 28, 2024, AMD's gross margin was 50%.\n",
      "- For the three months ended March 29, 2025, AMD's gross margin was 50%.\n",
      "\n",
      "There is no information provided about the gross profit margin for any other companies.\n",
      "\n",
      "======================================================================\n",
      "SOURCES (5)\n",
      "======================================================================\n",
      "\n",
      "1. ADVANCED MICRO DEVICES INC (AMD) - ITEM 2\n",
      "   Filing Date: 2024-10-30\n",
      "   Is Table: True\n",
      "   Re-Rank Score: -0.1454\n",
      "   Vector Score: 0.5921\n",
      "\n",
      "2. ADVANCED MICRO DEVICES INC (AMD) - ITEM 2\n",
      "   Filing Date: 2025-11-05\n",
      "   Is Table: True\n",
      "   Re-Rank Score: -0.1522\n",
      "   Vector Score: 0.5399\n",
      "\n",
      "3. ADVANCED MICRO DEVICES INC (AMD) - ITEM 2\n",
      "   Filing Date: 2025-11-05\n",
      "   Is Table: True\n",
      "   Re-Rank Score: -1.5825\n",
      "   Vector Score: 0.5121\n",
      "\n",
      "4. ADVANCED MICRO DEVICES INC (AMD) - ITEM 2\n",
      "   Filing Date: 2024-10-30\n",
      "   Is Table: True\n",
      "   Re-Rank Score: -1.7765\n",
      "   Vector Score: 0.5701\n",
      "\n",
      "5. ADVANCED MICRO DEVICES INC (AMD) - ITEM 2\n",
      "   Filing Date: 2025-05-07\n",
      "   Is Table: True\n",
      "   Re-Rank Score: -2.3971\n",
      "   Vector Score: 0.5564\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "--- All queries complete. Saving all results to: data\\RAG_full_pipeline_results.json ---\n",
      "✓ Successfully saved all answers.\n",
      "\n",
      "Pipeline run finished.\n"
     ]
    }
   ],
   "source": [
    "DATA_FOLDER = \"data\"\n",
    "if not os.path.exists(DATA_FOLDER):\n",
    "    os.makedirs(DATA_FOLDER); print(f\"Created data folder: {DATA_FOLDER}\")\n",
    "\n",
    "queries_to_run = [\n",
    "    {\"question\": \"What are the main risk factors mentioned by each companies?\", \"ticker_filter\": None},\n",
    "    {\"question\": \"What risks did Apple disclose in their latest 10-Q?\", \"ticker_filter\": None},\n",
    "    {\"question\": \"Compare the revenue trends of NVIDIA and AMD\", \"ticker_filter\": None},\n",
    "    {\"question\": \"What was Tesla's R&D spending in the latest quarter?\", \"ticker_filter\": None},\n",
    "    {\"question\": \"How has Microsoft's operating income changed over the last year?\", \"ticker_filter\": None},\n",
    "    {\"question\": \"What was the gross profit margin for all companies?\", \"ticker_filter\": None},\n",
    "]\n",
    "\n",
    "print(f\"\\nRunning {len(queries_to_run)} queries with (Model 5) Full Advanced RAG...\")\n",
    "all_results = []\n",
    "for query in queries_to_run:\n",
    "    print(f\"\\n--- Running Query: {query['question']} ---\")\n",
    "    result = pipeline.query(question=query[\"question\"], ticker_filter=query[\"ticker_filter\"])\n",
    "    all_results.append({\"question\": query[\"question\"], \"ticker_filter_manual\": query[\"ticker_filter\"], \"response\": result})\n",
    "\n",
    "save_path = os.path.join(DATA_FOLDER, \"RAG_full_pipeline_results.json\")\n",
    "print(f\"\\n--- All queries complete. Saving all results to: {save_path} ---\")\n",
    "try:\n",
    "    with open(save_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_results, f, indent=4)\n",
    "    print(f\"✓ Successfully saved all answers.\")\n",
    "except Exception as e: print(f\"✗ Failed to save results: {e}\")\n",
    "print(\"\\nPipeline run finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSA4213-SEC-Fillings-Chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
