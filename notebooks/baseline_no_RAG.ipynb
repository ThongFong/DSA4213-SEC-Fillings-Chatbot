{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fb4308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and API Key Setup\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Load API keys from your .env file\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "LLM_MODEL = \"gpt-4o\" # Use the same model as your RAG pipeline for a fair comparison\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    print(\"Error: OPENAI_API_KEY not found. Please set it in your .env file.\")\n",
    "else:\n",
    "    print(f\"OpenAI API Key loaded. Using model: {LLM_MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the \"Benchmark\" Chain (No RAG)\n",
    "\n",
    "# 1. Initialize the LLM (same as your RAG pipeline)\n",
    "llm = ChatOpenAI(\n",
    "    model=LLM_MODEL,\n",
    "    api_key=OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "# 2. Define the Prompt Template\n",
    "# This prompt asks the LLM to answer from its own knowledge,\n",
    "# unlike the RAG prompt which FORCED it to use provided context.\n",
    "prompt_template_str = \"\"\"You are a helpful financial analyst assistant.\n",
    "\n",
    "Answer the following question regarding 10q reports in SEC Fillings to the best of your ability based on your general, pre-trained knowledge.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(prompt_template_str)\n",
    "\n",
    "# 3. Define a simple string output parser\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# 4. Create the \"benchmark\" chain\n",
    "# This is the \"base benchmark\" chain: Prompt -> LLM -> Output\n",
    "benchmark_chain = prompt | llm | output_parser\n",
    "\n",
    "print(\"Benchmark (non-RAG) chain created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Queries and Get Answers\n",
    "# TO BE CHANGED AFTER EVALUATION QA ARE GENERATED\n",
    "# Use the same questions from your 'base_RAG.ipynb' for a fair comparison\n",
    "questions = [\n",
    "    \"What are the main risk factors mentioned by tech companies?\",\n",
    "    \"What risks did Apple disclose in their latest 10-Q?\",\n",
    "    \"Compare the revenue trends of NVIDIA and AMD\",\n",
    "    \"What was Tesla's R&D spending in the latest quarter?\"\n",
    "]\n",
    "\n",
    "print(f\"--- STARTING BENCHMARK QUERIES (NO RAG) ---\")\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"QUERY: {q}\")\n",
    "    print(f\"{'-'*70}\")\n",
    "\n",
    "    # Invoke the chain\n",
    "    answer = benchmark_chain.invoke({\"question\": q})\n",
    "\n",
    "    print(f\"ANSWER (from gpt-4o's internal knowledge):\\n{answer}\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "print(\"\\n--- BENCHMARK QUERIES COMPLETE ---\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
