{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "3d0b67a3",
      "metadata": {
        "id": "3d0b67a3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\nfrom google.colab import userdata\\n\\n# Load secrets from Colab and set them as environment variables\\nos.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\\nos.environ['QDRANT_URL'] = userdata.get('QDRANT_URL')\\nos.environ['QDRANT_API_KEY'] = userdata.get('QDRANT_API_KEY')\\n\""
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "SEC 10-Q Filing RAG System\n",
        "A complete pipeline for loading, processing, embedding, and querying SEC 10-Q filings\n",
        "using Qdrant vector database and OpenAI GPT-4o.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import re\n",
        "import uuid\n",
        "import time\n",
        "import requests\n",
        "import gc\n",
        "import pandas as pd\n",
        "from transformers import pipeline as transformers_pipeline\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from typing import List, Dict, Tuple\n",
        "from collections import defaultdict\n",
        "from bs4 import BeautifulSoup, NavigableString\n",
        "\n",
        "# External libraries (install via pip)\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
        "from qdrant_client import QdrantClient, models\n",
        "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
        "from openai import OpenAI\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "'''\n",
        "from google.colab import userdata\n",
        "\n",
        "# Load secrets from Colab and set them as environment variables\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "os.environ['QDRANT_URL'] = userdata.get('QDRANT_URL')\n",
        "os.environ['QDRANT_API_KEY'] = userdata.get('QDRANT_API_KEY')\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "f3255ddf",
      "metadata": {
        "id": "f3255ddf"
      },
      "outputs": [],
      "source": [
        "# ===========================================================================\n",
        "# PART 1: CONFIGURATION & SETUP\n",
        "# ===========================================================================\n",
        "\n",
        "class Config:\n",
        "    \"\"\"Configuration class for API keys and model settings\"\"\"\n",
        "\n",
        "    # API Keys - SET THESE BEFORE RUNNING\n",
        "    QDRANT_URL = os.getenv(\"QDRANT_URL\")\n",
        "    QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")\n",
        "    OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "    # SEC EDGAR Configuration\n",
        "    SEC_HEADERS = {'User-Agent': 'SEC10Q-RAG-System research@example.com'}\n",
        "    CIK_MAP_URL = 'https://www.sec.gov/files/company_tickers.json'\n",
        "\n",
        "    # Model Configuration\n",
        "    EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"  # 384 dimensions\n",
        "    CROSS_ENCODER_MODEL = \"cross-encoder/ms-marco-MiniLM-L-6-v2\" # For re-ranking\n",
        "    LLM_MODEL = \"gpt-4o\"\n",
        "\n",
        "    # Collection Configuration\n",
        "    COLLECTION_NAME = \"sec_filings_10q_GOLDEN_BENCHMARK\"\n",
        "    VECTOR_SIZE = 384  # Dimension for all-MiniLM-L6-v2\n",
        "\n",
        "    # Chunking Configuration\n",
        "    CHUNK_SIZE = 800  # Characters per chunk\n",
        "    CHUNK_OVERLAP = 200  # Overlap between chunks\n",
        "\n",
        "    # Retrieve more candidates (K=20) for the re-ranker\n",
        "    RETRIEVAL_TOP_K = 20\n",
        "    # The final number of chunks (K=5) to send to the LLM\n",
        "    FINAL_TOP_K = 5\n",
        "\n",
        "    # Company Tickers\n",
        "    TICKERS = ['NVDA', 'AAPL', 'MSFT', 'AMZN', 'META', 'GOOGL', 'TSLA', 'ORCL', 'JPM', 'AMD']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "bfb2dfc0",
      "metadata": {
        "id": "bfb2dfc0"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PART 2: DOCUMENT LOADING\n",
        "# ============================================================================\n",
        "\n",
        "class SECDocumentLoader:\n",
        "    \"\"\"Handles fetching and parsing of SEC 10-Q filings\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def get_recent_10q_metadata(ticker: str, num_filings: int = 4) -> List[Dict[str, str]]:\n",
        "        \"\"\"\n",
        "        Fetches the metadata (links, dates, etc.) for the latest N 10-Q filings.\n",
        "        Does NOT fetch the actual HTML content.\n",
        "\n",
        "        Args:\n",
        "            ticker: The company ticker (e.g., 'AAPL')\n",
        "            num_filings: The number of recent 10-Q filings to fetch\n",
        "\n",
        "        Returns:\n",
        "            List of metadata dictionaries\n",
        "        \"\"\"\n",
        "        print(f\"  → Fetching CIK for ticker: {ticker}...\")\n",
        "\n",
        "        # Get CIK mapping\n",
        "        response = requests.get(Config.CIK_MAP_URL, headers=Config.SEC_HEADERS)\n",
        "        response.raise_for_status()\n",
        "        company_data = response.json()\n",
        "\n",
        "        # Find CIK\n",
        "        cik = None\n",
        "        company_name = None\n",
        "        for company in company_data.values():\n",
        "            if company['ticker'] == ticker.upper():\n",
        "                cik = str(company['cik_str']).zfill(10)\n",
        "                company_name = company['title']\n",
        "                break\n",
        "\n",
        "        if not cik:\n",
        "            raise ValueError(f\"Ticker '{ticker}' not found in SEC CIK mapping.\")\n",
        "\n",
        "        print(f\"  → Found CIK: {cik} ({company_name})\")\n",
        "\n",
        "        # Fetch submission history\n",
        "        submissions_url = f\"https://data.sec.gov/submissions/CIK{cik}.json\"\n",
        "        time.sleep(0.1)  # Rate limiting\n",
        "        response = requests.get(submissions_url, headers=Config.SEC_HEADERS)\n",
        "        response.raise_for_status()\n",
        "        submissions = response.json()\n",
        "\n",
        "        # Find latest N 10-Q filings metadata\n",
        "        filings_metadata = []\n",
        "        for i, form in enumerate(submissions['filings']['recent']['form']):\n",
        "            if form == '10-Q':\n",
        "                accession_number = submissions['filings']['recent']['accessionNumber'][i]\n",
        "                primary_document = submissions['filings']['recent']['primaryDocument'][i]\n",
        "                filing_date = submissions['filings']['recent']['filingDate'][i]\n",
        "\n",
        "                accession_number_clean = accession_number.replace('-', '')\n",
        "\n",
        "                # Construct the filing URL\n",
        "                filing_url = (\n",
        "                    f\"https://www.sec.gov/Archives/edgar/data/{cik}/\"\n",
        "                    f\"{accession_number_clean}/{primary_document}\"\n",
        "                )\n",
        "\n",
        "                metadata = {\n",
        "                    'ticker': ticker.upper(),\n",
        "                    'company_name': company_name,\n",
        "                    'filing_date': filing_date,\n",
        "                    'cik': cik,\n",
        "                    'filing_url': filing_url\n",
        "                }\n",
        "                filings_metadata.append(metadata)\n",
        "\n",
        "                if len(filings_metadata) >= num_filings:\n",
        "                    break\n",
        "\n",
        "        if not filings_metadata:\n",
        "            raise ValueError(f\"No recent 10-Q filings found for ticker '{ticker}'.\")\n",
        "\n",
        "        print(f\"  → Found {len(filings_metadata)} recent 10-Q filing metadata entries.\")\n",
        "        return filings_metadata\n",
        "\n",
        "    @staticmethod\n",
        "    def get_filing_html(filing_url: str) -> str:\n",
        "        \"\"\"Fetches the HTML content for a single filing URL.\"\"\"\n",
        "        time.sleep(0.1)  # Rate limiting\n",
        "        response = requests.get(filing_url, headers=Config.SEC_HEADERS)\n",
        "        response.raise_for_status()\n",
        "        return response.text\n",
        "\n",
        "    @staticmethod\n",
        "    def _normalize_header_text(text: str) -> str:\n",
        "        \"\"\"Normalizes header text to standard format\"\"\"\n",
        "        text = text.strip().upper()\n",
        "\n",
        "        # Match \"PART I\" or \"PART II\"\n",
        "        part_match = re.search(r'^\\s*(PART\\s+I{1,2})', text)\n",
        "        if part_match:\n",
        "            return re.sub(r'\\s+', ' ', part_match.group(1))\n",
        "\n",
        "        # Match \"ITEM 1\", \"ITEM 1A\", etc.\n",
        "        item_match = re.search(r'^\\s*(ITEM\\s+\\d[A-Z]?)', text)\n",
        "        if item_match:\n",
        "            return re.sub(r'\\s+', ' ', item_match.group(1))\n",
        "\n",
        "        return None\n",
        "\n",
        "    @staticmethod\n",
        "    def _parse_html_table(table_tag) -> str:\n",
        "        \"\"\"\n",
        "        Converts HTML table to Markdown format with robust column handling\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Skip nested tables\n",
        "            if table_tag.find_parent('table'):\n",
        "                return \"\"\n",
        "            \n",
        "            markdown_rows = []\n",
        "            max_cols = 0\n",
        "            \n",
        "            # First pass: collect all rows and find max column count\n",
        "            for tr in table_tag.find_all('tr', recursive=False):\n",
        "                cells = []\n",
        "                for cell in tr.find_all(['td', 'th'], recursive=False):\n",
        "                    # Get text and clean it\n",
        "                    text = \" \".join(cell.get_text(strip=True).split())\n",
        "                    # Handle colspan\n",
        "                    colspan = int(cell.get('colspan', 1))\n",
        "                    cells.extend([text] + [''] * (colspan - 1))\n",
        "                \n",
        "                if cells and any(c for c in cells):  # Skip empty rows\n",
        "                    markdown_rows.append(cells)\n",
        "                    max_cols = max(max_cols, len(cells))\n",
        "            \n",
        "            if not markdown_rows or max_cols == 0:\n",
        "                return \"\"\n",
        "            \n",
        "            # Normalize all rows to have same column count\n",
        "            for row in markdown_rows:\n",
        "                while len(row) < max_cols:\n",
        "                    row.append(\"\")\n",
        "                if len(row) > max_cols:\n",
        "                    row[:] = row[:max_cols]\n",
        "            \n",
        "            # Build markdown output\n",
        "            md_output = []\n",
        "            \n",
        "            # Use first row as header (or create generic header)\n",
        "            header = markdown_rows[0] if markdown_rows else [f\"Col{i}\" for i in range(max_cols)]\n",
        "            header = [h if h else f\"Column_{i}\" for i, h in enumerate(header)]\n",
        "            \n",
        "            md_output.append(\"| \" + \" | \".join(header) + \" |\")\n",
        "            md_output.append(\"| \" + \" | \".join(['---'] * len(header)) + \" |\")\n",
        "            \n",
        "            # Add data rows\n",
        "            for row in markdown_rows[1:]:\n",
        "                # Ensure row length matches header\n",
        "                while len(row) < len(header):\n",
        "                    row.append(\"\")\n",
        "                row = row[:len(header)]\n",
        "                md_output.append(\"| \" + \" | \".join(row) + \" |\")\n",
        "            \n",
        "            result = \"\\n\" + \"\\n\".join(md_output) + \"\\n\"\n",
        "            \n",
        "            return result\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  [HTML Table Parser Error] Failed to parse HTML table: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    @classmethod\n",
        "    def parse_10q(cls, html_content: str) -> Dict:\n",
        "        \"\"\"Parses HTML content into structured dictionary\"\"\"\n",
        "        # Use 'lxml' for better memory efficiency\n",
        "        soup = BeautifulSoup(html_content, 'lxml')\n",
        "\n",
        "        potential_headers = soup.find_all(['p', 'b', 'strong', 'div'])\n",
        "\n",
        "        doc_headers = []\n",
        "        for header in potential_headers:\n",
        "            text = header.get_text(strip=True)\n",
        "            if len(text) > 100:\n",
        "                continue\n",
        "\n",
        "            normalized_key = cls._normalize_header_text(text)\n",
        "            if normalized_key:\n",
        "                if not header.find_parent('a'):\n",
        "                    doc_headers.append({'tag': header, 'key': normalized_key})\n",
        "\n",
        "        if not doc_headers:\n",
        "            return {}\n",
        "\n",
        "        parsed_data = defaultdict(lambda: defaultdict(str))\n",
        "        current_part_key = None\n",
        "\n",
        "        for i, header_info in enumerate(doc_headers):\n",
        "            current_key = header_info['key']\n",
        "\n",
        "            if 'PART' in current_key:\n",
        "                current_part_key = current_key\n",
        "                continue\n",
        "\n",
        "            if 'ITEM' in current_key:\n",
        "                if not current_part_key:\n",
        "                    current_part_key = \"PART I\"\n",
        "\n",
        "                start_node = header_info['tag']\n",
        "                end_node = doc_headers[i + 1]['tag'] if i + 1 < len(doc_headers) else None\n",
        "\n",
        "                content_parts = []\n",
        "                element = start_node.next_element\n",
        "\n",
        "                while element and element != end_node:\n",
        "                    if isinstance(element, NavigableString):\n",
        "                        if not element.find_parent('table'):\n",
        "                            text = element.strip()\n",
        "                            if text:\n",
        "                                content_parts.append(text)\n",
        "                    elif element.name == 'table':\n",
        "                        if not element.find_parent('table'):\n",
        "                            table_markdown = cls._parse_html_table(element)\n",
        "                            if table_markdown:\n",
        "                                content_parts.append(table_markdown)\n",
        "\n",
        "                    element = element.next_element\n",
        "\n",
        "                full_content = \"\\n\".join(content_parts)\n",
        "                clean_content = re.sub(r'\\n{3,}', '\\n\\n', full_content).strip()\n",
        "\n",
        "                parsed_data[current_part_key][current_key] = clean_content\n",
        "\n",
        "        return {part: dict(items) for part, items in parsed_data.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "2cbff8d1",
      "metadata": {
        "id": "2cbff8d1"
      },
      "outputs": [],
      "source": [
        "# ===========================================================================\n",
        "# PART 3: TEXT CHUNKING & EMBEDDING\n",
        "# ===========================================================================\n",
        "class DocumentProcessor:\n",
        "    \"\"\"\n",
        "    Processes documents into chunks using LangChain's splitter\n",
        "    and then embeds them in batches.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embedding_model_name: str = Config.EMBEDDING_MODEL):\n",
        "        \"\"\"Initialize with embedding model and LangChain text splitter\"\"\"\n",
        "        print(f\"\\n Loading embedding model: {embedding_model_name}\")\n",
        "        self.model = SentenceTransformer(embedding_model_name)\n",
        "        print(f\"   ✓ Model loaded (dimension: {self.model.get_sentence_embedding_dimension()})\\n\")\n",
        "\n",
        "        # Initialize LangChain's splitter\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=Config.CHUNK_SIZE,\n",
        "            chunk_overlap=Config.CHUNK_OVERLAP,\n",
        "            length_function=len,\n",
        "            add_start_index=False,\n",
        "        )\n",
        "        print(f\"   ✓ Initialized RecursiveCharacterTextSplitter (chunk: {Config.CHUNK_SIZE}, overlap: {Config.CHUNK_OVERLAP})\")\n",
        "\n",
        "    def generate_document_chunks(self, parsed_data: Dict, metadata: Dict,\n",
        "                                 embed_batch_size: int = 1024):\n",
        "        \"\"\"\n",
        "        Processes parsed 10-Q data using LangChain's splitter,\n",
        "        then YIELDS chunk points one by one after batch-embedding.\n",
        "        Tags table chunks with metadata\n",
        "        \"\"\"\n",
        "\n",
        "        # Convert the parsed dict into a list of LangChain Document objects\n",
        "        all_docs = []\n",
        "        for part, items in parsed_data.items():\n",
        "            for item, content in items.items():\n",
        "                if not content:\n",
        "                    continue\n",
        "\n",
        "                # Check if the content is a markdown table\n",
        "                # A simple heuristic: starts with | and has --- in the 2nd line\n",
        "                is_table = False\n",
        "                if content.strip().startswith(\"|\"):\n",
        "                    lines = content.strip().split('\\n')\n",
        "                    if len(lines) > 1 and \"---\" in lines[1]:\n",
        "                        is_table = True\n",
        "                \n",
        "                # Create a metadata dict for *this specific document*\n",
        "                # before it gets chunked.\n",
        "                doc_metadata = {\n",
        "                    'ticker': metadata['ticker'],\n",
        "                    'company_name': metadata['company_name'],\n",
        "                    'filing_date': metadata['filing_date'],\n",
        "                    'filing_url': metadata['filing_url'],\n",
        "                    'part': part,\n",
        "                    'item': item,\n",
        "                    'is_table': is_table\n",
        "                }\n",
        "\n",
        "                doc = Document(\n",
        "                    page_content=content,\n",
        "                    metadata=doc_metadata\n",
        "                )\n",
        "                all_docs.append(doc)\n",
        "\n",
        "        if not all_docs:\n",
        "            return\n",
        "\n",
        "        # Split all documents at once using the LangChain splitter\n",
        "        print(f\"     → Splitting {len(all_docs)} high-level 'Items' into smaller chunks...\")\n",
        "        chunked_docs = self.text_splitter.split_documents(all_docs)\n",
        "        print(f\"     → Generated {len(chunked_docs)} chunks\")\n",
        "\n",
        "        text_batch = []\n",
        "        metadata_batch = []\n",
        "\n",
        "        # Consume the list one chunk at a time\n",
        "        for chunk in chunked_docs:\n",
        "            text_batch.append(chunk.page_content)\n",
        "            # The splitter automatically copies metadata to each chunk\n",
        "            metadata_batch.append(chunk.metadata) \n",
        "\n",
        "            # If batch is full, process it\n",
        "            if len(text_batch) >= embed_batch_size:\n",
        "                # 1. Embed the entire batch in one call\n",
        "                embeddings = self.model.encode(text_batch, show_progress_bar=False)\n",
        "\n",
        "                # 2. Yield each point from the processed batch\n",
        "                for txt, emb, meta in zip(text_batch, embeddings, metadata_batch):\n",
        "                    # The metadata (meta) already contains everything\n",
        "                    # from the doc_metadata we built above\n",
        "                    payload = {\n",
        "                        'text': txt,\n",
        "                        **meta # Unpack all metadata keys (ticker, item, part, is_table, etc.)\n",
        "                    }\n",
        "                    yield PointStruct(\n",
        "                        id=str(uuid.uuid4()),\n",
        "                        vector=emb.tolist(),\n",
        "                        payload=payload\n",
        "                    )\n",
        "\n",
        "                # 3. Reset the batch\n",
        "                text_batch = []\n",
        "                metadata_batch = []\n",
        "\n",
        "        if text_batch:\n",
        "            # 1. Embed the final batch\n",
        "            embeddings = self.model.encode(text_batch, show_progress_bar=False)\n",
        "\n",
        "            # 2. Yield each point\n",
        "            for txt, emb, meta in zip(text_batch, embeddings, metadata_batch):\n",
        "                payload = {\n",
        "                    'text': txt,\n",
        "                    **meta\n",
        "                }\n",
        "                yield PointStruct(\n",
        "                    id=str(uuid.uuid4()),\n",
        "                    vector=emb.tolist(),\n",
        "                    payload=payload\n",
        "                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "80bed17c",
      "metadata": {
        "id": "80bed17c"
      },
      "outputs": [],
      "source": [
        "# ===========================================================================\n",
        "# PART 4: QDRANT VECTOR DATABASE\n",
        "# ===========================================================================\n",
        "class QdrantManager:\n",
        "    \"\"\"Manages Qdrant vector database operations\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize Qdrant client\"\"\"\n",
        "        print(f\"\\nConnecting to Qdrant Cloud...\")\n",
        "        self.client = QdrantClient(\n",
        "            url=Config.QDRANT_URL,\n",
        "            api_key=Config.QDRANT_API_KEY\n",
        "        )\n",
        "        print(f\"   ✓ Connected to Qdrant\")\n",
        "\n",
        "    def create_collection(self, collection_name: str = Config.COLLECTION_NAME,\n",
        "                         vector_size: int = Config.VECTOR_SIZE):\n",
        "        \"\"\"\n",
        "        Create or recreate collection AND set up payload indexes\n",
        "        --- MODIFIED: 'is_table' index removed ---\n",
        "        \"\"\"\n",
        "        print(f\"\\n Setting up collection: {collection_name}\")\n",
        "\n",
        "        collections = self.client.get_collections().collections\n",
        "        exists = any(col.name == collection_name for col in collections)\n",
        "\n",
        "        if exists:\n",
        "            print(f\"   ⚠ Collection exists, recreating...\")\n",
        "            self.client.delete_collection(collection_name)\n",
        "\n",
        "        self.client.create_collection(\n",
        "            collection_name=collection_name,\n",
        "            vectors_config=models.VectorParams(\n",
        "                size=vector_size,\n",
        "                distance=models.Distance.COSINE\n",
        "            )\n",
        "        )\n",
        "        print(f\"   ✓ Collection created\")\n",
        "\n",
        "        print(f\"   → Creating payload index for 'ticker'...\")\n",
        "        self.client.create_payload_index(\n",
        "            collection_name=collection_name,\n",
        "            field_name=\"ticker\",\n",
        "            field_schema=models.PayloadSchemaType.KEYWORD\n",
        "        )\n",
        "        print(f\"   → Creating payload index for 'item'...\")\n",
        "        self.client.create_payload_index(\n",
        "            collection_name=collection_name,\n",
        "            field_name=\"item\",\n",
        "            field_schema=models.PayloadSchemaType.KEYWORD\n",
        "        )\n",
        "        \n",
        "        print(f\"   ✓ Payload indexes created.\")\n",
        "\n",
        "    def upsert_documents(self, points_generator,\n",
        "                        collection_name: str = Config.COLLECTION_NAME,\n",
        "                        batch_size: int = 2048) -> int:\n",
        "        \"\"\"Uploads document chunks from a generator in batches.\"\"\"\n",
        "        print(f\" Uploading chunks to Qdrant in batches of {batch_size}...\")\n",
        "        batch = []\n",
        "        count = 0\n",
        "        for point in points_generator:\n",
        "            batch.append(point)\n",
        "            if len(batch) >= batch_size:\n",
        "                self.client.upsert(\n",
        "                    collection_name=collection_name,\n",
        "                    points=batch,\n",
        "                    wait=False\n",
        "                )\n",
        "                count += len(batch)\n",
        "                print(f\"     → Uploaded {count} chunks so far...\")\n",
        "                batch = []\n",
        "        if batch:\n",
        "            self.client.upsert(\n",
        "                collection_name=collection_name,\n",
        "                points=batch,\n",
        "                wait=False\n",
        "            )\n",
        "            count += len(batch)\n",
        "        print(f\"  ✓ All chunks uploaded for this document. Total: {count}\")\n",
        "        return count\n",
        "\n",
        "    def search(self, query_vector: List[float],\n",
        "              collection_name: str = Config.COLLECTION_NAME,\n",
        "              limit: int = Config.RETRIEVAL_TOP_K,\n",
        "              filter_dict: Dict = None) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Search for similar documents\n",
        "        --- MODIFIED: Removed boolean filter logic ---\n",
        "        \"\"\"\n",
        "        qdrant_filter = None\n",
        "        if filter_dict:\n",
        "            must_conditions = []\n",
        "            for key, value in filter_dict.items():\n",
        "                # Standard keyword match for strings (like ticker)\n",
        "                must_conditions.append(\n",
        "                    models.FieldCondition(\n",
        "                        key=key,\n",
        "                        match=models.MatchValue(value=value)\n",
        "                    )\n",
        "                )\n",
        "            \n",
        "            qdrant_filter = models.Filter(must=must_conditions)\n",
        "\n",
        "        results = self.client.search(\n",
        "            collection_name=collection_name,\n",
        "            query_vector=query_vector,\n",
        "            limit=limit,\n",
        "            query_filter=qdrant_filter,\n",
        "            with_payload=True\n",
        "        )\n",
        "\n",
        "        return [\n",
        "            {'score': result.score, 'payload': result.payload}\n",
        "            for result in results\n",
        "        ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "8471b72b",
      "metadata": {
        "id": "8471b72b"
      },
      "outputs": [],
      "source": [
        "# ===========================================================================\n",
        "# PART 5: Advancd RAG Engine\n",
        "# ===========================================================================\n",
        "\n",
        "class ManualRAGEngine:\n",
        "    \"\"\"\n",
        "    This is the manual RAG query engine, ENHANCED with\n",
        "    a Cross-Encoder for re-ranking.\n",
        "    --- MODIFIED: TaPas and table routing removed ---\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, document_processor: 'DocumentProcessor', qdrant_manager: 'QdrantManager'):\n",
        "        print(\"\\n Initializing Manual RAG Query Engine (v2 with Re-ranking)...\\n\")\n",
        "\n",
        "        # 1. Get the embedding model\n",
        "        self.embedding_model = document_processor.model\n",
        "        print(\"   ✓ Using existing embedding model from DocumentProcessor\")\n",
        "\n",
        "        # 2. Load the Cross-Encoder Model\n",
        "        print(f\"   → Loading Cross-Encoder: {Config.CROSS_ENCODER_MODEL}...\")\n",
        "        self.cross_encoder = CrossEncoder(Config.CROSS_ENCODER_MODEL)\n",
        "        print(\"   ✓ Cross-Encoder model loaded.\")\n",
        "\n",
        "        # 3. Get the Qdrant client\n",
        "        self.qdrant_manager = qdrant_manager\n",
        "        print(\"   ✓ Using existing QdrantManager for search\")\n",
        "\n",
        "        # 4. Initialize the LLM\n",
        "        self.llm = ChatOpenAI(\n",
        "            model=Config.LLM_MODEL,\n",
        "            api_key=Config.OPENAI_API_KEY,\n",
        "            temperature=0\n",
        "        )\n",
        "        print(\"   ✓ Initialized ChatOpenAI LLM\")\n",
        "\n",
        "        # 5. Create a prompt template\n",
        "        template = \"\"\"You are a helpful financial analyst assistant. Your role is to answer questions about SEC 10-Q filings based ONLY on the provided context.\n",
        "- Base your answer strictly on the provided context from SEC filings\n",
        "- Cite specific sections (e.g., \"According to Item 1A...\") when referencing information\n",
        "- If the answer is not in the context, clearly state that\n",
        "\n",
        "Context:\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "\n",
        "Question: {input}\n",
        "\n",
        "Answer:\"\"\"\n",
        "        self.prompt = ChatPromptTemplate.from_template(template)\n",
        "        \n",
        "        print(\"   ✓ Manual RAG Engine (v2) ready.\")\n",
        "\n",
        "\n",
        "    def _format_context(self, search_results: List[Dict]) -> str:\n",
        "        \"\"\"\n",
        "        Helper function to format retrieved text contexts.\n",
        "        \"\"\"\n",
        "        context_str = \"\"\n",
        "        for i, result in enumerate(search_results, 1):\n",
        "            payload = result.get('payload', {})\n",
        "            text = payload.get('text', 'No text found')\n",
        "            item = payload.get('item', 'N/A')\n",
        "            ticker = payload.get('ticker', 'N/A')\n",
        "            context_str += f\"Source {i} ({ticker} - {item}):\\n\\\"{text}\\\"\\n\\n\"\n",
        "        return context_str.strip()\n",
        "\n",
        "\n",
        "    def query(self, question: str, ticker_filter: str = None):\n",
        "        print(f\"\\n Processing query with Manual Engine (v2): '{question}'\")\n",
        "\n",
        "        # 1. Manually embed the query\n",
        "        print(\"   → Manually embedding query...\")\n",
        "        query_vector = self.embedding_model.encode(question).tolist()\n",
        "\n",
        "\n",
        "        # 2. Filter Logic\n",
        "        final_filter_dict = {}\n",
        "        if ticker_filter:\n",
        "            print(f\"   → Applying ticker filter: {ticker_filter}\")\n",
        "            final_filter_dict[\"ticker\"] = ticker_filter.upper()\n",
        "        \n",
        "        if not final_filter_dict:\n",
        "            print(\"   → No filters applied. Searching all documents.\")\n",
        "        \n",
        "        # 3. Manually search Qdrant (Candidate Retrieval)\n",
        "        print(f\"   → Searching Qdrant (retrieving Top {Config.RETRIEVAL_TOP_K} candidates)...\\n\")\n",
        "        search_results = self.qdrant_manager.search(\n",
        "            query_vector=query_vector,\n",
        "            limit=Config.RETRIEVAL_TOP_K, # Retrieve 20\n",
        "            filter_dict=final_filter_dict # Only contains ticker, if present\n",
        "        )\n",
        "\n",
        "        if not search_results:\n",
        "            return {'answer': 'No relevant context was found in the documents to answer this question.', 'sources': []}\n",
        "        \n",
        "        print(f\"   → Retrieved {len(search_results)} candidates.\")\n",
        "\n",
        "        # 4. Re-ranking\n",
        "        # All retrieved chunks are now treated as text.\n",
        "        print(f\"   → Re-ranking all {len(search_results)} candidates...\")\n",
        "        \n",
        "        passages = [result['payload']['text'] for result in search_results]\n",
        "        query_passage_pairs = [(question, passage) for passage in passages]\n",
        "        \n",
        "        print(f\"   → Re-ranking {len(passages)} text chunks with Cross-Encoder...\")\n",
        "        cross_encoder_scores = self.cross_encoder.predict(query_passage_pairs)\n",
        "        \n",
        "        final_ranked_results = []\n",
        "        for score, result in zip(cross_encoder_scores, search_results):\n",
        "            result['rerank_score'] = float(score)\n",
        "            final_ranked_results.append(result)\n",
        "\n",
        "        final_ranked_results.sort(key=lambda x: x['rerank_score'], reverse=True)\n",
        "        \n",
        "        # Get the final Top-K\n",
        "        final_top_k_chunks = final_ranked_results[:Config.FINAL_TOP_K]\n",
        "\n",
        "        # Store payloads for citation\n",
        "        final_sources_with_scores = []\n",
        "        for result in final_top_k_chunks:\n",
        "            source_data = result['payload']\n",
        "            source_data['retrieval_score'] = result['score']\n",
        "            source_data['rerank_score'] = result['rerank_score']\n",
        "            final_sources_with_scores.append(source_data)\n",
        "            \n",
        "        print(f\"   → Re-ranked. Final {len(final_sources_with_scores)} sources selected.\")\n",
        "        \n",
        "        # 5. Generation Step\n",
        "        print(\"   → Formatting context for LLM...\")\n",
        "        \n",
        "        # All chunks are text chunks\n",
        "        formatted_context = self._format_context(final_top_k_chunks)\n",
        "\n",
        "        if not formatted_context:\n",
        "             return {'answer': 'Retrieved context, but failed to process it.', 'sources': []}\n",
        "\n",
        "        final_prompt_message = self.prompt.format_messages(\n",
        "            context=formatted_context,\n",
        "            input=question\n",
        "        )\n",
        "\n",
        "        # 6. Manually invoke the LLM\n",
        "        print(\"   → Sending prompt to LLM...\")\n",
        "        llm_response = self.llm.invoke(final_prompt_message)\n",
        "        answer = llm_response.content\n",
        "\n",
        "        # 7. Format sources (using all 5 chunks for citation)\n",
        "        sources_output = []\n",
        "        for i, source_data in enumerate(final_sources_with_scores, 1):\n",
        "            sources_output.append({\n",
        "                'ticker': source_data.get('ticker'),\n",
        "                'company': source_data.get('company_name'),\n",
        "                'item': source_data.get('item'),\n",
        "                'part': source_data.get('part'),\n",
        "                # 'is_table': source_data.get('is_table'), <-- REMOVED\n",
        "                'filing_date': source_data.get('filing_date'),\n",
        "                'rerank_score': source_data['rerank_score'],\n",
        "                'retrieval_score': source_data['retrieval_score']\n",
        "            })\n",
        "\n",
        "        return {\n",
        "            'answer': answer,\n",
        "            'sources': sources_output\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "9b12c056",
      "metadata": {
        "id": "9b12c056"
      },
      "outputs": [],
      "source": [
        "# ===========================================================================\n",
        "# PART 6: MAIN PIPELINE ORCHESTRATOR\n",
        "# ===========================================================================\n",
        "\n",
        "class SECFilingRAGPipeline:\n",
        "    \"\"\"Main pipeline orchestrator\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize all components\"\"\"\n",
        "        print(\"=\" * 70)\n",
        "        print(\"SEC 10-Q FILING RAG SYSTEM (v2 with Re-ranking)\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        # Ingestion components\n",
        "        self.loader = SECDocumentLoader()\n",
        "        self.processor = DocumentProcessor()\n",
        "        self.qdrant_manager = QdrantManager()\n",
        "\n",
        "        # Query component\n",
        "        self.query_engine = None\n",
        "\n",
        "\n",
        "    def load_and_index_filings(self, tickers: List[str] = Config.TICKERS, num_filings_per_ticker: int = 1):\n",
        "        \"\"\"\n",
        "        MODIFIED: This function now only checks if the Golden Set exists.\n",
        "        It does NOT perform indexing.\n",
        "        \"\"\"\n",
        "        print(f\"\\\\n{'=' * 70}\")\n",
        "        print(f\"CHECKING DATABASE: {Config.COLLECTION_NAME}\")\n",
        "        print(f\"{'=' * 70}\")\n",
        "        \n",
        "        try:\n",
        "            count = self.qdrant_manager.client.count(Config.COLLECTION_NAME, exact=True)\n",
        "            if count.count > 0:\n",
        "                print(f\"✓ Collection '{Config.COLLECTION_NAME}' found with {count.count} docs. Proceeding to query.\")\n",
        "            else:\n",
        "                print(f\"✗ ERROR: Collection '{Config.COLLECTION_NAME}' is empty or not found.\")\n",
        "                print(\"Please run '0_build_database.ipynb' first.\")\n",
        "                raise Exception(\"Database not found.\")\n",
        "        except Exception as e:\n",
        "            print(f\"✗ ERROR: Could not connect to or find collection '{Config.COLLECTION_NAME}'.\")\n",
        "            print(\"Please run '0_build_database.ipynb' first.\")\n",
        "            raise e\n",
        "        \n",
        "        print(f\"{'=' * 70}\\\\n\")\n",
        "\n",
        "\n",
        "    def query(self, question: str, ticker_filter: str = None):\n",
        "        \"\"\"\n",
        "        Query the indexed filings using the new MANUAL engine\n",
        "        \"\"\"\n",
        "        if self.query_engine is None:\n",
        "            self.query_engine = ManualRAGEngine(\n",
        "                document_processor=self.processor,\n",
        "                qdrant_manager=self.qdrant_manager\n",
        "            )\n",
        "\n",
        "        result = self.query_engine.query(question, ticker_filter)\n",
        "\n",
        "        # Print results\n",
        "        print(f\"\\n{'=' * 70}\")\n",
        "        print(f\"ANSWER\")\n",
        "        print(f\"__{result['answer']}__\\n\") # Bolding answer\n",
        "\n",
        "        print(f\"{'=' * 70}\")\n",
        "        print(f\"SOURCES ({len(result['sources'])} chunks)\")\n",
        "        print(f\"{'=' * 70}\")\n",
        "        for i, source in enumerate(result['sources'], 1):\n",
        "            print(f\"\\n{i}. {source['company']} ({source['ticker']}) - {source['item']}\")\n",
        "            print(f\"   Filing Date: {source['filing_date']}\")\n",
        "            # --- MODIFICATION ---\n",
        "            # print(f\"   Is Table: {source['is_table']}\") # <-- REMOVED\n",
        "            # --- END MODIFICATION ---\n",
        "            print(f\"   Re-Rank Score: {source['rerank_score']:.4f} (Main score)\")\n",
        "            print(f\"   Vector Score: {source['retrieval_score']:.4f} (Initial retrieval)\")\n",
        "\n",
        "        print(f\"\\n{'=' * 70}\\n\")\n",
        "\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "7fb9524b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fb9524b",
        "outputId": "68e39ca6-af07-4360-9353-71f56d4754e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "SEC 10-Q FILING RAG SYSTEM (v2 with Re-ranking)\n",
            "======================================================================\n",
            "\n",
            " Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
            "   ✓ Model loaded (dimension: 384)\n",
            "\n",
            "   ✓ Initialized RecursiveCharacterTextSplitter (chunk: 800, overlap: 200)\n",
            "\n",
            "Connecting to Qdrant Cloud...\n",
            "   ✓ Connected to Qdrant\n",
            "Starting the indexing process... This may take a long time.\n",
            "\\n======================================================================\n",
            "CHECKING DATABASE: sec_filings_10q_GOLDEN_BENCHMARK\n",
            "======================================================================\n",
            "✓ Collection 'sec_filings_10q_GOLDEN_BENCHMARK' found with 12807 docs. Proceeding to query.\n",
            "======================================================================\\n\n",
            "Indexing complete. Proceeding to queries.\n"
          ]
        }
      ],
      "source": [
        "# ===========================================================================\n",
        "# USAGE EXAMPLE (Running 6 questions and saving all results to one file)\n",
        "# ===========================================================================\n",
        "import os\n",
        "import json\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # =======================================================================\n",
        "    # 1. INITIALIZE PIPELINE\n",
        "    # =======================================================================\n",
        "    pipeline = SECFilingRAGPipeline()\n",
        "\n",
        "    # =======================================================================\n",
        "    # 2. LOAD AND INDEX FILINGS\n",
        "    # =======================================================================\n",
        "    # --- IMPORTANT ---\n",
        "    # As we discussed, this model uses a new collection ('sec_filings_10q_v2').\n",
        "    # You MUST run this line ONCE to create and fill that collection.\n",
        "    # After the first successful run, you can comment it out.\n",
        "    \n",
        "    print(\"Starting the indexing process... This may take a long time.\")\n",
        "    pipeline.load_and_index_filings(num_filings_per_ticker=4)\n",
        "    print(\"Indexing complete. Proceeding to queries.\")\n",
        "    # --- End of Indexing ---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "0aa8ad26",
      "metadata": {
        "id": "0aa8ad26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\\nRunning 6 queries with (Model 2) Post-Retrieval Re-ranking...\n",
            "\\n--- Running Query: What are the main risk factors mentioned by each companies? ---\n",
            "\n",
            " Initializing Manual RAG Query Engine (v2 with Re-ranking)...\n",
            "\n",
            "   ✓ Using existing embedding model from DocumentProcessor\n",
            "   → Loading Cross-Encoder: cross-encoder/ms-marco-MiniLM-L-6-v2...\n",
            "   ✓ Cross-Encoder model loaded.\n",
            "   ✓ Using existing QdrantManager for search\n",
            "   ✓ Initialized ChatOpenAI LLM\n",
            "   ✓ Manual RAG Engine (v2) ready.\n",
            "\n",
            " Processing query with Manual Engine (v2): 'What are the main risk factors mentioned by each companies?'\n",
            "   → Manually embedding query...\n",
            "   → No filters applied. Searching all documents.\n",
            "   → Searching Qdrant (retrieving Top 20 candidates)...\n",
            "\n",
            "   → Retrieved 20 candidates.\n",
            "   → Re-ranking all 20 candidates...\n",
            "   → Re-ranking 20 text chunks with Cross-Encoder...\n",
            "   → Re-ranked. Final 5 sources selected.\n",
            "   → Formatting context for LLM...\n",
            "   → Sending prompt to LLM...\n",
            "\n",
            "======================================================================\n",
            "ANSWER\n",
            "__According to the provided context:\n",
            "\n",
            "For Apple (AAPL):\n",
            "- The main risk factors are not explicitly detailed in the provided context. However, it is mentioned that there have been no material changes to the Company's risk factors since the 2024 Form 10-K, as stated in \"Item 1A. Risk Factors.\"\n",
            "\n",
            "For AMD:\n",
            "- The main risk factors include:\n",
            "  - Intel Corporation’s dominance of the microprocessor market and its aggressive business practices, which may limit AMD's ability to compete effectively (Source 2).\n",
            "  - The highly competitive and rapidly evolving markets in which AMD's products are sold (Source 4 and Source 5).\n",
            "  - The cyclical nature of the semiconductor industry, which has experienced severe downturns (Source 4 and Source 5).__\n",
            "\n",
            "======================================================================\n",
            "SOURCES (5 chunks)\n",
            "======================================================================\n",
            "\n",
            "1. Apple Inc. (AAPL) - ITEM 1A\n",
            "   Filing Date: 2025-01-31\n",
            "   Re-Rank Score: 4.0525 (Main score)\n",
            "   Vector Score: 0.6650 (Initial retrieval)\n",
            "\n",
            "2. ADVANCED MICRO DEVICES INC (AMD) - ITEM 1A\n",
            "   Filing Date: 2024-10-30\n",
            "   Re-Rank Score: 3.5313 (Main score)\n",
            "   Vector Score: 0.6457 (Initial retrieval)\n",
            "\n",
            "3. ADVANCED MICRO DEVICES INC (AMD) - ITEM 1A\n",
            "   Filing Date: 2025-05-07\n",
            "   Re-Rank Score: 3.5313 (Main score)\n",
            "   Vector Score: 0.6457 (Initial retrieval)\n",
            "\n",
            "4. ADVANCED MICRO DEVICES INC (AMD) - ITEM 1A\n",
            "   Filing Date: 2025-11-05\n",
            "   Re-Rank Score: 2.6218 (Main score)\n",
            "   Vector Score: 0.6942 (Initial retrieval)\n",
            "\n",
            "5. ADVANCED MICRO DEVICES INC (AMD) - ITEM 1A\n",
            "   Filing Date: 2025-08-06\n",
            "   Re-Rank Score: 2.6218 (Main score)\n",
            "   Vector Score: 0.6942 (Initial retrieval)\n",
            "\n",
            "======================================================================\n",
            "\n",
            "\\n--- Running Query: What risks did Apple disclose in their latest 10-Q? ---\n",
            "\n",
            " Processing query with Manual Engine (v2): 'What risks did Apple disclose in their latest 10-Q?'\n",
            "   → Manually embedding query...\n",
            "   → No filters applied. Searching all documents.\n",
            "   → Searching Qdrant (retrieving Top 20 candidates)...\n",
            "\n",
            "   → Retrieved 20 candidates.\n",
            "   → Re-ranking all 20 candidates...\n",
            "   → Re-ranking 20 text chunks with Cross-Encoder...\n",
            "   → Re-ranked. Final 5 sources selected.\n",
            "   → Formatting context for LLM...\n",
            "   → Sending prompt to LLM...\n",
            "\n",
            "======================================================================\n",
            "ANSWER\n",
            "__According to the provided context, Apple disclosed several risks in their latest 10-Q. These include:\n",
            "\n",
            "1. **Supply Chain and Manufacturing Risks**: Events that can make it difficult or impossible for Apple to manufacture and deliver products, create delays and inefficiencies in the supply and manufacturing chain, result in slowdowns and outages to service offerings, increase costs, and negatively impact consumer spending and demand in affected areas (Source 3, AAPL - ITEM 1A).\n",
            "\n",
            "2. **Macroeconomic Conditions**: Inflation, interest rates, and currency fluctuations have directly and indirectly impacted, and could in the future materially impact, Apple's results of operations and financial condition (Source 5, AAPL - ITEM 2).\n",
            "\n",
            "3. **Gross Margin Volatility**: Apple's future gross margins can be impacted by a variety of factors, leading to volatility and downward pressure (Source 2, AAPL - ITEM 2).\n",
            "\n",
            "There have been no material changes to the Company’s risk factors since the 2023 Form 10-K (Source 1, AAPL - ITEM 1A).__\n",
            "\n",
            "======================================================================\n",
            "SOURCES (5 chunks)\n",
            "======================================================================\n",
            "\n",
            "1. Apple Inc. (AAPL) - ITEM 1A\n",
            "   Filing Date: 2024-08-02\n",
            "   Re-Rank Score: 3.0329 (Main score)\n",
            "   Vector Score: 0.7313 (Initial retrieval)\n",
            "\n",
            "2. Apple Inc. (AAPL) - ITEM 2\n",
            "   Filing Date: 2025-05-02\n",
            "   Re-Rank Score: 2.4537 (Main score)\n",
            "   Vector Score: 0.6190 (Initial retrieval)\n",
            "\n",
            "3. Apple Inc. (AAPL) - ITEM 1A\n",
            "   Filing Date: 2025-05-02\n",
            "   Re-Rank Score: 0.8328 (Main score)\n",
            "   Vector Score: 0.6020 (Initial retrieval)\n",
            "\n",
            "4. Apple Inc. (AAPL) - ITEM 4\n",
            "   Filing Date: 2025-01-31\n",
            "   Re-Rank Score: 0.6507 (Main score)\n",
            "   Vector Score: 0.5810 (Initial retrieval)\n",
            "\n",
            "5. Apple Inc. (AAPL) - ITEM 2\n",
            "   Filing Date: 2025-05-02\n",
            "   Re-Rank Score: -0.7296 (Main score)\n",
            "   Vector Score: 0.6400 (Initial retrieval)\n",
            "\n",
            "======================================================================\n",
            "\n",
            "\\n--- Running Query: Compare the revenue trends of NVIDIA and AMD ---\n",
            "\n",
            " Processing query with Manual Engine (v2): 'Compare the revenue trends of NVIDIA and AMD'\n",
            "   → Manually embedding query...\n",
            "   → No filters applied. Searching all documents.\n",
            "   → Searching Qdrant (retrieving Top 20 candidates)...\n",
            "\n",
            "   → Retrieved 20 candidates.\n",
            "   → Re-ranking all 20 candidates...\n",
            "   → Re-ranking 20 text chunks with Cross-Encoder...\n",
            "   → Re-ranked. Final 5 sources selected.\n",
            "   → Formatting context for LLM...\n",
            "   → Sending prompt to LLM...\n",
            "\n",
            "======================================================================\n",
            "ANSWER\n",
            "__Based on the provided context, we can observe the following revenue trends for NVIDIA and AMD:\n",
            "\n",
            "**NVIDIA:**\n",
            "- One customer represented approximately 17% and 13% of total revenue for the second quarter and first half of fiscal year 2024, respectively, indicating a significant contribution from a single customer to NVIDIA's revenue (Source 1).\n",
            "- Revenue from sales to customers outside of the United States accounted for 53% and 48% of total revenue for the first quarter of fiscal years 2026 and 2025, respectively, showing a strong international presence (Source 3).\n",
            "\n",
            "**AMD:**\n",
            "- Net revenue for the three months ended September 27, 2025, was $9.2 billion, a 36% increase compared to the prior year period. This increase was driven by strong demand in the Client and Gaming segment, particularly for AMD Ryzen™ processors, semi-custom game console SoCs, and Radeon™ gaming GPUs, as well as an increase in the Data Center segment due to demand for AMD EPYC™ processors and AMD Instinct™ MI350 Series GPUs (Source 4).\n",
            "\n",
            "In summary, NVIDIA's revenue trends show a significant reliance on a major customer and a strong international market presence, while AMD's revenue trends indicate substantial growth driven by demand in specific product segments, particularly in gaming and data center markets.__\n",
            "\n",
            "======================================================================\n",
            "SOURCES (5 chunks)\n",
            "======================================================================\n",
            "\n",
            "1. NVIDIA CORP (NVDA) - ITEM 1\n",
            "   Filing Date: 2024-08-28\n",
            "   Re-Rank Score: 1.2985 (Main score)\n",
            "   Vector Score: 0.6962 (Initial retrieval)\n",
            "\n",
            "2. NVIDIA CORP (NVDA) - ITEM 1\n",
            "   Filing Date: 2025-08-27\n",
            "   Re-Rank Score: 0.8773 (Main score)\n",
            "   Vector Score: 0.5922 (Initial retrieval)\n",
            "\n",
            "3. NVIDIA CORP (NVDA) - ITEM 1\n",
            "   Filing Date: 2025-05-28\n",
            "   Re-Rank Score: 0.6547 (Main score)\n",
            "   Vector Score: 0.5925 (Initial retrieval)\n",
            "\n",
            "4. ADVANCED MICRO DEVICES INC (AMD) - ITEM 2\n",
            "   Filing Date: 2025-11-05\n",
            "   Re-Rank Score: 0.6101 (Main score)\n",
            "   Vector Score: 0.6258 (Initial retrieval)\n",
            "\n",
            "5. NVIDIA CORP (NVDA) - ITEM 1\n",
            "   Filing Date: 2024-08-28\n",
            "   Re-Rank Score: 0.0571 (Main score)\n",
            "   Vector Score: 0.6392 (Initial retrieval)\n",
            "\n",
            "======================================================================\n",
            "\n",
            "\\n--- Running Query: What was Tesla's R&D spending in the latest quarter? ---\n",
            "\n",
            " Processing query with Manual Engine (v2): 'What was Tesla's R&D spending in the latest quarter?'\n",
            "   → Manually embedding query...\n",
            "   → No filters applied. Searching all documents.\n",
            "   → Searching Qdrant (retrieving Top 20 candidates)...\n",
            "\n",
            "   → Retrieved 20 candidates.\n",
            "   → Re-ranking all 20 candidates...\n",
            "   → Re-ranking 20 text chunks with Cross-Encoder...\n",
            "   → Re-ranked. Final 5 sources selected.\n",
            "   → Formatting context for LLM...\n",
            "   → Sending prompt to LLM...\n",
            "\n",
            "======================================================================\n",
            "ANSWER\n",
            "__The specific amount of Tesla's R&D spending in the latest quarter is not provided in the context. However, according to Source 1 (TSLA - ITEM 2), R&D expenses increased by $591 million, or 57%, in the three months ended September 30, 2025, compared to the same period in 2024.__\n",
            "\n",
            "======================================================================\n",
            "SOURCES (5 chunks)\n",
            "======================================================================\n",
            "\n",
            "1. Tesla, Inc. (TSLA) - ITEM 2\n",
            "   Filing Date: 2025-10-23\n",
            "   Re-Rank Score: -0.5320 (Main score)\n",
            "   Vector Score: 0.5687 (Initial retrieval)\n",
            "\n",
            "2. Tesla, Inc. (TSLA) - ITEM 2\n",
            "   Filing Date: 2025-07-24\n",
            "   Re-Rank Score: -0.9581 (Main score)\n",
            "   Vector Score: 0.5555 (Initial retrieval)\n",
            "\n",
            "3. Alphabet Inc. (GOOGL) - ITEM 2\n",
            "   Filing Date: 2024-10-30\n",
            "   Re-Rank Score: -1.2043 (Main score)\n",
            "   Vector Score: 0.4775 (Initial retrieval)\n",
            "\n",
            "4. Tesla, Inc. (TSLA) - ITEM 2\n",
            "   Filing Date: 2024-10-24\n",
            "   Re-Rank Score: -1.2376 (Main score)\n",
            "   Vector Score: 0.5818 (Initial retrieval)\n",
            "\n",
            "5. Tesla, Inc. (TSLA) - ITEM 2\n",
            "   Filing Date: 2024-10-24\n",
            "   Re-Rank Score: -1.2654 (Main score)\n",
            "   Vector Score: 0.5433 (Initial retrieval)\n",
            "\n",
            "======================================================================\n",
            "\n",
            "\\n--- Running Query: How has Microsoft's operating income changed over the last year? ---\n",
            "\n",
            " Processing query with Manual Engine (v2): 'How has Microsoft's operating income changed over the last year?'\n",
            "   → Manually embedding query...\n",
            "   → No filters applied. Searching all documents.\n",
            "   → Searching Qdrant (retrieving Top 20 candidates)...\n",
            "\n",
            "   → Retrieved 20 candidates.\n",
            "   → Re-ranking all 20 candidates...\n",
            "   → Re-ranking 20 text chunks with Cross-Encoder...\n",
            "   → Re-ranked. Final 5 sources selected.\n",
            "   → Formatting context for LLM...\n",
            "   → Sending prompt to LLM...\n",
            "\n",
            "======================================================================\n",
            "ANSWER\n",
            "__The provided context does not contain any information about Microsoft's operating income. The context only includes information about Amazon's and AMD's operating income.__\n",
            "\n",
            "======================================================================\n",
            "SOURCES (5 chunks)\n",
            "======================================================================\n",
            "\n",
            "1. AMAZON COM INC (AMZN) - ITEM 2\n",
            "   Filing Date: 2025-08-01\n",
            "   Re-Rank Score: 1.6405 (Main score)\n",
            "   Vector Score: 0.5818 (Initial retrieval)\n",
            "\n",
            "2. AMAZON COM INC (AMZN) - ITEM 2\n",
            "   Filing Date: 2025-05-02\n",
            "   Re-Rank Score: 0.6784 (Main score)\n",
            "   Vector Score: 0.5744 (Initial retrieval)\n",
            "\n",
            "3. AMAZON COM INC (AMZN) - ITEM 2\n",
            "   Filing Date: 2024-11-01\n",
            "   Re-Rank Score: 0.2361 (Main score)\n",
            "   Vector Score: 0.5781 (Initial retrieval)\n",
            "\n",
            "4. ADVANCED MICRO DEVICES INC (AMD) - ITEM 2\n",
            "   Filing Date: 2024-10-30\n",
            "   Re-Rank Score: -0.2592 (Main score)\n",
            "   Vector Score: 0.5474 (Initial retrieval)\n",
            "\n",
            "5. ADVANCED MICRO DEVICES INC (AMD) - ITEM 2\n",
            "   Filing Date: 2024-10-30\n",
            "   Re-Rank Score: -0.7683 (Main score)\n",
            "   Vector Score: 0.5497 (Initial retrieval)\n",
            "\n",
            "======================================================================\n",
            "\n",
            "\\n--- Running Query: What was the gross profit margin for all companies? ---\n",
            "\n",
            " Processing query with Manual Engine (v2): 'What was the gross profit margin for all companies?'\n",
            "   → Manually embedding query...\n",
            "   → No filters applied. Searching all documents.\n",
            "   → Searching Qdrant (retrieving Top 20 candidates)...\n",
            "\n",
            "   → Retrieved 20 candidates.\n",
            "   → Re-ranking all 20 candidates...\n",
            "   → Re-ranking 20 text chunks with Cross-Encoder...\n",
            "   → Re-ranked. Final 5 sources selected.\n",
            "   → Formatting context for LLM...\n",
            "   → Sending prompt to LLM...\n",
            "\n",
            "======================================================================\n",
            "ANSWER\n",
            "__According to the provided context:\n",
            "\n",
            "- For NVIDIA (NVDA):\n",
            "  - The gross margin was 75.1% for the second quarter of fiscal year 2025 and 76.6% for the first half of fiscal year 2025 (Source 1).\n",
            "  - The gross margin was 74.6% for the third quarter of fiscal year 2025 (Source 2).\n",
            "\n",
            "- For Tesla (TSLA):\n",
            "  - The gross margin was 19.8% for the period referenced in Source 3.\n",
            "  - The gross margin was 17.2% and 18.0% for the periods referenced in Source 4.\n",
            "  - The gross margin was 18.0% and 19.8% for the periods referenced in Source 5.\n",
            "\n",
            "These are the gross profit margins for the periods mentioned in the context for both companies.__\n",
            "\n",
            "======================================================================\n",
            "SOURCES (5 chunks)\n",
            "======================================================================\n",
            "\n",
            "1. NVIDIA CORP (NVDA) - ITEM 2\n",
            "   Filing Date: 2024-08-28\n",
            "   Re-Rank Score: 4.2103 (Main score)\n",
            "   Vector Score: 0.6454 (Initial retrieval)\n",
            "\n",
            "2. NVIDIA CORP (NVDA) - ITEM 2\n",
            "   Filing Date: 2024-11-20\n",
            "   Re-Rank Score: 3.3715 (Main score)\n",
            "   Vector Score: 0.5876 (Initial retrieval)\n",
            "\n",
            "3. Tesla, Inc. (TSLA) - ITEM 2\n",
            "   Filing Date: 2024-10-24\n",
            "   Re-Rank Score: 2.1722 (Main score)\n",
            "   Vector Score: 0.6659 (Initial retrieval)\n",
            "\n",
            "4. Tesla, Inc. (TSLA) - ITEM 2\n",
            "   Filing Date: 2025-07-24\n",
            "   Re-Rank Score: 1.9313 (Main score)\n",
            "   Vector Score: 0.6528 (Initial retrieval)\n",
            "\n",
            "5. Tesla, Inc. (TSLA) - ITEM 2\n",
            "   Filing Date: 2025-10-23\n",
            "   Re-Rank Score: 1.8279 (Main score)\n",
            "   Vector Score: 0.6704 (Initial retrieval)\n",
            "\n",
            "======================================================================\n",
            "\n",
            "\\n--- All queries complete. Saving all results to: data\\results_model_2_post_retrieval.json ---\n",
            "✓ Successfully saved all answers.\n",
            "\\nPipeline run finished.\n"
          ]
        }
      ],
      "source": [
        "# =======================================================================\n",
        "# DEFINE QUESTIONS AND SAVE ALL ANSWERS TO ONE FILE\n",
        "# =======================================================================\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Define the output folder\n",
        "DATA_FOLDER = \"data\"\n",
        "if not os.path.exists(DATA_FOLDER):\n",
        "    os.makedirs(DATA_FOLDER)\n",
        "    print(f\"Created data folder: {DATA_FOLDER}\")\n",
        "\n",
        "# --- Standardized Query Set ---\n",
        "queries_to_run = [\n",
        "    {\"question\": \"What are the main risk factors mentioned by each companies?\", \"ticker_filter\": None},\n",
        "    {\"question\": \"What risks did Apple disclose in their latest 10-Q?\", \"ticker_filter\": None},\n",
        "    {\"question\": \"Compare the revenue trends of NVIDIA and AMD\", \"ticker_filter\": None},\n",
        "    {\"question\": \"What was Tesla's R&D spending in the latest quarter?\", \"ticker_filter\": None},\n",
        "    {\"question\": \"How has Microsoft's operating income changed over the last year?\", \"ticker_filter\": None},\n",
        "    {\"question\": \"What was the gross profit margin for all companies?\", \"ticker_filter\": None},\n",
        "]\n",
        "\n",
        "print(f\"\\\\nRunning {len(queries_to_run)} queries with (Model 2) Post-Retrieval Re-ranking...\")\n",
        "\n",
        "all_results = []\n",
        "\n",
        "for query in queries_to_run:\n",
        "    print(f\"\\\\n--- Running Query: {query['question']} ---\")\n",
        "    \n",
        "    result = pipeline.query(\n",
        "        question=query[\"question\"],\n",
        "        ticker_filter=query[\"ticker_filter\"]\n",
        "    )\n",
        "    \n",
        "    query_output = {\n",
        "        \"question\": query[\"question\"],\n",
        "        \"ticker_filter_manual\": query[\"ticker_filter\"],\n",
        "        \"response\": result\n",
        "    }\n",
        "    all_results.append(query_output)\n",
        "\n",
        "# --- Standardized Save Path ---\n",
        "save_path = os.path.join(DATA_FOLDER, \"results_model_2_post_retrieval.json\")\n",
        "print(f\"\\\\n--- All queries complete. Saving all results to: {save_path} ---\")\n",
        "\n",
        "try:\n",
        "    with open(save_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(all_results, f, indent=4)\n",
        "    print(f\"✓ Successfully saved all answers.\")\n",
        "except Exception as e:\n",
        "    print(f\"✗ Failed to save results: {e}\")\n",
        "\n",
        "print(\"\\\\nPipeline run finished.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
