{"cells":[{"cell_type":"markdown","id":"32ad6a86","metadata":{"id":"32ad6a86"},"source":["# Query Processing (Expansion + Intent Classification with FinBERT)"]},{"cell_type":"markdown","id":"0b3710ed","metadata":{"id":"0b3710ed"},"source":["This notebook builds the query processing module with expansion/paraphrasing and FinBERT-based intent classification."]},{"cell_type":"markdown","id":"bb92d11a","metadata":{"id":"bb92d11a"},"source":["\n","## 0. Environment & Dependencies\n","Toggles to control whether we use Flan-T5 and Sentence-BERT.\n"]},{"cell_type":"code","execution_count":null,"id":"3c51349b","metadata":{"id":"3c51349b"},"outputs":[],"source":["\n","# Toggles and model names\n","USE_FLAN_T5 = True\n","USE_SBERT = True\n","USE_FINBERT_INTENT = True\n","\n","FINBERT_MODEL_NAME = \"ProsusAI/finbert\"\n","FLAN_T5_MODEL_NAME = \"google/flan-t5-base\"\n","SBERT_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n"]},{"cell_type":"markdown","id":"a711b21d","metadata":{"id":"a711b21d"},"source":["## 1. Utilities & Normalization"]},{"cell_type":"code","execution_count":null,"id":"effa1324","metadata":{"id":"effa1324"},"outputs":[],"source":["\n","#!pip install -r requirements.txt\n","import re, unicodedata\n","from typing import List, Dict, Optional, Any\n","\n","def normalize(text: str) -> str:\n","    text = unicodedata.normalize(\"NFKC\", text).strip()\n","    text = re.sub(r\"\\s+\", \" \", text)\n","    return text\n","\n","def simple_tokenize(text: str) -> List[str]:\n","    return re.findall(r\"[A-Za-z0-9&$%\\.\\-]+\", text)\n","\n","def extract_entities(text: str) -> Dict[str, List[str]]:\n","    out: Dict[str, List[str]] = {}\n","    low = text.lower()\n","\n","    q = re.findall(r\"\\b(q[1-4])\\s*([12][0-9]{3})\\b\", low)\n","    if q:\n","        out[\"quarter\"] = [f\"{p.upper()} {y}\" for p, y in q]\n","\n","    years = re.findall(r\"\\b(20[0-4][0-9]|19[0-9]{2})\\b\", text)\n","    if years:\n","        out.setdefault(\"year\", list(sorted(set(years))))\n","\n","    tickers = re.findall(r\"\\b[A-Z]{1,5}\\b\", text.upper())\n","    tickers = [t for t in tickers if len(t) <= 5 and not t.startswith(\"Q\")]\n","    if tickers:\n","        out[\"ticker\"] = list(sorted(set(tickers)))\n","\n","    return out\n","\n","DOMAIN_SYNONYMS = {\n","    \"risk\": [\"risk factor\",\"risk factors\",\"uncertainty\",\"exposure\",\"threat\"],\n","    \"cyber\": [\"cybersecurity\",\"information security\",\"infosec\",\"data breach\",\"security incident\"],\n","    \"performance\": [\"revenue\",\"growth\",\"margin\",\"profit\",\"loss\",\"guidance\",\"results\"],\n","    \"strategy\": [\"roadmap\",\"plan\",\"initiative\",\"expansion\",\"capex\",\"restructuring\",\"acquisition\"],\n","    \"md&a\": [\"management discussion\",\"md&a\",\"results of operations\"],\n","}\n"]},{"cell_type":"markdown","id":"4a76ad36","metadata":{"id":"4a76ad36"},"source":["## 2. Sentence-BERT for Semantic Expansion & Embeddings"]},{"cell_type":"code","execution_count":null,"id":"dd127705","metadata":{"id":"dd127705"},"outputs":[],"source":["\n","try:\n","    if USE_SBERT:\n","        from sentence_transformers import SentenceTransformer, util\n","        sbert = SentenceTransformer(SBERT_MODEL_NAME)\n","    else:\n","        sbert = None\n","except Exception as e:\n","    print(\"Sentence-BERT not available:\", e)\n","    sbert = None\n","\n","from typing import List, Dict, Optional, Any\n","\n","def sbert_embed(text: str) -> Optional[List[float]]:\n","    if sbert is None:\n","        return None\n","    vec = sbert.encode([text], normalize_embeddings=True)[0]\n","    return vec.tolist()\n","\n","def semantic_expand(base_terms: List[str], k: int = 5) -> List[str]:\n","    if sbert is None:\n","        return []\n","    vocab = sorted({t for syns in DOMAIN_SYNONYMS.values() for t in syns})\n","    if not vocab:\n","        return []\n","    base_vecs = sbert.encode(base_terms, normalize_embeddings=True) if base_terms else []\n","    vocab_vecs = sbert.encode(vocab, normalize_embeddings=True)\n","    expansions = set()\n","    for bv in base_vecs:\n","        sims = util.cos_sim(bv, vocab_vecs).squeeze(0).tolist()\n","        top_idx = sorted(range(len(vocab)), key=lambda i: sims[i], reverse=True)[:k]\n","        for i in top_idx:\n","            expansions.add(vocab[i])\n","    return list(sorted(expansions))\n"]},{"cell_type":"markdown","id":"a92d3953","metadata":{"id":"a92d3953"},"source":["## 3. Flan-T5 for Paraphrase-based Expansion"]},{"cell_type":"code","execution_count":null,"id":"9729f3ac","metadata":{"id":"9729f3ac"},"outputs":[],"source":["import torch\n","from typing import List\n","try:\n","    if USE_FLAN_T5:\n","        from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","\n","        FLAN_T5_MODEL_NAME = FLAN_T5_MODEL_NAME if \"FLAN_T5_MODEL_NAME\" in globals() else \"google/flan-t5-small\"\n","        flan_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","        flan_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n","\n","        flan_tok = AutoTokenizer.from_pretrained(FLAN_T5_MODEL_NAME)\n","        flan_mdl = AutoModelForSeq2SeqLM.from_pretrained(\n","            FLAN_T5_MODEL_NAME,\n","            torch_dtype=flan_dtype\n","        ).to(flan_device)\n","        flan_mdl.eval()\n","    else:\n","        flan_tok = None\n","        flan_mdl = None\n","        flan_device = \"cpu\"\n","except Exception as e:\n","    print(\"Flan-T5 not available:\", e)\n","    flan_tok = None\n","    flan_mdl = None\n","    flan_device = \"cpu\"\n","\n","def t5_paraphrases_safe(\n","    query: str,\n","    num_return: int = 5,\n","    max_new_tokens: int = 48,\n","    max_input_tokens: int = 128\n",") -> list[str]:\n","    if not (USE_FLAN_T5 and 'flan_tok' in globals() and flan_tok is not None and 'flan_mdl' in globals() and flan_mdl is not None):\n","        return []\n","\n","    prompt = f\"Paraphrase the user query in multiple diverse ways. Keep the meaning and keep it concise.\\nQuery: {query}\"\n","    x = flan_tok(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_input_tokens)\n","    x = {k: v.to(flan_device) for k, v in x.items()}\n","\n","    # sampling (no beams) tends to be more diverse than beam search\n","    with torch.no_grad():\n","        out = flan_mdl.generate(\n","            **x,\n","            do_sample=True,\n","            top_k=50,\n","            top_p=0.92,\n","            temperature=0.9,\n","            num_return_sequences=num_return,\n","            max_new_tokens=max_new_tokens,\n","            repetition_penalty=1.1,\n","            no_repeat_ngram_size=3,\n","            early_stopping=True\n","        )\n","\n","    paras = flan_tok.batch_decode(out, skip_special_tokens=True)\n","    # drop clones of the input (case/punct insensitive)\n","    base = re.sub(r\"\\W+\", \" \", query).strip().lower()\n","    seen, uniq = set(), []\n","    for p in paras:\n","        p2 = normalize(p)\n","        p2_cmp = re.sub(r\"\\W+\", \" \", p2).strip().lower()\n","        if p2_cmp == base:\n","            continue\n","        if p2 and p2 not in seen:\n","            seen.add(p2)\n","            uniq.append(p2)\n","    return uniq\n","\n","\n","    # Try full settings, then back off on OOM or other generation errors\n","    try:\n","        return _generate(num_return, max_new_tokens, num_beams)\n","    except RuntimeError as e:\n","        if \"out of memory\" in str(e).lower():\n","            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n","            try:\n","                # lighter retry\n","                return _generate(min(3, num_return), min(32, max_new_tokens), max(4, min(num_beams, 4)))\n","            except Exception:\n","                return []\n","        else:\n","            # any other transient error: one lighter retry\n","            try:\n","                return _generate(min(3, num_return), min(32, max_new_tokens), max(4, num_beams))\n","            except Exception:\n","                return []\n","    except Exception:\n","        return []"]},{"cell_type":"code","source":["TICKER_STOPWORDS = {\"THE\",\"AND\",\"FOR\",\"WITH\",\"WHAT\",\"NEW\",\"RISKS\",\"DID\",\"IN\",\"OF\",\"ON\",\"BY\",\"LAST\",\"VS\",\"S\",\"DATA\"}\n","\n","def extract_entities(raw_text: str) -> Dict[str, List[str]]:\n","    out: Dict[str, List[str]] = {}\n","\n","    # Quarter/Year (from lower)\n","    low = raw_text.lower()\n","    q = re.findall(r\"\\b(q[1-4])\\s*([12][0-9]{3})\\b\", low)\n","    if q:\n","        out[\"quarter\"] = [f\"{p.upper()} {y}\" for p, y in q]\n","\n","    years = re.findall(r\"\\b(20[0-4][0-9]|19[0-9]{2})\\b\", raw_text)\n","    if years:\n","        out.setdefault(\"year\", sorted(set(years)))\n","\n","    # Tickers ONLY from raw patterns: $TSLA, (TSLA), NASDAQ: TSLA, NYSE: TSLA, or clean ALL-CAPS (stopwords filtered)\n","    cands = set()\n","    cands.update(re.findall(r\"\\$([A-Z]{1,5})\\b\", raw_text))\n","    cands.update(re.findall(r\"\\(([A-Z]{1,5})\\)\", raw_text))\n","    cands.update(re.findall(r\"\\b(?:NASDAQ|NYSE)\\s*:\\s*([A-Z]{1,5})\\b\", raw_text))\n","    for m in re.findall(r\"\\b[A-Z]{2,5}\\b\", raw_text):\n","        if m not in TICKER_STOPWORDS:\n","            cands.add(m)\n","    if cands:\n","        out[\"ticker\"] = sorted(cands)\n","\n","    return out\n","\n","# quick probe\n","print(extract_entities(\"What new cyber risks did Tesla disclose in Q2 2024?\"))\n","# expect: {'quarter': ['Q2 2024'], 'year': ['2024']} (no ticker)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7m3cLplO5Lp0","executionInfo":{"status":"ok","timestamp":1762696903301,"user_tz":-480,"elapsed":13,"user":{"displayName":"Calmen Cher","userId":"08862292980448501589"}},"outputId":"c04ce0a4-fa83-45f9-9adf-a6cfef469a35"},"id":"7m3cLplO5Lp0","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'quarter': ['Q2 2024'], 'year': ['2024']}\n"]}]},{"cell_type":"markdown","id":"6f3f296f","metadata":{"id":"6f3f296f"},"source":["## 4. Intent Classification with FinBERT (adapter or fine-tune)"]},{"cell_type":"code","execution_count":null,"id":"d7d79ba0","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d7d79ba0","executionInfo":{"status":"ok","timestamp":1762696903371,"user_tz":-480,"elapsed":53,"user":{"displayName":"Calmen Cher","userId":"08862292980448501589"}},"outputId":"3b2ef993-ce11-47b5-bf0d-0992900327b1"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n","  warnings.warn(\n"]}],"source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.pipeline import Pipeline\n","from typing import Tuple\n","\n","INTENT_LABELS = [\"risk\",\"performance\",\"strategy\"]\n","\n","# tiny seed so it actually trains; add more later\n","X_train = [\n","    \"What new risk factors were disclosed?\",\n","    \"Cybersecurity breach details for Tesla\",\n","    \"Explain Apple revenue growth and margins\",\n","    \"Compare Microsoft profit guidance last quarter\",\n","    \"Outline Nvidia expansion strategy in data centers\",\n","    \"What restructuring plan is management proposing?\"\n","]\n","y_train = [\"risk\",\"risk\",\"performance\",\"performance\",\"strategy\",\"strategy\"]\n","\n","intent_clf = Pipeline([\n","    (\"tfidf\", TfidfVectorizer(ngram_range=(1,2), min_df=1)),\n","    (\"lr\", LogisticRegression(max_iter=300, class_weight=\"balanced\", multi_class=\"ovr\"))\n","]).fit(X_train, y_train)\n","\n","RISK_KW = {\"risk\",\"risk factor\",\"risk factors\",\"uncertainty\",\"cyber\",\"cybersecurity\",\"breach\",\"litigation\",\"security\"}\n","PERF_KW = {\"revenue\",\"growth\",\"margin\",\"profit\",\"loss\",\"guidance\",\"results\",\"compare\",\"last quarter\",\"quarterly\"}\n","STRAT_KW= {\"strategy\",\"plan\",\"roadmap\",\"expansion\",\"acquisition\",\"restructuring\",\"capex\",\"data center\",\"data centers\"}\n","\n","def _kw_score(t: str, kws: set[str]) -> int:\n","    return sum(1 for k in kws if k in t)\n","\n","def classify_intent(text: str) -> Tuple[str, float]:\n","    tx = normalize(text)\n","    proba = intent_clf.predict_proba([tx])[0].tolist()  # [risk, perf, strat] due to fit order\n","    k_r = _kw_score(tx, RISK_KW); k_p = _kw_score(tx, PERF_KW); k_s = _kw_score(tx, STRAT_KW)\n","    k_sum = max(1, (k_r + k_p + k_s))\n","    priors = [k_r/k_sum, k_p/k_sum, k_s/k_sum]\n","    alpha, beta = 0.6, 0.4\n","    blended = [alpha*proba[i] + beta*priors[i] for i in range(3)]\n","    s = sum(blended) or 1.0\n","    blended = [b/s for b in blended]\n","    idx = max(range(3), key=lambda i: blended[i])\n","    return INTENT_LABELS[idx], float(blended[idx])\n"]},{"cell_type":"markdown","id":"5d9f1f4d","metadata":{"id":"5d9f1f4d"},"source":["### 4.A Optional: Fine-tune FinBERT with HF Trainer"]},{"cell_type":"code","execution_count":null,"id":"b5371c44","metadata":{"id":"b5371c44"},"outputs":[],"source":["\n","# Uncomment to fine-tune FinBERT. After training, replace classify_intent with a wrapper\n","# that calls the trained model to get label and confidence.\n","\n","# from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n","# from datasets import Dataset\n","# import numpy as np, torch\n","# tokenizer = AutoTokenizer.from_pretrained(FINBERT_MODEL_NAME)\n","# model = AutoModelForSequenceClassification.from_pretrained(FINBERT_MODEL_NAME, num_labels=len(INTENT_LABELS))\n","# # Build Dataset from your dataframe df with columns 'text' and 'label'.\n","# # Train and evaluate, then define:\n","# # def finbert_predict(text: str) -> Tuple[str, float]: ...\n","# # classify_intent = finbert_predict\n"]},{"cell_type":"markdown","id":"4ff2a2d5","metadata":{"id":"4ff2a2d5"},"source":["## 5. Expansion functions (lexical, semantic, paraphrase)"]},{"cell_type":"code","execution_count":null,"id":"05794ea3","metadata":{"id":"05794ea3"},"outputs":[],"source":["def keyword_expand(tokens: list[str]) -> list[str]:\n","    ex = []\n","    for t in tokens:\n","        t0 = t.strip(\".-\").lower()\n","        ex.extend(DOMAIN_SYNONYMS.get(t0, []))\n","    # dedup preserve order\n","    seen, out = set(), []\n","    for w in ex:\n","        if w not in seen:\n","            seen.add(w); out.append(w)\n","    return out\n","\n","def build_keywords(tokens: list[str], expansions: list[str]) -> list[str]:\n","    kept = []\n","    for t in tokens + expansions:\n","        if re.search(r\"[a-z0-9]\", t) and t not in kept:\n","            kept.append(t.lower())\n","    return kept\n","\n","def expand_query(query: str) -> dict:\n","    norm = normalize(query)\n","    toks = simple_tokenize(norm)\n","\n","    # lexical expansions\n","    lex_ex = keyword_expand(toks)\n","\n","    # T5 paraphrases (diverse)\n","    paras = t5_paraphrases_safe(norm, num_return=5, max_new_tokens=48) if USE_FLAN_T5 else []\n","\n","    # mine paraphrase tokens → expand via domain synonyms again\n","    para_tokens = []\n","    for p in paras:\n","        para_tokens.extend(simple_tokenize(p))\n","    para_tokens = list(dict.fromkeys(para_tokens))\n","    para_ex = keyword_expand(para_tokens) if para_tokens else []\n","\n","    # merge\n","    expansions = []\n","    for lst in (lex_ex, para_ex):\n","        for w in lst:\n","            if w not in expansions:\n","                expansions.append(w)\n","\n","    keywords = build_keywords(toks, expansions)\n","    return {\n","        \"normalized\": norm,\n","        \"tokens\": toks,\n","        \"expansions\": expansions,\n","        \"paraphrases\": paras,\n","        \"keywords\": keywords\n","    }\n"]},{"cell_type":"markdown","id":"327c567f","metadata":{"id":"327c567f"},"source":["## 6. QueryProcessor class"]},{"cell_type":"code","execution_count":null,"id":"add17266","metadata":{"id":"add17266"},"outputs":[],"source":["from dataclasses import dataclass, field\n","from typing import List, Dict, Any, Optional\n","\n","@dataclass\n","class QueryProcessorConfig:\n","    labels: List[str] = field(default_factory=lambda: [\"risk\",\"performance\",\"strategy\"])\n","    use_flan_t5: bool = False\n","    use_sbert: bool = False\n","\n","class QueryProcessor:\n","    def __init__(self, config: QueryProcessorConfig = QueryProcessorConfig()):\n","        self.config = config\n","\n","    def process(self, query: str) -> Dict[str, Any]:\n","        ex = expand_query(query)\n","        ents = extract_entities(ex[\"normalized\"])\n","        label, conf = classify_intent(ex[\"normalized\"])\n","        emb = sbert_embed(ex[\"normalized\"]) if USE_SBERT else None\n","        return {\n","            \"normalized\": ex[\"normalized\"],\n","            \"label\": label,\n","            \"confidence\": conf,\n","            \"expansions\": ex[\"expansions\"],\n","            \"paraphrases\": ex[\"paraphrases\"],\n","            \"keywords\": ex[\"keywords\"],\n","            \"entities\": ents,\n","            \"filters\": ents.copy(),\n","            \"embedding\": emb\n","        }\n"]},{"cell_type":"markdown","id":"af54ef2e","metadata":{"id":"af54ef2e"},"source":["## 7. Smoke tests"]},{"cell_type":"code","execution_count":null,"id":"d6d9e21e","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d6d9e21e","executionInfo":{"status":"ok","timestamp":1762697261342,"user_tz":-480,"elapsed":8948,"user":{"displayName":"Calmen Cher","userId":"08862292980448501589"}},"outputId":"b80e2ded-a240-4b79-a400-2e3eed5e2dd4"},"outputs":[{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Q: What new cyber risks did Tesla disclose in Q2 2024?\n","Label: risk Conf: 0.577\n","Filters: {'quarter': ['Q2 2024'], 'year': ['2024']}\n","Top keywords: ['what', 'new', 'cyber', 'risks', 'did', 'tesla', 'disclose', 'in', 'q2', '2024', 'cybersecurity', 'information security']\n","Paraphrase sample: ['What new cyber risks did Tesla reveal in Q2 2024?']\n","Embedding: present\n","\n","Q: Compare Apple's revenue growth vs Microsoft last quarter\n","Label: performance Conf: 0.57\n","Filters: {}\n","Top keywords: ['compare', 'apple', 's', 'revenue', 'growth', 'vs', 'microsoft', 'last', 'quarter']\n","Paraphrase sample: [\"Apple's revenue growth was 35% higher last quarter compared to Microsoft's.\"]\n","Embedding: present\n","\n","Q: Outline Nvidia's expansion strategy in data centers\n","Label: strategy Conf: 0.662\n","Filters: {}\n","Top keywords: ['outline', 'nvidia', 's', 'expansion', 'strategy', 'in', 'data', 'centers', 'roadmap', 'plan', 'initiative', 'capex']\n","Paraphrase sample: [\"The user can find Nvidia's data center expansion strategy in the following pages.\"]\n","Embedding: present\n"]}],"source":["qp = QueryProcessor()\n","\n","tests = [\n","    \"What new cyber risks did Tesla disclose in Q2 2024?\",\n","    \"Compare Apple's revenue growth vs Microsoft last quarter\",\n","    \"Outline Nvidia's expansion strategy in data centers\"\n","]\n","\n","for t in tests:\n","    out = qp.process(t)\n","    print(\"\\nQ:\", t)\n","    print(\"Label:\", out[\"label\"], \"Conf:\", round(out[\"confidence\"], 3))\n","    print(\"Filters:\", out[\"filters\"])\n","    print(\"Top keywords:\", out[\"keywords\"][:12])\n","    print(\"Paraphrase sample:\", (out[\"paraphrases\"][:1] if out[\"paraphrases\"] else []))\n","    print(\"Embedding:\", \"present\" if out[\"embedding\"] is not None else \"None\")\n"]},{"cell_type":"markdown","id":"5da0dc70","metadata":{"id":"5da0dc70"},"source":["## 8. Integration into RAG"]},{"cell_type":"code","source":["# === write query_processor.py (importable) ===\n","from textwrap import dedent\n","\n","code = dedent(r'''\n","# Lightweight export of your QueryProcessor so other notebooks can import it\n","import re, unicodedata\n","from dataclasses import dataclass, field\n","from typing import List, Dict, Any, Optional, Tuple\n","\n","# ---- toggles (you can flip them later from base_RAG if needed)\n","USE_SBERT   = True\n","USE_FLAN_T5 = True\n","\n","def normalize(text: str) -> str:\n","    text = unicodedata.normalize(\"NFKC\", text).strip()\n","    text = re.sub(r\"\\s+\", \" \", text)\n","    return text\n","\n","TOKEN_RE = re.compile(r\"[A-Za-z0-9]+(?:'[A-Za-z0-9]+)?|[&$%.\\-]+\")\n","def simple_tokenize(text: str) -> List[str]:\n","    toks = TOKEN_RE.findall(text)\n","    cleaned = []\n","    for t in toks:\n","        if t.lower() == \"'s\": continue\n","        if t.endswith(\"'s\"): t = t[:-2]\n","        cleaned.append(t)\n","    return cleaned\n","\n","DOMAIN_SYNONYMS = {\n","    \"risk\": [\"risk factor\",\"risk factors\",\"uncertainty\",\"exposure\",\"threat\"],\n","    \"cyber\": [\"cybersecurity\",\"information security\",\"infosec\",\"data breach\",\"security incident\"],\n","    \"performance\": [\"revenue\",\"growth\",\"margin\",\"profit\",\"loss\",\"guidance\",\"results\"],\n","    \"strategy\": [\"roadmap\",\"plan\",\"initiative\",\"expansion\",\"capex\",\"restructuring\",\"acquisition\"],\n","    \"md&a\": [\"management discussion\",\"md&a\",\"results of operations\"],\n","}\n","\n","def keyword_expand(tokens: List[str]) -> List[str]:\n","    ex = []\n","    for t in tokens:\n","        t0 = t.strip(\".-\").lower()\n","        ex.extend(DOMAIN_SYNONYMS.get(t0, []))\n","    seen, out = set(), []\n","    for w in ex:\n","        if w not in seen:\n","            seen.add(w); out.append(w)\n","    return out\n","\n","def build_keywords(tokens: List[str], expansions: List[str]) -> List[str]:\n","    kept = []\n","    for t in tokens + expansions:\n","        t = t.lower()\n","        if not re.search(r\"[a-z0-9]\", t):\n","            continue\n","        if t not in kept:\n","            kept.append(t)\n","    return kept\n","\n","COMPANY_TICKERS = {\"tesla\":\"TSLA\",\"apple\":\"AAPL\",\"microsoft\":\"MSFT\",\"nvidia\":\"NVDA\"}\n","\n","# --- entities (quarter/year/company/ticker) ---\n","try:\n","    import spacy\n","    _nlp = spacy.load(\"en_core_web_sm\")\n","except Exception:\n","    _nlp = None\n","\n","def extract_entities(raw_text: str) -> dict:\n","    out = {}\n","    low = raw_text.lower()\n","    q = re.findall(r\"\\b(q[1-4])\\s*([12][0-9]{3})\\b\", low)\n","    if q: out[\"quarter\"] = [f\"{p.upper()} {y}\" for p, y in q]\n","    years = re.findall(r\"\\b(20[0-4][0-9]|19[0-9]{2})\\b\", raw_text)\n","    if years: out[\"year\"] = sorted(set(years))\n","    companies = set()\n","    if _nlp is not None:\n","        doc = _nlp(raw_text)\n","        for ent in doc.ents:\n","            if ent.label_ == \"ORG\":\n","                companies.add(ent.text.strip())\n","    low_raw = raw_text.lower()\n","    for name in COMPANY_TICKERS:\n","        if name in low_raw: companies.add(name.title())\n","    if companies: out[\"company\"] = sorted(companies)\n","    tickers = set(COMPANY_TICKERS.get(c.lower(),\"\") for c in companies if COMPANY_TICKERS.get(c.lower()))\n","    tickers.update(re.findall(r\"\\$([A-Z]{1,5})\\b\", raw_text))\n","    tickers.update(re.findall(r\"\\(([A-Z]{1,5})\\)\", raw_text))\n","    tickers.update(re.findall(r\"\\b(?:NASDAQ|NYSE)\\s*:\\s*([A-Z]{1,5})\\b\", raw_text))\n","    tickers = {t for t in tickers if t}\n","    if tickers: out[\"ticker\"] = sorted(tickers)\n","    return out\n","\n","# --- SBERT embedding (matches base_RAG Config) ---\n","try:\n","    from sentence_transformers import SentenceTransformer\n","    _sbert = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\") if USE_SBERT else None\n","except Exception:\n","    _sbert = None\n","\n","def sbert_embed(text: str) -> Optional[List[float]]:\n","    if _sbert is None: return None\n","    v = _sbert.encode([text], normalize_embeddings=True)[0]\n","    return v.tolist()\n","\n","# --- Flan-T5 paraphrasing (optional) ---\n","try:\n","    import torch\n","    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","    _flan_device = \"cuda\" if USE_FLAN_T5 and torch.cuda.is_available() else \"cpu\"\n","    _flan_tok = AutoTokenizer.from_pretrained(\"google/flan-t5-small\") if USE_FLAN_T5 else None\n","    _flan_mdl = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\").to(_flan_device).eval() if USE_FLAN_T5 else None\n","except Exception:\n","    _flan_tok = _flan_mdl = None\n","    _flan_device = \"cpu\"\n","\n","def t5_paraphrases_safe(q: str, num_return: int = 5, max_new_tokens: int = 48) -> List[str]:\n","    if not (USE_FLAN_T5 and _flan_tok is not None and _flan_mdl is not None): return []\n","    import torch, re\n","    prompt = (\"Rewrite the query into multiple short paraphrases without adding facts or numbers. \"\n","              \"Keep meaning; avoid speculation or meta text.\\nQuery: \" + q)\n","    x = _flan_tok(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n","    x = {k: v.to(_flan_device) for k, v in x.items()}\n","    with torch.no_grad():\n","        out = _flan_mdl.generate(\n","            **x, do_sample=True, top_k=50, top_p=0.92, temperature=0.9,\n","            num_return_sequences=num_return, max_new_tokens=max_new_tokens,\n","            repetition_penalty=1.1, no_repeat_ngram_size=3\n","        )\n","    paras = _flan_tok.batch_decode(out, skip_special_tokens=True)\n","    base = re.sub(r\"\\W+\",\" \", q).strip().lower()\n","    seen, kept = set(), []\n","    for p in paras:\n","        p2 = normalize(p)\n","        p2_cmp = re.sub(r\"\\W+\",\" \", p2).strip().lower()\n","        if p2_cmp == base: continue\n","        if p2 and p2 not in seen:\n","            seen.add(p2); kept.append(p2)\n","    return kept[:num_return]\n","\n","# --- intent (hybrid)\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.pipeline import Pipeline\n","INTENT_LABELS = [\"risk\",\"performance\",\"strategy\"]\n","X_train = [\n","    \"What new risk factors were disclosed?\",\n","    \"Cybersecurity breach details for Tesla\",\n","    \"Explain Apple revenue growth and margins\",\n","    \"Compare Microsoft profit guidance last quarter\",\n","    \"Outline Nvidia expansion strategy in data centers\",\n","    \"What restructuring plan is management proposing?\"\n","]\n","y_train = [\"risk\",\"risk\",\"performance\",\"performance\",\"strategy\",\"strategy\"]\n","_intent_clf = Pipeline([\n","    (\"tfidf\", TfidfVectorizer(ngram_range=(1,2), min_df=1)),\n","    (\"lr\", LogisticRegression(max_iter=300, class_weight=\"balanced\", multi_class=\"ovr\"))\n","]).fit(X_train, y_train)\n","\n","RISK_KW = {\"risk\",\"risk factor\",\"risk factors\",\"uncertainty\",\"cyber\",\"cybersecurity\",\"breach\",\"litigation\",\"security\"}\n","PERF_KW = {\"revenue\",\"growth\",\"margin\",\"profit\",\"loss\",\"guidance\",\"results\",\"compare\",\"last quarter\",\"quarterly\"}\n","STRAT_KW= {\"strategy\",\"plan\",\"roadmap\",\"expansion\",\"acquisition\",\"restructuring\",\"capex\",\"data center\",\"data centers\"}\n","\n","def _kw_score(t: str, kws: set[str]) -> int:\n","    return sum(1 for k in kws if k in t)\n","\n","def classify_intent(text: str) -> Tuple[str, float]:\n","    tx = normalize(text)\n","    proba = _intent_clf.predict_proba([tx])[0].tolist()\n","    k_r = _kw_score(tx, RISK_KW); k_p = _kw_score(tx, PERF_KW); k_s = _kw_score(tx, STRAT_KW)\n","    k_sum = max(1, (k_r + k_p + k_s))\n","    priors = [k_r/k_sum, k_p/k_sum, k_s/k_sum]\n","    alpha, beta = 0.6, 0.4\n","    blended = [alpha*proba[i] + beta*priors[i] for i in range(3)]\n","    s = sum(blended) or 1.0\n","    blended = [b/s for b in blended]\n","    idx = max(range(3), key=lambda i: blended[i])\n","    return INTENT_LABELS[idx], float(blended[idx])\n","\n","def expand_query(query: str) -> dict:\n","    norm = normalize(query)\n","    toks = simple_tokenize(norm)\n","    lex_ex = keyword_expand(toks)\n","    paras = t5_paraphrases_safe(norm, num_return=5, max_new_tokens=48) if USE_FLAN_T5 else []\n","    para_tokens = []\n","    for p in paras:\n","        para_tokens.extend(simple_tokenize(p))\n","    para_tokens = list(dict.fromkeys(para_tokens))\n","    para_ex = keyword_expand(para_tokens) if para_tokens else []\n","    expansions = []\n","    for lst in (lex_ex, para_ex):\n","        for w in lst:\n","            if w not in expansions:\n","                expansions.append(w)\n","    return {\n","        \"normalized\": norm,\n","        \"tokens\": toks,\n","        \"expansions\": expansions,\n","        \"paraphrases\": paras,\n","        \"keywords\": build_keywords(toks, expansions)\n","    }\n","\n","@dataclass\n","class QueryProcessorConfig:\n","    labels: List[str] = field(default_factory=lambda: [\"risk\",\"performance\",\"strategy\"])\n","\n","class QueryProcessor:\n","    def __init__(self, config: QueryProcessorConfig = QueryProcessorConfig()):\n","        self.config = config\n","    def process(self, query: str) -> Dict[str, Any]:\n","        raw = query\n","        ex  = expand_query(query)\n","        ents = extract_entities(raw)\n","        label, conf = classify_intent(ex[\"normalized\"])\n","        emb = sbert_embed(ex[\"normalized\"]) if USE_SBERT else None\n","        return {\n","            \"normalized\": ex[\"normalized\"],\n","            \"label\": label,\n","            \"confidence\": conf,\n","            \"expansions\": ex[\"expansions\"],\n","            \"paraphrases\": ex[\"paraphrases\"],\n","            \"keywords\": ex[\"keywords\"],\n","            \"entities\": ents,\n","            \"filters\": ents.copy(),\n","            \"embedding\": emb\n","        }\n","''')\n","\n","with open(\"query_processor.py\", \"w\", encoding=\"utf-8\") as f:\n","    f.write(code)\n","\n","print(\"✅ query_processor.py written\")\n"],"metadata":{"id":"mmAoPR-1CFeG","executionInfo":{"status":"ok","timestamp":1762698747538,"user_tz":-480,"elapsed":14,"user":{"displayName":"Calmen Cher","userId":"08862292980448501589"}},"outputId":"6f13ec86-aa57-41ca-fb2e-67eb651ac3b2","colab":{"base_uri":"https://localhost:8080/"}},"id":"mmAoPR-1CFeG","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ query_processor.py written\n"]}]},{"cell_type":"code","execution_count":null,"id":"31bd92a3","metadata":{"id":"31bd92a3"},"outputs":[],"source":["\n","def build_metadata_filter(filters: Dict[str, List[str]]):\n","    try:\n","        from qdrant_client.http.models import Filter, FieldCondition, MatchAny\n","        must = []\n","        if \"ticker\" in filters:\n","            must.append(FieldCondition(key=\"ticker\", match=MatchAny(any=filters[\"ticker\"])))\n","        if \"quarter\" in filters:\n","            must.append(FieldCondition(key=\"quarter\", match=MatchAny(any=filters[\"quarter\"])))\n","        return Filter(must=must) if must else None\n","    except Exception:\n","        return None\n","\n","def rag_retrieve(user_query: str, top_k: int = 20):\n","    q = qp.process(user_query)\n","    filt = build_metadata_filter(q[\"filters\"])\n","    embedding = q[\"embedding\"]\n","    terms = q[\"keywords\"]\n","\n","    vec_hits = []\n","    if embedding is not None:\n","        # vec_hits = qdrant_manager.search_vector(embedding, top_k=top_k, metadata_filter=filt)\n","        pass\n","\n","    bm25_hits = []\n","    # bm25_hits = es.search(terms, top_k=top_k, filters=q[\"filters\"])\n","\n","    hits = vec_hits + bm25_hits\n","    return q, hits\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.13.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}