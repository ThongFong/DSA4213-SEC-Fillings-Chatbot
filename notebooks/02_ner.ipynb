{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWUZP8p5aWtZ",
        "outputId": "f7727c71-1624-49a5-c8e7-1b81838bb0ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sec_api in /usr/local/lib/python3.12/dist-packages (1.0.32)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (5.4.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.10)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.10.5)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (2.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-lg==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.8.0/en_core_web_lg-3.8.0-py3-none-any.whl (400.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.7/400.7 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n"
          ]
        }
      ],
      "source": [
        "## Cell 1: Install Libraries\n",
        "# Install all required packages\n",
        "\n",
        "!pip install sec_api requests beautifulsoup4 lxml\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_lg\n",
        "!pip install transformers torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "legIxApaacqN"
      },
      "outputs": [],
      "source": [
        "## Cell 2: Imports\n",
        "import requests\n",
        "import re\n",
        "from bs4 import BeautifulSoup, NavigableString\n",
        "from collections import defaultdict\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from transformers import pipeline\n",
        "import json\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8jl-PWEqag4U"
      },
      "outputs": [],
      "source": [
        "## Cell 3: Data Loading Functions (Same as before)\n",
        "# --- Configuration ---\n",
        "HEADERS = {'User-Agent': 'MultiModelNLP-SEC-RAG-Project zhiminng03@gmail.com'}\n",
        "CIK_MAP_URL = 'https://www.sec.gov/files/company_tickers.json'\n",
        "\n",
        "def get_latest_10q(ticker: str) -> str:\n",
        "    \"\"\"Fetches the HTML content of the latest 10-Q filing for a given stock ticker.\"\"\"\n",
        "    print(f\"1. Fetching CIK for ticker: {ticker}...\")\n",
        "    response = requests.get(CIK_MAP_URL, headers=HEADERS)\n",
        "    response.raise_for_status()\n",
        "    company_data = response.json()\n",
        "\n",
        "    cik = None\n",
        "    for company in company_data.values():\n",
        "        if company['ticker'] == ticker.upper():\n",
        "            cik = str(company['cik_str']).zfill(10)\n",
        "            break\n",
        "\n",
        "    if not cik:\n",
        "        raise ValueError(f\"Ticker '{ticker}' not found in SEC CIK mapping.\")\n",
        "\n",
        "    print(f\"   Found CIK: {cik}\")\n",
        "\n",
        "    print(\"2. Fetching submission history from SEC EDGAR...\")\n",
        "    submissions_url = f\"https://data.sec.gov/submissions/CIK{cik}.json\"\n",
        "    response = requests.get(submissions_url, headers=HEADERS)\n",
        "    response.raise_for_status()\n",
        "    submissions = response.json()\n",
        "\n",
        "    latest_10q = None\n",
        "    for i, form in enumerate(submissions['filings']['recent']['form']):\n",
        "        if form == '10-Q':\n",
        "            accession_number = submissions['filings']['recent']['accessionNumber'][i]\n",
        "            primary_document = submissions['filings']['recent']['primaryDocument'][i]\n",
        "            filing_date = submissions['filings']['recent']['filingDate'][i]\n",
        "            latest_10q = {\n",
        "                'accession_number': accession_number.replace('-', ''),\n",
        "                'primary_document': primary_document,\n",
        "                'date': filing_date\n",
        "            }\n",
        "            break\n",
        "\n",
        "    if not latest_10q:\n",
        "        raise ValueError(f\"No recent 10-Q filings found for ticker '{ticker}'.\")\n",
        "\n",
        "    print(f\"   Found latest 10-Q filed on: {latest_10q['date']}\")\n",
        "\n",
        "    filing_url = (\n",
        "        f\"https://www.sec.gov/Archives/edgar/data/{cik}/\"\n",
        "        f\"{latest_10q['accession_number']}/{latest_10q['primary_document']}\"\n",
        "    )\n",
        "\n",
        "    print(f\"3. Fetching 10-Q document from: {filing_url}\")\n",
        "    response = requests.get(filing_url, headers=HEADERS)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    print(\"   Successfully fetched document.\")\n",
        "    return response.text\n",
        "\n",
        "def _normalize_header_text(text: str) -> str | None:\n",
        "    \"\"\"Normalizes header text to a standard format.\"\"\"\n",
        "    text = text.strip().upper()\n",
        "    part_match = re.search(r'^\\s*(PART\\s+I{1,2})', text)\n",
        "    if part_match:\n",
        "        return re.sub(r'\\s+', ' ', part_match.group(1))\n",
        "    item_match = re.search(r'^\\s*(ITEM\\s+\\d[A-Z]?)', text)\n",
        "    if item_match:\n",
        "        return re.sub(r'\\s+', ' ', item_match.group(1))\n",
        "    return None\n",
        "\n",
        "def _parse_html_table(table_tag: BeautifulSoup) -> str:\n",
        "    \"\"\"Converts a BeautifulSoup table Tag into a Markdown formatted string.\"\"\"\n",
        "    markdown_rows = []\n",
        "    for tr in table_tag.find_all('tr'):\n",
        "        cells = [\" \".join(cell.get_text(strip=True).split()) for cell in tr.find_all(['td', 'th'])]\n",
        "        if any(cells):\n",
        "            markdown_rows.append(cells)\n",
        "    if not markdown_rows:\n",
        "        return \"\"\n",
        "    md_output = []\n",
        "    header = markdown_rows[0]\n",
        "    md_output.append(\"| \" + \" | \".join(header) + \" |\")\n",
        "    md_output.append(\"| \" + \" | \".join(['---'] * len(header)) + \" |\")\n",
        "    for row in markdown_rows[1:]:\n",
        "        while len(row) < len(header):\n",
        "            row.append(\"\")\n",
        "        row = row[:len(header)]\n",
        "        md_output.append(\"| \" + \" | \".join(row) + \" |\")\n",
        "    return \"\\n\" + \"\\n\".join(md_output) + \"\\n\"\n",
        "\n",
        "def parse_10q(html_content: str) -> dict:\n",
        "    \"\"\"Parses the HTML of a 10-Q filing to extract Parts and Items.\"\"\"\n",
        "    print(\"4. Parsing HTML content...\")\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "    potential_headers = soup.find_all(['p', 'b', 'strong', 'div'])\n",
        "    doc_headers = []\n",
        "    for header in potential_headers:\n",
        "        text = header.get_text(strip=True)\n",
        "        if len(text) > 100:\n",
        "            continue\n",
        "        normalized_key = _normalize_header_text(text)\n",
        "        if normalized_key:\n",
        "            if not header.find_parent('a'):\n",
        "                doc_headers.append({'tag': header, 'key': normalized_key})\n",
        "\n",
        "    if not doc_headers:\n",
        "        print(\"   Warning: Could not find any standard Part/Item headers.\")\n",
        "        return {}\n",
        "\n",
        "    parsed_data = defaultdict(lambda: defaultdict(str))\n",
        "    current_part_key = None\n",
        "\n",
        "    for i, header_info in enumerate(doc_headers):\n",
        "        current_key = header_info['key']\n",
        "        if 'PART' in current_key:\n",
        "            current_part_key = current_key\n",
        "            continue\n",
        "        if 'ITEM' in current_key:\n",
        "            if not current_part_key:\n",
        "                current_part_key = \"PART I\"\n",
        "            start_node = header_info['tag']\n",
        "            end_node = doc_headers[i + 1]['tag'] if i + 1 < len(doc_headers) else None\n",
        "            content_parts = []\n",
        "            element = start_node.next_element\n",
        "            while element and element != end_node:\n",
        "                if isinstance(element, NavigableString):\n",
        "                    if not element.find_parent('table'):\n",
        "                        text = element.strip()\n",
        "                        if text:\n",
        "                            content_parts.append(text)\n",
        "                elif element.name == 'table':\n",
        "                    if not element.find_parent('table'):\n",
        "                         table_markdown = _parse_html_table(element)\n",
        "                         if table_markdown:\n",
        "                            content_parts.append(table_markdown)\n",
        "                element = element.next_element\n",
        "\n",
        "            full_content = \"\\n\".join(content_parts)\n",
        "            clean_content = re.sub(r'\\n{3,}', '\\n\\n', full_content).strip()\n",
        "            parsed_data[current_part_key][current_key] = clean_content\n",
        "\n",
        "    print(\"   Parsing complete.\")\n",
        "    return {part: dict(items) for part, items in parsed_data.items()}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KtpNeTlxalVd"
      },
      "outputs": [],
      "source": [
        "## Cell 4: Enhanced Knowledge Base and Normalization Mappings\n",
        "\n",
        "class FinancialKnowledgeBase:\n",
        "    \"\"\"\n",
        "    Comprehensive knowledge base for financial entity normalization and linking.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Build company mapping from SEC data\n",
        "        self.company_to_ticker = self._build_company_ticker_map()\n",
        "\n",
        "        # Financial metrics normalization\n",
        "        self.financial_metrics = {\n",
        "            # Revenue variants\n",
        "            \"revenue\": \"REVENUE\",\n",
        "            \"revenues\": \"REVENUE\",\n",
        "            \"total revenue\": \"REVENUE\",\n",
        "            \"total revenues\": \"REVENUE\",\n",
        "            \"net revenue\": \"REVENUE\",\n",
        "            \"sales\": \"REVENUE\",\n",
        "            \"total sales\": \"REVENUE\",\n",
        "\n",
        "            # Income variants\n",
        "            \"net income\": \"NET_INCOME\",\n",
        "            \"net earnings\": \"NET_INCOME\",\n",
        "            \"profit\": \"NET_INCOME\",\n",
        "            \"net profit\": \"NET_INCOME\",\n",
        "            \"earnings\": \"NET_INCOME\",\n",
        "            \"income\": \"NET_INCOME\",\n",
        "\n",
        "            # Operating metrics\n",
        "            \"operating income\": \"OPERATING_INCOME\",\n",
        "            \"operating profit\": \"OPERATING_INCOME\",\n",
        "            \"ebit\": \"EBIT\",\n",
        "            \"ebitda\": \"EBITDA\",\n",
        "            \"operating margin\": \"OPERATING_MARGIN\",\n",
        "            \"gross profit\": \"GROSS_PROFIT\",\n",
        "            \"gross margin\": \"GROSS_MARGIN\",\n",
        "\n",
        "            # Cash flow\n",
        "            \"cash flow\": \"CASH_FLOW\",\n",
        "            \"operating cash flow\": \"OPERATING_CASH_FLOW\",\n",
        "            \"free cash flow\": \"FREE_CASH_FLOW\",\n",
        "            \"fcf\": \"FREE_CASH_FLOW\",\n",
        "\n",
        "            # Balance sheet items\n",
        "            \"total assets\": \"TOTAL_ASSETS\",\n",
        "            \"assets\": \"TOTAL_ASSETS\",\n",
        "            \"total liabilities\": \"TOTAL_LIABILITIES\",\n",
        "            \"liabilities\": \"TOTAL_LIABILITIES\",\n",
        "            \"equity\": \"SHAREHOLDERS_EQUITY\",\n",
        "            \"shareholders' equity\": \"SHAREHOLDERS_EQUITY\",\n",
        "            \"stockholders' equity\": \"SHAREHOLDERS_EQUITY\",\n",
        "            \"cash and cash equivalents\": \"CASH_AND_EQUIVALENTS\",\n",
        "            \"cash\": \"CASH_AND_EQUIVALENTS\",\n",
        "\n",
        "            # Per share metrics\n",
        "            \"earnings per share\": \"EPS\",\n",
        "            \"eps\": \"EPS\",\n",
        "            \"diluted eps\": \"DILUTED_EPS\",\n",
        "            \"basic eps\": \"BASIC_EPS\",\n",
        "            \"book value per share\": \"BOOK_VALUE_PER_SHARE\",\n",
        "\n",
        "            # Ratios\n",
        "            \"p/e ratio\": \"PE_RATIO\",\n",
        "            \"price to earnings\": \"PE_RATIO\",\n",
        "            \"debt to equity\": \"DEBT_TO_EQUITY\",\n",
        "            \"current ratio\": \"CURRENT_RATIO\",\n",
        "            \"return on equity\": \"ROE\",\n",
        "            \"roe\": \"ROE\",\n",
        "            \"return on assets\": \"ROA\",\n",
        "            \"roa\": \"ROA\",\n",
        "\n",
        "            # Other\n",
        "            \"market cap\": \"MARKET_CAP\",\n",
        "            \"market capitalization\": \"MARKET_CAP\",\n",
        "            \"dividend\": \"DIVIDEND\",\n",
        "            \"dividend yield\": \"DIVIDEND_YIELD\",\n",
        "        }\n",
        "\n",
        "        # Risk types normalization\n",
        "        self.risk_types = {\n",
        "            \"market risk\": \"MARKET_RISK\",\n",
        "            \"credit risk\": \"CREDIT_RISK\",\n",
        "            \"operational risk\": \"OPERATIONAL_RISK\",\n",
        "            \"liquidity risk\": \"LIQUIDITY_RISK\",\n",
        "            \"interest rate risk\": \"INTEREST_RATE_RISK\",\n",
        "            \"currency risk\": \"CURRENCY_RISK\",\n",
        "            \"foreign exchange risk\": \"CURRENCY_RISK\",\n",
        "            \"commodity risk\": \"COMMODITY_RISK\",\n",
        "            \"regulatory risk\": \"REGULATORY_RISK\",\n",
        "            \"compliance risk\": \"COMPLIANCE_RISK\",\n",
        "            \"legal risk\": \"LEGAL_RISK\",\n",
        "            \"reputational risk\": \"REPUTATIONAL_RISK\",\n",
        "            \"strategic risk\": \"STRATEGIC_RISK\",\n",
        "            \"cybersecurity risk\": \"CYBERSECURITY_RISK\",\n",
        "            \"technology risk\": \"TECHNOLOGY_RISK\",\n",
        "            \"counterparty risk\": \"COUNTERPARTY_RISK\",\n",
        "            \"concentration risk\": \"CONCENTRATION_RISK\",\n",
        "            \"political risk\": \"POLITICAL_RISK\",\n",
        "            \"environmental risk\": \"ENVIRONMENTAL_RISK\",\n",
        "            \"climate risk\": \"CLIMATE_RISK\",\n",
        "        }\n",
        "\n",
        "        # Regex patterns for financial metrics detection\n",
        "        self.metric_patterns = [\n",
        "            (r'\\$\\s*\\d+(?:,\\d{3})*(?:\\.\\d+)?\\s*(?:million|billion|thousand|M|B|K)', 'MONETARY_VALUE'),\n",
        "            (r'\\d+(?:\\.\\d+)?%', 'PERCENTAGE'),\n",
        "            (r'\\d+(?:,\\d{3})*(?:\\.\\d+)?\\s*shares?', 'SHARE_COUNT'),\n",
        "        ]\n",
        "\n",
        "    def _build_company_ticker_map(self):\n",
        "        \"\"\"Build comprehensive company to ticker mapping from SEC data.\"\"\"\n",
        "        try:\n",
        "            response = requests.get(CIK_MAP_URL, headers=HEADERS)\n",
        "            response.raise_for_status()\n",
        "            company_data = response.json()\n",
        "\n",
        "            mapping = {}\n",
        "            for company in company_data.values():\n",
        "                ticker = company['ticker']\n",
        "                title = company['title']\n",
        "\n",
        "                # Add exact company name\n",
        "                mapping[title] = ticker\n",
        "                mapping[title.upper()] = ticker\n",
        "\n",
        "                # Add common variations\n",
        "                # Remove Inc., Corp., etc.\n",
        "                base_name = re.sub(\n",
        "                    r'\\s+(Inc\\.?|Corp\\.?|Corporation|Company|Co\\.?|Ltd\\.?|LLC|L\\.P\\.)\\s*,?',\n",
        "                    '',\n",
        "                    title,\n",
        "                    flags=re.IGNORECASE\n",
        "                ).strip()\n",
        "                mapping[base_name] = ticker\n",
        "                mapping[base_name.upper()] = ticker\n",
        "\n",
        "                # Add ticker itself\n",
        "                mapping[ticker] = ticker\n",
        "                mapping[ticker.upper()] = ticker\n",
        "\n",
        "            print(f\"   Built company mapping with {len(mapping)} entries\")\n",
        "            return mapping\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   Warning: Could not build company map from SEC. Using fallback. Error: {e}\")\n",
        "            # Fallback to major companies\n",
        "            return {\n",
        "                \"Alphabet\": \"GOOGL\", \"Alphabet Inc.\": \"GOOGL\", \"Google\": \"GOOGL\",\n",
        "                \"Microsoft\": \"MSFT\", \"Microsoft Corporation\": \"MSFT\",\n",
        "                \"Tesla\": \"TSLA\", \"Tesla, Inc.\": \"TSLA\",\n",
        "                \"Apple\": \"AAPL\", \"Apple Inc.\": \"AAPL\",\n",
        "                \"Amazon\": \"AMZN\", \"Amazon.com\": \"AMZN\",\n",
        "                \"Meta\": \"META\", \"Meta Platforms\": \"META\", \"Facebook\": \"META\",\n",
        "                \"NVIDIA\": \"NVDA\", \"Nvidia\": \"NVDA\",\n",
        "                \"Berkshire Hathaway\": \"BRK.B\",\n",
        "                \"JPMorgan\": \"JPM\", \"JPMorgan Chase\": \"JPM\",\n",
        "                \"Visa\": \"V\", \"Mastercard\": \"MA\",\n",
        "            }\n",
        "\n",
        "    def normalize_company(self, text):\n",
        "        \"\"\"Normalize company name to ticker.\"\"\"\n",
        "        # Try exact match first\n",
        "        if text in self.company_to_ticker:\n",
        "            return self.company_to_ticker[text]\n",
        "\n",
        "        # Try case-insensitive\n",
        "        text_upper = text.upper()\n",
        "        if text_upper in self.company_to_ticker:\n",
        "            return self.company_to_ticker[text_upper]\n",
        "\n",
        "        # Try fuzzy matching (remove common suffixes)\n",
        "        clean_text = re.sub(\n",
        "            r'\\s+(Inc\\.?|Corp\\.?|Corporation|Company|Co\\.?|Ltd\\.?|LLC)\\s*,?',\n",
        "            '',\n",
        "            text,\n",
        "            flags=re.IGNORECASE\n",
        "        ).strip()\n",
        "        if clean_text in self.company_to_ticker:\n",
        "            return self.company_to_ticker[clean_text]\n",
        "\n",
        "        return None\n",
        "\n",
        "    def normalize_metric(self, text):\n",
        "        \"\"\"Normalize financial metric to standard form.\"\"\"\n",
        "        text_lower = text.lower().strip()\n",
        "        return self.financial_metrics.get(text_lower)\n",
        "\n",
        "    def normalize_risk(self, text):\n",
        "        \"\"\"Normalize risk type to standard form.\"\"\"\n",
        "        text_lower = text.lower().strip()\n",
        "        return self.risk_types.get(text_lower)\n",
        "\n",
        "    def detect_metric_pattern(self, text):\n",
        "        \"\"\"Detect financial metrics using regex patterns.\"\"\"\n",
        "        for pattern, metric_type in self.metric_patterns:\n",
        "            if re.search(pattern, text, re.IGNORECASE):\n",
        "                return metric_type\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "H4ir3cpVaqBv"
      },
      "outputs": [],
      "source": [
        "## Cell 5: Enhanced Entity Extraction and Normalization\n",
        "\n",
        "class FinancialEntityExtractor:\n",
        "    \"\"\"\n",
        "    Enhanced entity extractor with financial domain knowledge.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, knowledge_base):\n",
        "        self.kb = knowledge_base\n",
        "\n",
        "        # Load NER models\n",
        "        print(\"Loading NER models...\")\n",
        "        self.spacy_nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "        # Use a reliable transformer model\n",
        "        try:\n",
        "            self.transformer_ner = pipeline(\n",
        "                \"token-classification\",\n",
        "                model=\"dslim/bert-base-NER\",\n",
        "                aggregation_strategy=\"simple\"\n",
        "            )\n",
        "            print(\"   Transformer model loaded successfully\")\n",
        "        except Exception as e:\n",
        "            print(f\"   Warning: Could not load transformer model: {e}\")\n",
        "            self.transformer_ner = None\n",
        "\n",
        "    def extract_entities_spacy(self, text):\n",
        "        \"\"\"Extract entities using SpaCy.\"\"\"\n",
        "        doc = self.spacy_nlp(text)\n",
        "        entities = []\n",
        "\n",
        "        for ent in doc.ents:\n",
        "            entities.append({\n",
        "                'text': ent.text,\n",
        "                'label': ent.label_,\n",
        "                'start': ent.start_char,\n",
        "                'end': ent.end_char,\n",
        "                'source': 'spacy'\n",
        "            })\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def extract_entities_transformer(self, text):\n",
        "        \"\"\"Extract entities using transformer model.\"\"\"\n",
        "        if not self.transformer_ner:\n",
        "            return []\n",
        "\n",
        "        # Handle long texts by chunking\n",
        "        max_length = 512\n",
        "        chunks = [text[i:i+max_length] for i in range(0, len(text), max_length)]\n",
        "\n",
        "        all_entities = []\n",
        "        offset = 0\n",
        "\n",
        "        for chunk in chunks:\n",
        "            try:\n",
        "                entities = self.transformer_ner(chunk)\n",
        "                for ent in entities:\n",
        "                    all_entities.append({\n",
        "                        'text': ent['word'],\n",
        "                        'label': ent['entity_group'],\n",
        "                        'start': offset + ent.get('start', 0),\n",
        "                        'end': offset + ent.get('end', 0),\n",
        "                        'score': ent.get('score', 1.0),\n",
        "                        'source': 'transformer'\n",
        "                    })\n",
        "            except Exception as e:\n",
        "                print(f\"   Warning: Error processing chunk: {e}\")\n",
        "\n",
        "            offset += len(chunk)\n",
        "\n",
        "        return all_entities\n",
        "\n",
        "    # def extract_financial_metrics_contextual(self, text):\n",
        "    #     \"\"\"Extract financial metrics using contextual patterns.\"\"\"\n",
        "    #     entities = []\n",
        "\n",
        "    #     # Pattern 1: Monetary values\n",
        "    #     monetary_pattern = r'(?:revenues?|sales|income|profit|loss|assets?|liabilities|equity|cash)[\\s\\w]*?(?:of|was|were|totaled|reached)?\\s*\\$\\s*(\\d+(?:,\\d{3})*(?:\\.\\d+)?)\\s*(million|billion|thousand|M|B|K)?'\n",
        "    #     for match in re.finditer(monetary_pattern, text, re.IGNORECASE):\n",
        "    #         entities.append({\n",
        "    #             'text': match.group(0),\n",
        "    #             'label': 'MONETARY_VALUE',\n",
        "    #             'start': match.start(),\n",
        "    #             'end': match.end(),\n",
        "    #             'source': 'pattern'\n",
        "    #         })\n",
        "\n",
        "    #     # Pattern 2: Percentage changes\n",
        "    #     percentage_pattern = r'(?:increased|decreased|grew|declined|rose|fell)[\\s\\w]*?by\\s*(\\d+(?:\\.\\d+)?%)'\n",
        "    #     for match in re.finditer(percentage_pattern, text, re.IGNORECASE):\n",
        "    #         entities.append({\n",
        "    #             'text': match.group(0),\n",
        "    #             'label': 'PERCENTAGE_CHANGE',\n",
        "    #             'start': match.start(),\n",
        "    #             'end': match.end(),\n",
        "    #             'source': 'pattern'\n",
        "    #         })\n",
        "\n",
        "    #     # Pattern 3: Risk mentions\n",
        "    #     risk_pattern = r'((?:\\w+\\s+)?risk(?:s)?(?:\\s+(?:of|related to|associated with|from))?\\s+[\\w\\s]{1,30})'\n",
        "    #     for match in re.finditer(risk_pattern, text, re.IGNORECASE):\n",
        "    #         risk_text = match.group(1).strip()\n",
        "    #         if len(risk_text.split()) <= 6:  # Avoid overly long matches\n",
        "    #             entities.append({\n",
        "    #                 'text': risk_text,\n",
        "    #                 'label': 'RISK_MENTION',\n",
        "    #                 'start': match.start(),\n",
        "    #                 'end': match.end(),\n",
        "    #                 'source': 'pattern'\n",
        "    #             })\n",
        "\n",
        "    #     return entities\n",
        "\n",
        "    def extract_financial_metrics_contextual(self, text):\n",
        "        \"\"\"Extract financial metrics using contextual patterns.\"\"\"\n",
        "        entities = []\n",
        "\n",
        "        # --- MODIFICATION 1 ---\n",
        "        # This pattern is simpler and more robust.\n",
        "        # It looks for:\n",
        "        # 1) A dollar sign followed by a number (e.g., $185.81)\n",
        "        # 2) A number with commas, (e.g., 19,077 or 2,571)\n",
        "        monetary_pattern = r'(\\$\\s*\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?|\\d{1,3}(?:,\\d{3})+)'\n",
        "\n",
        "        for match in re.finditer(monetary_pattern, text):\n",
        "            # Exclude 4-digit numbers without commas (likely years)\n",
        "            if ',' not in match.group(1) and len(match.group(1)) == 4 and '$' not in match.group(1):\n",
        "                continue\n",
        "\n",
        "            entities.append({\n",
        "                'text': match.group(1),\n",
        "                'label': 'MONETARY_VALUE',\n",
        "                'start': match.start(),\n",
        "                'end': match.end(),\n",
        "                'source': 'pattern'\n",
        "            })\n",
        "\n",
        "        # --- MODIFICATION 2 ---\n",
        "        # Build a dynamic risk pattern from your Knowledge Base\n",
        "        # This is much more effective than a generic regex\n",
        "        risk_keys = '|'.join(re.escape(k) for k in self.kb.risk_types.keys())\n",
        "        risk_pattern = rf'({risk_keys})'\n",
        "\n",
        "        for match in re.finditer(risk_pattern, text, re.IGNORECASE):\n",
        "            entities.append({\n",
        "                'text': match.group(1),\n",
        "                'label': 'RISK_MENTION',\n",
        "                'start': match.start(),\n",
        "                'end': match.end(),\n",
        "                'source': 'pattern'\n",
        "            })\n",
        "\n",
        "        # (Your percentage pattern is likely fine, but the two above are critical)\n",
        "        percentage_pattern = r'(?:increased|decreased|grew|declined|rose|fell)[\\\\s\\\\w]*?by\\\\s*(\\\\d+(?:\\\\.\\\\d+)?%)'\n",
        "        for match in re.finditer(percentage_pattern, text, re.IGNORECASE):\n",
        "            entities.append({\n",
        "                'text': match.group(0),\n",
        "                'label': 'PERCENTAGE_CHANGE',\n",
        "                'start': match.start(),\n",
        "                'end': match.end(),\n",
        "                'source': 'pattern'\n",
        "            })\n",
        "\n",
        "        return entities\n",
        "\n",
        "    # def normalize_and_link_entities(self, entities, text):\n",
        "    #     \"\"\"\n",
        "    #     Normalize entities and link them to knowledge base.\n",
        "    #     \"\"\"\n",
        "    #     normalized = []\n",
        "    #     seen_spans = set()  # Avoid duplicate entities\n",
        "\n",
        "    #     for ent in entities:\n",
        "    #         # Skip if we've already processed this span\n",
        "    #         span = (ent['start'], ent['end'])\n",
        "    #         if span in seen_spans:\n",
        "    #             continue\n",
        "    #         seen_spans.add(span)\n",
        "\n",
        "    #         ent_text = ent['text'].strip()\n",
        "    #         ent_label = ent['label']\n",
        "\n",
        "    #         result = {\n",
        "    #             'original_text': ent_text,\n",
        "    #             'normalized_text': None,\n",
        "    #             'entity_type': ent_label,\n",
        "    #             'linked_id': None,\n",
        "    #             'confidence': ent.get('score', 1.0),\n",
        "    #             'source': ent.get('source', 'unknown'),\n",
        "    #             'span': span\n",
        "    #         }\n",
        "\n",
        "    #         # Normalize based on entity type\n",
        "    #         if ent_label in ['ORG', 'ORGANIZATION']:\n",
        "    #             # Try to normalize to ticker\n",
        "    #             ticker = self.kb.normalize_company(ent_text)\n",
        "    #             if ticker:\n",
        "    #                 result['normalized_text'] = ticker\n",
        "    #                 result['entity_type'] = 'COMPANY'\n",
        "    #                 result['linked_id'] = f\"TICKER:{ticker}\"\n",
        "\n",
        "    #         elif ent_label in ['MONEY', 'MONETARY_VALUE']:\n",
        "    #             result['entity_type'] = 'MONETARY_VALUE'\n",
        "    #             # Extract numeric value\n",
        "    #             numbers = re.findall(r'\\d+(?:,\\d{3})*(?:\\.\\d+)?', ent_text)\n",
        "    #             if numbers:\n",
        "    #                 result['normalized_text'] = numbers[0].replace(',', '')\n",
        "\n",
        "    #         elif ent_label in ['PERCENT', 'PERCENTAGE', 'PERCENTAGE_CHANGE']:\n",
        "    #             result['entity_type'] = 'PERCENTAGE'\n",
        "    #             # Extract percentage value\n",
        "    #             pct = re.search(r'(\\d+(?:\\.\\d+)?)%', ent_text)\n",
        "    #             if pct:\n",
        "    #                 result['normalized_text'] = pct.group(1)\n",
        "\n",
        "    #         elif ent_label == 'RISK_MENTION':\n",
        "    #             # Try to normalize risk type\n",
        "    #             risk_type = self.kb.normalize_risk(ent_text)\n",
        "    #             if risk_type:\n",
        "    #                 result['normalized_text'] = risk_type\n",
        "    #                 result['entity_type'] = 'RISK_TYPE'\n",
        "    #                 result['linked_id'] = f\"RISK:{risk_type}\"\n",
        "\n",
        "    #         # Check if text contains a financial metric\n",
        "    #         metric = self.kb.normalize_metric(ent_text)\n",
        "    #         if metric:\n",
        "    #             result['entity_type'] = 'FINANCIAL_METRIC'\n",
        "    #             result['normalized_text'] = metric\n",
        "    #             result['linked_id'] = f\"METRIC:{metric}\"\n",
        "\n",
        "    #         normalized.append(result)\n",
        "\n",
        "    #     return normalized\n",
        "\n",
        "    def normalize_and_link_entities(self, entities, text):\n",
        "        \"\"\"\n",
        "        Normalize entities and link them to knowledge base.\n",
        "        \"\"\"\n",
        "        normalized = []\n",
        "        seen_spans = set()  # Avoid duplicate entities\n",
        "\n",
        "        for ent in entities:\n",
        "            # Skip if we've already processed this span\n",
        "            span = (ent['start'], ent['end'])\n",
        "            if span in seen_spans:\n",
        "                continue\n",
        "            seen_spans.add(span)\n",
        "\n",
        "            ent_text = ent['text'].strip()\n",
        "            ent_label = ent['label']\n",
        "\n",
        "            result = {\n",
        "                'original_text': ent_text,\n",
        "                'normalized_text': None,\n",
        "                'entity_type': ent_label,\n",
        "                'linked_id': None,\n",
        "                'confidence': ent.get('score', 1.0),\n",
        "                'source': ent.get('source', 'unknown'),\n",
        "                'span': span\n",
        "            }\n",
        "\n",
        "            # --- START OF LOGIC ---\n",
        "\n",
        "            # Normalize based on entity type\n",
        "            if ent_label in ['ORG', 'ORGANIZATION']:\n",
        "                # Try to normalize to ticker\n",
        "                ticker = self.kb.normalize_company(ent_text)\n",
        "                if ticker:\n",
        "                    result['normalized_text'] = ticker\n",
        "                    result['entity_type'] = 'COMPANY'\n",
        "                    result['linked_id'] = f\"TICKER:{ticker}\"\n",
        "\n",
        "            # --- THIS IS THE NEW BLOCK YOU WERE ADDING ---\n",
        "            elif ent_label == 'CARDINAL':\n",
        "                # If a \"cardinal\" number has a comma, it's almost certainly a monetary value\n",
        "                # in a financial report.\n",
        "                if ',' in ent_text:\n",
        "                    result['entity_type'] = 'MONETARY_VALUE'\n",
        "                    result['normalized_text'] = ent_text.replace(',', '')\n",
        "                # (If it doesn't have a comma, we leave it as CARDINAL)\n",
        "            # --- END OF NEW BLOCK ---\n",
        "\n",
        "            elif ent_label in ['MONEY', 'MONETARY_VALUE']:\n",
        "                result['entity_type'] = 'MONETARY_VALUE'\n",
        "                # Extract numeric value\n",
        "                numbers = re.findall(r'\\d+(?:,\\d{3})*(?:\\.\\d+)?', ent_text.replace('$', ''))\n",
        "                if numbers:\n",
        "                    result['normalized_text'] = numbers[0].replace(',', '')\n",
        "\n",
        "            elif ent_label in ['PERCENT', 'PERCENTAGE', 'PERCENTAGE_CHANGE']:\n",
        "                result['entity_type'] = 'PERCENTAGE'\n",
        "                # Extract percentage value\n",
        "                pct = re.search(r'(\\d+(?:\\.\\d+)?)%', ent_text)\n",
        "                if pct:\n",
        "                    result['normalized_text'] = pct.group(1)\n",
        "\n",
        "            elif ent_label == 'RISK_MENTION':\n",
        "                # Try to normalize risk type\n",
        "                risk_type = self.kb.normalize_risk(ent_text)\n",
        "                if risk_type:\n",
        "                    result['normalized_text'] = risk_type\n",
        "                    result['entity_type'] = 'RISK_TYPE'\n",
        "                    result['linked_id'] = f\"RISK:{risk_type}\"\n",
        "\n",
        "            # Check if text contains a financial metric\n",
        "            metric = self.kb.normalize_metric(ent_text)\n",
        "            if metric:\n",
        "                result['entity_type'] = 'FINANCIAL_METRIC'\n",
        "                result['normalized_text'] = metric\n",
        "                result['linked_id'] = f\"METRIC:{metric}\"\n",
        "\n",
        "            normalized.append(result)\n",
        "\n",
        "        return normalized\n",
        "\n",
        "    def extract_and_normalize(self, text):\n",
        "        \"\"\"\n",
        "        Complete pipeline: extract entities from multiple sources and normalize.\n",
        "        \"\"\"\n",
        "        print(\"\\nExtracting entities from multiple sources...\")\n",
        "\n",
        "        # Extract from SpaCy\n",
        "        spacy_entities = self.extract_entities_spacy(text)\n",
        "        print(f\"   SpaCy found {len(spacy_entities)} entities\")\n",
        "\n",
        "        # Extract from Transformer\n",
        "        transformer_entities = self.extract_entities_transformer(text)\n",
        "        print(f\"   Transformer found {len(transformer_entities)} entities\")\n",
        "\n",
        "        # Extract using patterns\n",
        "        pattern_entities = self.extract_financial_metrics_contextual(text)\n",
        "        print(f\"   Pattern matching found {len(pattern_entities)} entities\")\n",
        "\n",
        "        # Combine all entities\n",
        "        all_entities = spacy_entities + transformer_entities + pattern_entities\n",
        "\n",
        "        # Normalize and link\n",
        "        print(\"\\nNormalizing and linking entities...\")\n",
        "        normalized = self.normalize_and_link_entities(all_entities, text)\n",
        "\n",
        "        return normalized\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wzgJvWJauaI",
        "outputId": "305ba09a-890b-49ea-8851-4ef4744007d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "ENHANCED FINANCIAL NER SYSTEM\n",
            "============================================================\n",
            "\n",
            "Initializing Financial Knowledge Base...\n",
            "   Built company mapping with 34397 entries\n",
            "Loading NER models...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Transformer model loaded successfully\n",
            "1. Fetching CIK for ticker: GOOGL...\n",
            "   Found CIK: 0001652044\n",
            "2. Fetching submission history from SEC EDGAR...\n",
            "   Found latest 10-Q filed on: 2025-10-30\n",
            "3. Fetching 10-Q document from: https://www.sec.gov/Archives/edgar/data/0001652044/000165204425000091/goog-20250930.htm\n",
            "   Successfully fetched document.\n",
            "4. Parsing HTML content...\n",
            "   Parsing complete.\n",
            "\n",
            "   Analyzing: PART II, ITEM 2\n",
            "   Text length: 1608 characters\n"
          ]
        }
      ],
      "source": [
        "## Cell 6: Load Data and Initialize System\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"ENHANCED FINANCIAL NER SYSTEM\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Initialize knowledge base\n",
        "print(\"\\nInitializing Financial Knowledge Base...\")\n",
        "kb = FinancialKnowledgeBase()\n",
        "\n",
        "# Initialize entity extractor\n",
        "extractor = FinancialEntityExtractor(kb)\n",
        "\n",
        "# Load 10-Q data\n",
        "target_ticker = 'GOOGL'  # Change as needed\n",
        "# target_ticker = 'MSFT'\n",
        "# target_ticker = 'TSLA'\n",
        "\n",
        "try:\n",
        "    # Fetch and parse 10-Q\n",
        "    html = get_latest_10q(target_ticker)\n",
        "    report_data = parse_10q(html)\n",
        "\n",
        "    # Use specific section or full document\n",
        "    # For demonstration, let's use a specific item that's rich in entities\n",
        "    if 'PART II' in report_data and 'ITEM 2' in report_data['PART II']:\n",
        "        text_to_analyze = report_data['PART II']['ITEM 2']\n",
        "        print(f\"\\n   Analyzing: PART II, ITEM 2\")\n",
        "    else:\n",
        "        # Fallback to first available content\n",
        "        all_content = []\n",
        "        for part_key, items in report_data.items():\n",
        "            for item_key, content in items.items():\n",
        "                all_content.append(content)\n",
        "        text_to_analyze = \"\\n\\n\".join(all_content)[:10000]  # Limit for demo\n",
        "        print(f\"\\n   Analyzing: First 10000 characters of full document\")\n",
        "\n",
        "    print(f\"   Text length: {len(text_to_analyze)} characters\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nError loading data: {e}\")\n",
        "    print(\"Using sample text for demonstration...\")\n",
        "    text_to_analyze = \"\"\"\n",
        "    Apple Inc. reported total revenues of $394.3 billion for fiscal 2022,\n",
        "    an increase of 7.8% compared to the prior year. Net income reached $99.8 billion.\n",
        "    Microsoft Corporation saw its cloud revenue grow by 22%, while operating income\n",
        "    increased to $83.4 billion. The company faces market risk related to foreign\n",
        "    currency fluctuations and interest rate risk from its debt obligations.\n",
        "    Tesla reported delivery of 1.31 million vehicles and faces regulatory risk\n",
        "    in various markets. The company's liquidity risk is managed through cash reserves\n",
        "    of $22.2 billion.\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srH0v9XZaxjo",
        "outputId": "b5847f1a-022c-4870-f350-c024d5932e48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "RUNNING ENHANCED NER PIPELINE\n",
            "============================================================\n",
            "\n",
            "Extracting entities from multiple sources...\n",
            "   SpaCy found 27 entities\n",
            "   Transformer found 9 entities\n",
            "   Pattern matching found 8 entities\n",
            "\n",
            "Normalizing and linking entities...\n",
            "\n",
            "   Total entities extracted and normalized: 34\n"
          ]
        }
      ],
      "source": [
        "## Cell 7: Run Enhanced NER Pipeline\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RUNNING ENHANCED NER PIPELINE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Extract and normalize entities\n",
        "normalized_entities = extractor.extract_and_normalize(text_to_analyze)\n",
        "\n",
        "print(f\"\\n   Total entities extracted and normalized: {len(normalized_entities)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQ2GwbNba0J3",
        "outputId": "00170693-ab6e-4404-9027-079c0d637955"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "ENTITY ANALYSIS BY TYPE\n",
            "============================================================\n",
            "\n",
            "### CARDINAL (9 found)\n",
            "------------------------------------------------------------\n",
            "  Original: 2\n",
            "  Confidence: 1.00\n",
            "  Source: spacy\n",
            "\n",
            "  Original: 185.81\n",
            "  Confidence: 1.00\n",
            "  Source: spacy\n",
            "\n",
            "  Original: 203.08\n",
            "  Confidence: 1.00\n",
            "  Source: spacy\n",
            "\n",
            "  Original: 551\n",
            "  Confidence: 1.00\n",
            "  Source: spacy\n",
            "\n",
            "  Original: 241.96\n",
            "  Confidence: 1.00\n",
            "  Source: spacy\n",
            "\n",
            "  Original: 1\n",
            "  Confidence: 1.00\n",
            "  Source: spacy\n",
            "\n",
            "  Original: 10b5-1\n",
            "  Confidence: 1.00\n",
            "  Source: spacy\n",
            "\n",
            "  Original: 11\n",
            "  Confidence: 1.00\n",
            "  Source: spacy\n",
            "\n",
            "  Original: 2\n",
            "  Confidence: 1.00\n",
            "  Source: spacy\n",
            "\n",
            "\n",
            "### COMPANY (1 found)\n",
            "------------------------------------------------------------\n",
            "  Original: Alphabet\n",
            "  Normalized: GOOG\n",
            "  Linked ID: TICKER:GOOG\n",
            "  Confidence: 1.00\n",
            "  Source: spacy\n",
            "\n",
            "\n",
            "### DATE (6 found)\n",
            "------------------------------------------------------------\n",
            "  Original: the quarter ended September 30, 2025\n",
            "  Confidence: 1.00\n",
            "  Source: spacy\n",
            "\n",
            "  Original: May\n",
            "  Confidence: 1.00\n",
            "  Source: spacy\n",
            "\n",
            "  Original: July 1 - 31\n",
            "  Confidence: 1.00\n",
            "  Source: spacy\n",
            "\n",
            "  Original: August 1 - 31\n",
            "  Confidence: 1.00\n",
            "  Source: spacy\n",
            "\n",
            "  Original: September 1 - 30\n",
            "  Confidence: 1.00\n",
            "  Source: spacy\n",
            "\n",
            "  Original: Item 1\n",
            "  Confidence: 1.00\n",
            "  Source: spacy\n",
            "\n",
            "\n",
            "### MISC (5 found)\n",
            "------------------------------------------------------------\n",
            "  Original: A\n",
            "  Confidence: 0.60\n",
            "  Source: transformer\n",
            "\n",
            "  Original: C\n",
            "  Confidence: 0.58\n",
            "  Source: transformer\n",
            "\n",
            "  Original: Publicly\n",
            "  Confidence: 0.60\n",
            "  Source: transformer\n",
            "\n",
            "  Original: ##ounced\n",
            "  Confidence: 0.70\n",
            "  Source: transformer\n",
            "\n",
            "  Original: 10\n",
            "  Confidence: 0.76\n",
            "  Source: transformer\n",
            "\n",
            "\n",
            "### MONETARY_VALUE (8 found)\n",
            "------------------------------------------------------------\n",
            "  Original: 2,571\n",
            "  Normalized: 2571\n",
            "  Confidence: 1.00\n",
            "  Source: spacy\n",
            "\n",
            "  Original: 19,077\n",
            "  Normalized: 19077\n",
            "  Confidence: 1.00\n",
            "  Source: spacy\n",
            "\n",
            "  Original: 1,548\n",
            "  Normalized: 1548\n",
            "  Confidence: 1.00\n",
            "  Source: spacy\n",
            "\n",
            "  Original: 17,148\n",
            "  Normalized: 17148\n",
            "  Confidence: 1.00\n",
            "  Source: spacy\n",
            "\n",
            "  Original: 14,738\n",
            "  Normalized: 14738\n",
            "  Confidence: 1.00\n",
            "  Source: spacy\n",
            "\n",
            "  Original: 4,670\n",
            "  Normalized: 4670\n",
            "  Confidence: 1.00\n",
            "  Source: spacy\n",
            "\n",
            "  Original: 50,963\n",
            "  Normalized: 50963\n",
            "  Confidence: 1.00\n",
            "  Source: spacy\n",
            "\n",
            "  Original: 55,633\n",
            "  Normalized: 55633\n",
            "  Confidence: 1.00\n",
            "  Source: spacy\n",
            "\n",
            "\n",
            "### ORG (3 found)\n",
            "------------------------------------------------------------\n",
            "  Original: Consolidated Financial Statements\n",
            "  Confidence: 1.00\n",
            "  Source: spacy\n",
            "\n",
            "  Original: Equity Securities\n",
            "  Confidence: 0.99\n",
            "  Source: transformer\n",
            "\n",
            "  Original: Ave\n",
            "  Confidence: 0.80\n",
            "  Source: transformer\n",
            "\n",
            "\n",
            "### PRODUCT (1 found)\n",
            "------------------------------------------------------------\n",
            "  Original: Notes\n",
            "  Confidence: 1.00\n",
            "  Source: spacy\n",
            "\n",
            "\n",
            "### WORK_OF_ART (1 found)\n",
            "------------------------------------------------------------\n",
            "  Original: Quarterly Report\n",
            "  Confidence: 1.00\n",
            "  Source: spacy\n",
            "\n",
            "\n",
            "============================================================\n",
            "SUMMARY STATISTICS\n",
            "============================================================\n",
            "  Companies identified: 1\n",
            "  Financial metrics: 0\n",
            "  Risk types: 0\n",
            "  Monetary values: 8\n",
            "  Total entities: 34\n",
            "\n",
            "  Unique companies (tickers): GOOG\n"
          ]
        }
      ],
      "source": [
        "## Cell 8: Analyze and Display Results\n",
        "\n",
        "def display_entity_analysis(entities):\n",
        "    \"\"\"Display comprehensive entity analysis.\"\"\"\n",
        "\n",
        "    # Group by entity type\n",
        "    by_type = defaultdict(list)\n",
        "    for ent in entities:\n",
        "        by_type[ent['entity_type']].append(ent)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ENTITY ANALYSIS BY TYPE\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    for ent_type in sorted(by_type.keys()):\n",
        "        print(f\"\\n### {ent_type} ({len(by_type[ent_type])} found)\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "        for ent in by_type[ent_type][:10]:  # Show top 10 per type\n",
        "            print(f\"  Original: {ent['original_text']}\")\n",
        "            if ent['normalized_text']:\n",
        "                print(f\"  Normalized: {ent['normalized_text']}\")\n",
        "            if ent['linked_id']:\n",
        "                print(f\"  Linked ID: {ent['linked_id']}\")\n",
        "            print(f\"  Confidence: {ent['confidence']:.2f}\")\n",
        "            print(f\"  Source: {ent['source']}\")\n",
        "            print()\n",
        "\n",
        "        if len(by_type[ent_type]) > 10:\n",
        "            print(f\"  ... and {len(by_type[ent_type]) - 10} more\")\n",
        "\n",
        "    # Summary statistics\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"SUMMARY STATISTICS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    companies = [e for e in entities if e['entity_type'] == 'COMPANY']\n",
        "    metrics = [e for e in entities if e['entity_type'] == 'FINANCIAL_METRIC']\n",
        "    risks = [e for e in entities if e['entity_type'] == 'RISK_TYPE']\n",
        "    monetary = [e for e in entities if e['entity_type'] == 'MONETARY_VALUE']\n",
        "\n",
        "    print(f\"  Companies identified: {len(companies)}\")\n",
        "    print(f\"  Financial metrics: {len(metrics)}\")\n",
        "    print(f\"  Risk types: {len(risks)}\")\n",
        "    print(f\"  Monetary values: {len(monetary)}\")\n",
        "    print(f\"  Total entities: {len(entities)}\")\n",
        "\n",
        "    # Unique companies\n",
        "    unique_companies = set(e['normalized_text'] for e in companies if e['normalized_text'])\n",
        "    if unique_companies:\n",
        "        print(f\"\\n  Unique companies (tickers): {', '.join(sorted(unique_companies))}\")\n",
        "\n",
        "    # Unique risk types\n",
        "    unique_risks = set(e['normalized_text'] for e in risks if e['normalized_text'])\n",
        "    if unique_risks:\n",
        "        print(f\"  Unique risk types: {', '.join(sorted(unique_risks))}\")\n",
        "\n",
        "# Display analysis\n",
        "display_entity_analysis(normalized_entities)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
