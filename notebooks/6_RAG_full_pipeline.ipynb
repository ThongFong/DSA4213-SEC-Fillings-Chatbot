{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9acce990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import uuid\n",
    "import time\n",
    "import requests\n",
    "import gc\n",
    "import json\n",
    "import unicodedata\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import io\n",
    "from transformers import pipeline as transformers_pipeline\n",
    "from typing import List, Dict, Tuple, Any, Optional\n",
    "from collections import defaultdict\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "# External libraries\n",
    "from dotenv import load_dotenv\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder # Import CrossEncoder\n",
    "from qdrant_client import QdrantClient, models\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
    "from openai import OpenAI\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Imports for QueryProcessor\n",
    "import spacy\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0152ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# PART 1: CONFIGURATION\n",
    "# ===========================================================================\n",
    "class Config:\n",
    "    QDRANT_URL = os.getenv(\"QDRANT_URL\")\n",
    "    QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")\n",
    "    OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "    SEC_HEADERS = {'User-Agent': 'FullAdvancedRAG research@example.com'}\n",
    "    CIK_MAP_URL = 'https://www.sec.gov/files/company_tickers.json'\n",
    "\n",
    "    # Model Configuration\n",
    "    EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    LLM_MODEL = \"gpt-4o\"\n",
    "    FLAN_T5_MODEL_NAME = \"google/flan-t5-small\"\n",
    "    SPACY_MODEL = \"en_core_web_lg\" # Use the large, more accurate model\n",
    "    \n",
    "    # --- Post-Retrieval Config ---\n",
    "    CROSS_ENCODER_MODEL = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "    RETRIEVAL_TOP_K = 20 # Retrieve 20 candidates\n",
    "    FINAL_TOP_K = 5      # Re-rank down to 5\n",
    "\n",
    "    COLLECTION_NAME = \"sec_filings_10q_GOLDEN_BENCHMARK\"\n",
    "    VECTOR_SIZE = 384\n",
    "    CHUNK_SIZE = 800\n",
    "    CHUNK_OVERLAP = 200\n",
    "    TICKERS = ['NVDA', 'AAPL', 'MSFT', 'AMZN', 'META', 'GOOGL', 'TSLA', 'ORCL', 'JPM', 'AMD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6f98d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# PART 2: DOCUMENT LOADING\n",
    "# ===========================================================================\n",
    "\n",
    "class SECDocumentLoader:\n",
    "    @staticmethod\n",
    "    def get_recent_10q_metadata(ticker: str, num_filings: int = 4) -> List[Dict[str, str]]:\n",
    "        print(f\"  → Fetching CIK for ticker: {ticker}...\")\n",
    "        response = requests.get(Config.CIK_MAP_URL, headers=Config.SEC_HEADERS)\n",
    "        response.raise_for_status()\n",
    "        company_data = response.json()\n",
    "        cik = None\n",
    "        company_name = None\n",
    "        for company in company_data.values():\n",
    "            if company['ticker'] == ticker.upper():\n",
    "                cik = str(company['cik_str']).zfill(10)\n",
    "                company_name = company['title']\n",
    "                break\n",
    "        if not cik: raise ValueError(f\"Ticker '{ticker}' not found\")\n",
    "        print(f\"  → Found CIK: {cik} ({company_name})\")\n",
    "        submissions_url = f\"https://data.sec.gov/submissions/CIK{cik}.json\"\n",
    "        time.sleep(0.1)\n",
    "        response = requests.get(submissions_url, headers=Config.SEC_HEADERS)\n",
    "        response.raise_for_status()\n",
    "        submissions = response.json()\n",
    "        filings_metadata = []\n",
    "        for i, form in enumerate(submissions['filings']['recent']['form']):\n",
    "            if form == '10-Q':\n",
    "                accession_number = submissions['filings']['recent']['accessionNumber'][i]\n",
    "                primary_document = submissions['filings']['recent']['primaryDocument'][i]\n",
    "                filing_date = submissions['filings']['recent']['filingDate'][i]\n",
    "                accession_number_clean = accession_number.replace('-', '')\n",
    "                filing_url = f\"https://www.sec.gov/Archives/edgar/data/{cik}/{accession_number_clean}/{primary_document}\"\n",
    "                metadata = {'ticker': ticker.upper(), 'company_name': company_name, 'filing_date': filing_date, 'cik': cik, 'filing_url': filing_url}\n",
    "                filings_metadata.append(metadata)\n",
    "                if len(filings_metadata) >= num_filings: break\n",
    "        if not filings_metadata: raise ValueError(f\"No recent 10-Q filings found for ticker '{ticker}'\")\n",
    "        print(f\"  → Found {len(filings_metadata)} recent 10-Q filing metadata entries.\")\n",
    "        return filings_metadata\n",
    "\n",
    "    @staticmethod\n",
    "    def get_filing_html(filing_url: str) -> str:\n",
    "        time.sleep(0.1); response = requests.get(filing_url, headers=Config.SEC_HEADERS); response.raise_for_status(); return response.text\n",
    "\n",
    "    @staticmethod\n",
    "    def _normalize_header_text(text: str) -> str:\n",
    "        text = text.strip().upper()\n",
    "        part_match = re.search(r'^\\s*(PART\\s+I{1,2})', text)\n",
    "        if part_match: return re.sub(r'\\s+', ' ', part_match.group(1))\n",
    "        item_match = re.search(r'^\\s*(ITEM\\s+\\d[A-Z]?)', text)\n",
    "        if item_match: return re.sub(r'\\s+', ' ', item_match.group(1))\n",
    "        return None\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_html_table(table_tag) -> str:\n",
    "        rows = []\n",
    "        for tr in table_tag.find_all('tr'):\n",
    "            cells = [\" \".join(cell.get_text(strip=True).split()) for cell in tr.find_all(['td', 'th'])]\n",
    "            if any(cells): rows.append(cells)\n",
    "        if not rows: return \"\"\n",
    "        header = rows[0]; md = [\"| \" + \" | \".join(header) + \" |\", \"| \" + \" | \".join(['---'] * len(header)) + \" |\"]\n",
    "        for row in rows[1:]:\n",
    "            while len(row) < len(header): row.append(\"\")\n",
    "            md.append(\"| \" + \" | \".join(row[:len(header)]) + \" |\")\n",
    "        return \"\\n\" + \"\\n\".join(md) + \"\\n\"\n",
    "\n",
    "    @classmethod\n",
    "    def parse_10q(cls, html_content: str) -> Dict:\n",
    "        soup = BeautifulSoup(html_content, 'lxml'); headers = []\n",
    "        for h in soup.find_all(['p', 'b', 'strong', 'div']):\n",
    "            text = h.get_text(strip=True)\n",
    "            if len(text) > 100: continue\n",
    "            key = cls._normalize_header_text(text)\n",
    "            if key and not h.find_parent('a'): headers.append({'tag': h, 'key': key})\n",
    "        if not headers: return {}\n",
    "        data = defaultdict(lambda: defaultdict(str)); part_key = None\n",
    "        for i, h_info in enumerate(headers):\n",
    "            key = h_info['key']\n",
    "            if 'PART' in key: part_key = key; continue\n",
    "            if 'ITEM' in key:\n",
    "                if not part_key: part_key = \"PART I\"\n",
    "                start_node = h_info['tag']; end_node = headers[i + 1]['tag'] if i + 1 < len(headers) else None\n",
    "                parts = []\n",
    "                for elem in start_node.next_elements:\n",
    "                    if elem == end_node: break\n",
    "                    if isinstance(elem, NavigableString) and not elem.find_parent('table'):\n",
    "                        if txt := elem.strip(): parts.append(txt)\n",
    "                    elif elem.name == 'table' and not elem.find_parent('table'):\n",
    "                        if md := cls._parse_html_table(elem): parts.append(md)\n",
    "                data[part_key][key] = re.sub(r'\\n{3,}', '\\n\\n', \"\\n\".join(parts)).strip()\n",
    "        return {p: dict(i) for p, i in data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d6e8cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# PART 3: TEXT CHUNKING & EMBEDDING\n",
    "# ===========================================================================\n",
    "class DocumentProcessor:\n",
    "    def __init__(self, embedding_model_name: str = Config.EMBEDDING_MODEL):\n",
    "        print(f\"\\n Loading embedding model: {embedding_model_name}\")\n",
    "        self.model = SentenceTransformer(embedding_model_name)\n",
    "        print(f\"   ✓ Model loaded (dimension: {self.model.get_sentence_embedding_dimension()})\")\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(chunk_size=Config.CHUNK_SIZE, chunk_overlap=Config.CHUNK_OVERLAP, length_function=len, add_start_index=False)\n",
    "        print(f\"   ✓ Initialized RecursiveCharacterTextSplitter\")\n",
    "        \n",
    "    def generate_document_chunks(self, parsed_data: Dict, metadata: Dict, embed_batch_size: int = 1024):\n",
    "        all_docs = []\n",
    "        for part, items in parsed_data.items():\n",
    "            for item, content in items.items():\n",
    "                if not content: \n",
    "                    continue\n",
    "                \n",
    "                doc_metadata = {\n",
    "                    'ticker': metadata['ticker'], \n",
    "                    'company_name': metadata['company_name'], \n",
    "                    'filing_date': metadata['filing_date'], \n",
    "                    'filing_url': metadata['filing_url'], \n",
    "                    'part': part, \n",
    "                    'item': item\n",
    "                }\n",
    "                all_docs.append(Document(page_content=content, metadata=doc_metadata))\n",
    "        \n",
    "        if not all_docs: \n",
    "            return\n",
    "        \n",
    "        print(f\"     → Splitting {len(all_docs)} high-level 'Items' into smaller chunks...\")\n",
    "        chunked_docs = self.text_splitter.split_documents(all_docs)\n",
    "        print(f\"     → Generated {len(chunked_docs)} chunks\")\n",
    "        \n",
    "        text_batch, metadata_batch = [], []\n",
    "        for chunk in chunked_docs:\n",
    "            text_batch.append(chunk.page_content)\n",
    "            metadata_batch.append(chunk.metadata)\n",
    "            \n",
    "            if len(text_batch) >= embed_batch_size:\n",
    "                embeddings = self.model.encode(text_batch, show_progress_bar=False)\n",
    "                for txt, emb, meta in zip(text_batch, embeddings, metadata_batch):\n",
    "                    yield PointStruct(\n",
    "                        id=str(uuid.uuid4()), \n",
    "                        vector=emb.tolist(), \n",
    "                        payload={'text': txt, **meta}\n",
    "                    )\n",
    "                text_batch, metadata_batch = [], []\n",
    "        \n",
    "        if text_batch:\n",
    "            embeddings = self.model.encode(text_batch, show_progress_bar=False)\n",
    "            for txt, emb, meta in zip(text_batch, embeddings, metadata_batch):\n",
    "                yield PointStruct(\n",
    "                    id=str(uuid.uuid4()), \n",
    "                    vector=emb.tolist(), \n",
    "                    payload={'text': txt, **meta}\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa91cb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# PART 4: QDRANT VECTOR DATABASE\n",
    "# ===========================================================================\n",
    "class QdrantManager:\n",
    "    def __init__(self):\n",
    "        print(f\"\\nConnecting to Qdrant Cloud...\")\n",
    "        self.client = QdrantClient(url=Config.QDRANT_URL, api_key=Config.QDRANT_API_KEY)\n",
    "        print(f\"   ✓ Connected to Qdrant\")\n",
    "        \n",
    "    def create_collection(self, collection_name: str = Config.COLLECTION_NAME, vector_size: int = Config.VECTOR_SIZE):\n",
    "        print(f\"\\n Setting up collection: {collection_name}\")\n",
    "        collections = self.client.get_collections().collections\n",
    "        exists = any(col.name == collection_name for col in collections)\n",
    "        if exists:\n",
    "            print(f\"   ⚠ Collection '{collection_name}' exists, recreating...\")\n",
    "            self.client.delete_collection(collection_name)\n",
    "            \n",
    "        self.client.create_collection(collection_name=collection_name, vectors_config=models.VectorParams(size=vector_size, distance=models.Distance.COSINE))\n",
    "        print(f\"   ✓ Collection created\")\n",
    "        \n",
    "        for field in [\"ticker\", \"item\"]:\n",
    "            print(f\"   → Creating payload index for '{field}' (Keyword)...\")\n",
    "            self.client.create_payload_index(collection_name=collection_name, field_name=field, field_schema=models.PayloadSchemaType.KEYWORD)\n",
    "            \n",
    "        print(f\"   ✓ Payload indexes created.\")\n",
    "\n",
    "    def upsert_documents(self, points_generator, collection_name: str = Config.COLLECTION_NAME, batch_size: int = 2048) -> int:\n",
    "        print(f\" Uploading chunks to Qdrant in batches of {batch_size}...\"); batch, count = [], 0\n",
    "        for point in points_generator:\n",
    "            batch.append(point)\n",
    "            if len(batch) >= batch_size:\n",
    "                self.client.upsert(collection_name=collection_name, points=batch, wait=False); count += len(batch); print(f\"     → Uploaded {count} chunks...\"); batch = []\n",
    "        if batch: self.client.upsert(collection_name=collection_name, points=batch, wait=False); count += len(batch)\n",
    "        print(f\"  ✓ All chunks uploaded. Total: {count}\"); return count\n",
    "\n",
    "    def search(self, query_vector: List[float], collection_name: str = Config.COLLECTION_NAME, limit: int = Config.RETRIEVAL_TOP_K, filter_dict: Dict = None) -> List[Dict]:\n",
    "        qdrant_filter = None\n",
    "        if filter_dict:\n",
    "            must_conditions = []\n",
    "            for key, value in filter_dict.items():\n",
    "                if isinstance(value, list): \n",
    "                    must_conditions.append(models.FieldCondition(key=key, match=models.MatchAny(any=value)))\n",
    "                else: \n",
    "                    must_conditions.append(models.FieldCondition(key=key, match=models.MatchValue(value=value)))\n",
    "            qdrant_filter = models.Filter(must=must_conditions)\n",
    "            \n",
    "        results = self.client.search(collection_name=collection_name, query_vector=query_vector, limit=limit, query_filter=qdrant_filter, with_payload=True)\n",
    "        return [{'score': result.score, 'payload': result.payload} for result in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1daa59d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded spaCy model for NER\n",
      "✓ Loaded SentenceTransformer model 'sentence-transformers/all-MiniLM-L6-v2'\n",
      "✓ Loaded Flan-T5 model 'google/flan-t5-small' on cpu\n"
     ]
    }
   ],
   "source": [
    "# ===========================================================================\n",
    "# PART 5: QUERY PROCESSOR MODULE\n",
    "# ===========================================================================\n",
    "\n",
    "USE_SBERT = True\n",
    "USE_FLAN_T5 = True\n",
    "\n",
    "def normalize(text: str) -> str:\n",
    "    text = unicodedata.normalize(\"NFKC\", text).strip()\n",
    "    return re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "TOKEN_RE = re.compile(r\"[A-Za-z0-9]+(?:'[A-Za-z0-9]+)?|[&$%.\\-]+\")\n",
    "def simple_tokenize(text: str) -> List[str]:\n",
    "    toks = TOKEN_RE.findall(text); cleaned = []\n",
    "    for t in toks:\n",
    "        if t.lower() == \"'s\": continue\n",
    "        if t.endswith(\"'s\"): t = t[:-2]\n",
    "        cleaned.append(t)\n",
    "    return cleaned\n",
    "\n",
    "DOMAIN_SYNONYMS = {\"risk\": [\"risk factor\",\"risk factors\",\"uncertainty\",\"exposure\",\"threat\"], \"cyber\": [\"cybersecurity\",\"information security\",\"infosec\",\"data breach\",\"security incident\"], \"performance\": [\"revenue\",\"growth\",\"margin\",\"profit\",\"loss\",\"guidance\",\"results\"], \"strategy\": [\"roadmap\",\"plan\",\"initiative\",\"expansion\",\"capex\",\"restructuring\",\"acquisition\"], \"md&a\": [\"management discussion\",\"md&a\",\"results of operations\"],}\n",
    "COMPANY_TICKERS = {\"tesla\":\"TSLA\", \"apple\":\"AAPL\", \"microsoft\":\"MSFT\", \"nvidia\":\"NVDA\", \"google\":\"GOOGL\", \"alphabet\":\"GOOGL\", \"meta\":\"META\", \"amazon\":\"AMZN\", \"amd\":\"AMD\", \"oracle\":\"ORCL\", \"jpmorgan\":\"JPM\", \"jpm\":\"JPM\"}\n",
    "\n",
    "def keyword_expand(tokens: List[str]) -> List[str]:\n",
    "    ex = []; t0s = [t.strip(\".-\").lower() for t in tokens]\n",
    "    for t0 in t0s: ex.extend(DOMAIN_SYNONYMS.get(t0, []))\n",
    "    seen, out = set(), [];\n",
    "    for w in ex:\n",
    "        if w not in seen: seen.add(w); out.append(w)\n",
    "    return out\n",
    "\n",
    "def build_keywords(tokens: List[str], expansions: List[str]) -> List[str]:\n",
    "    kept = []\n",
    "    for t in tokens + expansions:\n",
    "        t = t.lower()\n",
    "        if re.search(r\"[a-z0-9]\", t) and t not in kept: kept.append(t)\n",
    "    return kept\n",
    "\n",
    "try:\n",
    "    _nlp = spacy.load(Config.SPACY_MODEL)\n",
    "    print(\"✓ Loaded spaCy model for NER\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Failed to load spaCy model '{Config.SPACY_MODEL}'. NER will be limited.\")\n",
    "    print(\"  Run: python -m spacy download en_core_web_lg\")\n",
    "    _nlp = None\n",
    "\n",
    "def extract_entities(raw_text: str) -> dict:\n",
    "    out = {}; low = raw_text.lower()\n",
    "    q = re.findall(r\"\\b(q[1-4])\\s*([12][0-9]{3})\\b\", low)\n",
    "    if q: out[\"quarter\"] = [f\"{p.upper()} {y}\" for p, y in q]\n",
    "    years = re.findall(r\"\\b(20[0-4][0-9]|19[0-9]{2})\\b\", raw_text)\n",
    "    if years: out[\"year\"] = sorted(set(years))\n",
    "    \n",
    "    companies = set()\n",
    "    if _nlp is not None:\n",
    "        doc = _nlp(raw_text)\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == \"ORG\": companies.add(ent.text.strip())\n",
    "            \n",
    "    low_raw = raw_text.lower() \n",
    "    \n",
    "    for name in COMPANY_TICKERS:\n",
    "        if name in low_raw: companies.add(name.title()) # 'low_raw' is now defined\n",
    "        \n",
    "    if companies: out[\"company\"] = sorted(companies)\n",
    "    \n",
    "    tickers = set(COMPANY_TICKERS.get(c.lower(),\"\") for c in companies if COMPANY_TICKERS.get(c.lower()))\n",
    "    tickers.update(re.findall(r\"\\$([A-Z]{1,5})\\b\", raw_text))\n",
    "    tickers.update(re.findall(r\"\\(([A-Z]{1,5})\\)\", raw_text))\n",
    "    tickers.update(re.findall(r\"\\b(?:NASDAQ|NYSE)\\s*:\\s*([A-Z]{1,5})\\b\", raw_text))\n",
    "    tickers = {t for t in tickers if t}\n",
    "    if tickers: out[\"ticker\"] = sorted(tickers)\n",
    "    return out\n",
    "\n",
    "try:\n",
    "    _sbert = SentenceTransformer(Config.EMBEDDING_MODEL) if USE_SBERT else None\n",
    "    print(f\"✓ Loaded SentenceTransformer model '{Config.EMBEDDING_MODEL}'\")\n",
    "except Exception: _sbert = None; print(f\"✗ Failed to load SentenceTransformer\")\n",
    "\n",
    "def sbert_embed(text: str) -> Optional[List[float]]:\n",
    "    if _sbert is None: return None\n",
    "    v = _sbert.encode([text], normalize_embeddings=True)[0]\n",
    "    return v.tolist()\n",
    "\n",
    "try:\n",
    "    _flan_device = \"cuda\" if USE_FLAN_T5 and torch.cuda.is_available() else \"cpu\"\n",
    "    _flan_tok = AutoTokenizer.from_pretrained(Config.FLAN_T5_MODEL_NAME) if USE_FLAN_T5 else None\n",
    "    _flan_mdl = AutoModelForSeq2SeqLM.from_pretrained(Config.FLAN_T5_MODEL_NAME).to(_flan_device).eval() if USE_FLAN_T5 else None\n",
    "    print(f\"✓ Loaded Flan-T5 model '{Config.FLAN_T5_MODEL_NAME}' on {_flan_device}\")\n",
    "except Exception as e:\n",
    "    _flan_tok = _flan_mdl = None; _flan_device = \"cpu\"; print(f\"✗ Failed to load Flan-T5 model: {e}\")\n",
    "\n",
    "def t5_paraphrases_safe(q: str, num_return: int = 3, max_new_tokens: int = 48) -> List[str]:\n",
    "    if not (USE_FLAN_T5 and _flan_tok is not None and _flan_mdl is not None): return []\n",
    "    prompt = f\"Rewrite the query into multiple short paraphrases without adding facts or numbers. Keep meaning.\\nQuery: {q}\"\n",
    "    x = _flan_tok(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    x = {k: v.to(_flan_device) for k, v in x.items()}\n",
    "    with torch.no_grad():\n",
    "        out = _flan_mdl.generate(**x, do_sample=True, top_k=50, top_p=0.92, temperature=0.9, num_return_sequences=num_return, max_new_tokens=max_new_tokens, repetition_penalty=1.1, no_repeat_ngram_size=3)\n",
    "    paras = _flan_tok.batch_decode(out, skip_special_tokens=True)\n",
    "    base = re.sub(r\"\\W+\",\" \", q).strip().lower()\n",
    "    seen, kept = set(), []\n",
    "    for p in paras:\n",
    "        p2 = normalize(p); p2_cmp = re.sub(r\"\\W+\",\" \", p2).strip().lower()\n",
    "        if p2_cmp == base: continue\n",
    "        if p2 and p2 not in seen: seen.add(p2); kept.append(p2)\n",
    "    return kept[:num_return]\n",
    "\n",
    "INTENT_LABELS = [\"risk\",\"performance\",\"strategy\"]\n",
    "X_train = [\"What new risk factors?\", \"Cybersecurity breach Tesla\", \"Explain Apple revenue growth\", \"Compare Microsoft profit guidance\", \"Outline Nvidia expansion strategy\", \"What restructuring plan?\"]\n",
    "y_train = [\"risk\",\"risk\",\"performance\",\"performance\",\"strategy\",\"strategy\"]\n",
    "_intent_clf = Pipeline([(\"tfidf\", TfidfVectorizer(ngram_range=(1,2), min_df=1)), (\"lr\", LogisticRegression(max_iter=300, class_weight=\"balanced\", multi_class=\"ovr\"))]).fit(X_train, y_train)\n",
    "RISK_KW = {\"risk\",\"risk factor\",\"risk factors\",\"uncertainty\",\"cyber\",\"cybersecurity\",\"breach\",\"litigation\",\"security\"}\n",
    "PERF_KW = {\"revenue\",\"growth\",\"margin\",\"profit\",\"loss\",\"guidance\",\"results\",\"compare\",\"last quarter\",\"quarterly\"}\n",
    "STRAT_KW= {\"strategy\",\"plan\",\"roadmap\",\"expansion\",\"acquisition\",\"restructuring\",\"capex\",\"data center\",\"data centers\"}\n",
    "def _kw_score(t: str, kws: set[str]) -> int: return sum(1 for k in kws if k in t)\n",
    "def classify_intent(text: str) -> Tuple[str, float]:\n",
    "    tx = normalize(text); proba = _intent_clf.predict_proba([tx])[0].tolist()\n",
    "    k_r = _kw_score(tx, RISK_KW); k_p = _kw_score(tx, PERF_KW); k_s = _kw_score(tx, STRAT_KW)\n",
    "    k_sum = max(1, (k_r + k_p + k_s)); priors = [k_r/k_sum, k_p/k_sum, k_s/k_sum]\n",
    "    alpha, beta = 0.6, 0.4; blended = [alpha*proba[i] + beta*priors[i] for i in range(3)]\n",
    "    s = sum(blended) or 1.0; blended = [b/s for b in blended]\n",
    "    idx = max(range(3), key=lambda i: blended[i])\n",
    "    return INTENT_LABELS[idx], float(blended[idx])\n",
    "\n",
    "def expand_query(query: str) -> dict:\n",
    "    norm = normalize(query); toks = simple_tokenize(norm); lex_ex = keyword_expand(toks)\n",
    "    paras = t5_paraphrases_safe(norm, num_return=3, max_new_tokens=48) if USE_FLAN_T5 else []\n",
    "    para_tokens = list(dict.fromkeys(t for p in paras for t in simple_tokenize(p)))\n",
    "    para_ex = keyword_expand(para_tokens) if para_tokens else []\n",
    "    expansions = list(dict.fromkeys(w for lst in (lex_ex, para_ex) for w in lst))\n",
    "    return {\"normalized\": norm, \"tokens\": toks, \"expansions\": expansions, \"paraphrases\": paras, \"keywords\": build_keywords(toks, expansions)}\n",
    "\n",
    "@dataclass\n",
    "class QueryProcessorConfig:\n",
    "    labels: List[str] = field(default_factory=lambda: [\"risk\",\"performance\",\"strategy\"])\n",
    "class QueryProcessor:\n",
    "    def __init__(self, config: QueryProcessorConfig = QueryProcessorConfig()):\n",
    "        self.config = config; print(\"✓ QueryProcessor initialized\")\n",
    "    def process(self, query: str) -> Dict[str, Any]:\n",
    "        raw = query; ex = expand_query(query); ents = extract_entities(raw)\n",
    "        label, conf = classify_intent(ex[\"normalized\"])\n",
    "        emb = sbert_embed(ex[\"normalized\"]) if USE_SBERT else None\n",
    "        return {\"normalized\": ex[\"normalized\"], \"label\": label, \"confidence\": conf, \"expansions\": ex[\"expansions\"], \"paraphrases\": ex[\"paraphrases\"], \"keywords\": ex[\"keywords\"], \"entities\": ents, \"filters\": ents.copy(), \"embedding\": emb}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b674411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# PART 6: Full RAG Engine\n",
    "# ===========================================================================\n",
    "\n",
    "class FullAdvancedRAGEngine:\n",
    "    def __init__(self, document_processor: DocumentProcessor, \n",
    "                 qdrant_manager: QdrantManager, \n",
    "                 query_processor: QueryProcessor):\n",
    "        print(\"\\n Initializing Full Advanced RAG Engine (Model 6: Re-Rank Only)..\")\n",
    "        \n",
    "        # --- All Components ---\n",
    "        self.embedding_model = document_processor.model\n",
    "        self.qdrant_manager = qdrant_manager\n",
    "        self.query_processor = query_processor\n",
    "        print(\"   ✓ Processor, QdrantManager, and QueryProcessor attached.\")\n",
    "\n",
    "        # --- Post-Retrieval ---\n",
    "        print(f\"   → Loading Cross-Encoder: {Config.CROSS_ENCODER_MODEL}...\")\n",
    "        self.cross_encoder = CrossEncoder(Config.CROSS_ENCODER_MODEL)\n",
    "        print(\"   ✓ Cross-Encoder model loaded.\")\n",
    "        \n",
    "        # --- LLM & Prompts ---\n",
    "        self.llm = ChatOpenAI(model=Config.LLM_MODEL, api_key=Config.OPENAI_API_KEY, temperature=0)\n",
    "        print(\"   ✓ Initialized ChatOpenAI LLM\")\n",
    "        \n",
    "        # Generation Prompt\n",
    "        template = \"\"\"You are a helpful financial analyst assistant. Your role is to answer questions about SEC 10-Q filings based ONLY on the provided context.\n",
    "- Base your answer strictly on the provided context from SEC filings\n",
    "- Cite specific sections (e.g., \"According to Item 1A...\") when referencing information\n",
    "- If the answer is not in the context, clearly state that\n",
    "\n",
    "Context:\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        self.prompt = ChatPromptTemplate.from_template(template)\n",
    "        print(\"   ✓ Full Advanced RAG Engine ready.\")\n",
    "\n",
    "    def _format_context(self, text_chunks: List[Dict]) -> str:\n",
    "        \"\"\"Helper to format the final text chunks for the LLM.\"\"\"\n",
    "        context_str = \"\"\n",
    "        source_count = 1\n",
    "        for chunk in text_chunks:\n",
    "            payload = chunk.get('payload', {})\n",
    "            context_str += f\"Source {source_count} ({payload.get('ticker','N/A')} - {payload.get('item','N/A')}):\\n\\\"{payload.get('text','No text')}\\\"\\n\\n\"\n",
    "            source_count += 1\n",
    "        return context_str.strip()\n",
    "\n",
    "    def query(self, question: str, ticker_filter: str = None):\n",
    "        print(f\"\\n Processing query with Full Advanced Engine: '{question}'\")\n",
    "        \n",
    "        # 1. --- PRE-RETRIEVAL + NER (QueryProcessor) ---\n",
    "        print(\"   → Step 1: Processing query (NER, Intent, Embedding)...\")\n",
    "        q_obj = self.query_processor.process(question)\n",
    "        query_vector = q_obj[\"embedding\"]\n",
    "        if query_vector is None:\n",
    "            print(\"   → Warning: QP embedding failed, using base model embedding.\")\n",
    "            query_vector = self.embedding_model.encode(question).tolist()\n",
    "\n",
    "        # 2. --- FILTER GENERATION (NER only) ---\n",
    "        print(\"   → Step 2: Generating filters...\")\n",
    "        base_filter_dict = {}\n",
    "        if \"ticker\" in q_obj[\"filters\"]:\n",
    "            base_filter_dict[\"ticker\"] = q_obj[\"filters\"][\"ticker\"]\n",
    "            print(f\"   → NER auto-detected filter: {base_filter_dict}\")\n",
    "        \n",
    "        if ticker_filter:\n",
    "            print(f\"   → MANUAL OVERRIDE: Applying filter: {ticker_filter}\")\n",
    "            base_filter_dict = {\"ticker\": [ticker_filter.upper()]}\n",
    "        \n",
    "        \n",
    "        # 3. SINGLE RETRIEVAL\n",
    "        print(f\"   → Step 3: Performing retrieval...\")\n",
    "        \n",
    "        search_results = self.qdrant_manager.search(\n",
    "            query_vector=query_vector,\n",
    "            limit=Config.RETRIEVAL_TOP_K,\n",
    "            filter_dict=base_filter_dict # Will be empty or contain ticker\n",
    "        )\n",
    "        print(f\"   → Retrieved {len(search_results)} candidates.\")\n",
    "        \n",
    "        if not search_results:\n",
    "            return {'answer': 'No relevant context found.', 'sources': []}\n",
    "\n",
    "        # 4. RE-RANKING\n",
    "        print(f\"   → Step 4: Re-ranking all {len(search_results)} candidates...\")\n",
    "        \n",
    "        passages = [result['payload']['text'] for result in search_results]\n",
    "        query_passage_pairs = [(question, passage) for passage in passages]\n",
    "        \n",
    "        print(f\"   → Re-ranking {len(passages)} text chunks with Cross-Encoder...\")\n",
    "        cross_encoder_scores = self.cross_encoder.predict(query_passage_pairs)\n",
    "        \n",
    "        final_ranked_results = []\n",
    "        for score, result in zip(cross_encoder_scores, search_results):\n",
    "            result['rerank_score'] = float(score)\n",
    "            final_ranked_results.append(result)\n",
    "\n",
    "        final_ranked_results.sort(key=lambda x: x['rerank_score'], reverse=True)\n",
    "        \n",
    "        # d) Get the final Top-K\n",
    "        final_top_k_chunks = final_ranked_results[:Config.FINAL_TOP_K]\n",
    "        print(f\"   → Re-ranked. Final {len(final_top_k_chunks)} sources selected.\")\n",
    "\n",
    "        # 5. GENERATION STEP\n",
    "        print(\"   → Step 5: Formatting final context for LLM...\")\n",
    "        \n",
    "        final_sources_for_citation = []\n",
    "        for chunk in final_top_k_chunks:\n",
    "            chunk['payload']['retrieval_score'] = chunk.get('score', 0.0)\n",
    "            chunk['payload']['rerank_score'] = chunk.get('rerank_score', 0.0)\n",
    "            final_sources_for_citation.append(chunk['payload'])\n",
    "        \n",
    "        print(f\"     → All {len(final_top_k_chunks)} chunks are text.\")\n",
    "        \n",
    "        text_context = self._format_context(final_top_k_chunks)\n",
    "        formatted_context = text_context.strip()\n",
    "\n",
    "        if not formatted_context:\n",
    "             return {'answer': 'Retrieved context, but failed to process it.', 'sources': []}\n",
    "\n",
    "        # 6. --- FINAL GENERATION ---\n",
    "        print(\"   → Step 6: Sending context to LLM...\")\n",
    "        final_prompt_message = self.prompt.format_messages(context=formatted_context, input=question)\n",
    "        \n",
    "        llm_response = self.llm.invoke(final_prompt_message)\n",
    "        answer = llm_response.content\n",
    "\n",
    "        # 7. --- FORMAT OUTPUT ---\n",
    "        print(\"   → Step 7: Formatting output sources...\")\n",
    "        sources_output = []\n",
    "        for source_data in final_sources_for_citation:\n",
    "            sources_output.append({\n",
    "                'ticker': source_data.get('ticker'),\n",
    "                'company': source_data.get('company_name'),\n",
    "                'item': source_data.get('item'),\n",
    "                'filing_date': source_data.get('filing_date'),\n",
    "                'rerank_score': source_data.get('rerank_score', 0.0),\n",
    "                'retrieval_score': source_data.get('retrieval_score', 0.0)\n",
    "            })\n",
    "            \n",
    "        return {'answer': answer, 'sources': sources_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "366e76b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# PART 7: MAIN PIPELINE ORCHESTRATOR\n",
    "# ===========================================================================\n",
    "class SECFilingRAGPipeline:\n",
    "    def __init__(self):\n",
    "        print(\"=\" * 70); print(\"SEC 10-Q RAG SYSTEM (Model 5: Full Advanced)\"); print(\"=\" * 70)\n",
    "        self.loader = SECDocumentLoader()\n",
    "        self.processor = DocumentProcessor()\n",
    "        self.qdrant_manager = QdrantManager()\n",
    "        self.query_processor = QueryProcessor()\n",
    "        self.query_engine = None\n",
    "\n",
    "    def load_and_index_filings(self, tickers: List[str] = Config.TICKERS, num_filings_per_ticker: int = 4):\n",
    "        \"\"\"\n",
    "        MODIFIED: This function now only checks if the Golden Set exists.\n",
    "        It does NOT perform indexing.\n",
    "        \"\"\"\n",
    "        print(f\"\\\\n{'=' * 70}\\\\nCHECKING DATABASE: {Config.COLLECTION_NAME}\\\\n{'=' * 70}\")\n",
    "        \n",
    "        try:\n",
    "            count = self.qdrant_manager.client.count(Config.COLLECTION_NAME, exact=True)\n",
    "            if count.count > 0:\n",
    "                print(f\"✓ Collection '{Config.COLLECTION_NAME}' found with {count.count} docs. Proceeding to query.\")\n",
    "            else:\n",
    "                print(f\"✗ ERROR: Collection '{Config.COLLECTION_NAME}' is empty or not found.\")\n",
    "                print(\"Please run '0_build_database.ipynb' first.\")\n",
    "                raise Exception(\"Database not found.\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ ERROR: Could not connect to or find collection '{Config.COLLECTION_NAME}'.\")\n",
    "            print(\"Please run '0_build_database.ipynb' first.\")\n",
    "            raise e\n",
    "        \n",
    "        print(f\"{'=' * 70}\\\\n\")\n",
    "\n",
    "    def query(self, question: str, ticker_filter: str = None):\n",
    "        if self.query_engine is None:\n",
    "            self.query_engine = FullAdvancedRAGEngine(\n",
    "                document_processor=self.processor,\n",
    "                qdrant_manager=self.qdrant_manager,\n",
    "                query_processor=self.query_processor\n",
    "            )\n",
    "        result = self.query_engine.query(question, ticker_filter)\n",
    "        print(f\"\\n{'=' * 70}\\nANSWER\\n{'=' * 70}\\n\\n{result['answer']}\\n\")\n",
    "        print(f\"{'=' * 70}\\nSOURCES ({len(result['sources'])})\\n{'=' * 70}\")\n",
    "        for i, source in enumerate(result['sources'], 1):\n",
    "            print(f\"\\n{i}. {source['company']} ({source['ticker']}) - {source['item']}\")\n",
    "            print(f\"   Filing Date: {source['filing_date']}\")\n",
    "            print(f\"   Re-Rank Score: {source.get('rerank_score', 'N/A'):.4f}\")\n",
    "            print(f\"   Vector Score: {source.get('retrieval_score', 'N/A'):.4f}\")\n",
    "        print(f\"\\n{'=' * 70}\\n\")\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1fe67ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SEC 10-Q RAG SYSTEM (Model 5: Full Advanced)\n",
      "======================================================================\n",
      "\n",
      " Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "   ✓ Model loaded (dimension: 384)\n",
      "   ✓ Initialized RecursiveCharacterTextSplitter\n",
      "\n",
      "Connecting to Qdrant Cloud...\n",
      "   ✓ Connected to Qdrant\n",
      "✓ QueryProcessor initialized\n",
      "Checking if Golden Database 'sec_filings_10q_GOLDEN_BENCHMARK' is available...\n",
      "\\n======================================================================\\nCHECKING DATABASE: sec_filings_10q_GOLDEN_BENCHMARK\\n======================================================================\n",
      "✓ Collection 'sec_filings_10q_GOLDEN_BENCHMARK' found with 12807 docs. Proceeding to query.\n",
      "======================================================================\\n\n",
      "Database check complete. Proceeding to queries.\n",
      "\\nRunning 6 queries with (Model 5) Full Advanced RAG...\n",
      "\\n--- Running Query: What are the main risk factors mentioned by each companies? ---\n",
      "\n",
      " Initializing Full Advanced RAG Engine (Model 6: Re-Rank Only)..\n",
      "   ✓ Processor, QdrantManager, and QueryProcessor attached.\n",
      "   → Loading Cross-Encoder: cross-encoder/ms-marco-MiniLM-L-6-v2...\n",
      "   ✓ Cross-Encoder model loaded.\n",
      "   ✓ Initialized ChatOpenAI LLM\n",
      "   ✓ Full Advanced RAG Engine ready.\n",
      "\n",
      " Processing query with Full Advanced Engine: 'What are the main risk factors mentioned by each companies?'\n",
      "   → Step 1: Processing query (NER, Intent, Embedding)...\n",
      "   → Step 2: Generating filters...\n",
      "   → Step 3: Performing retrieval...\n",
      "   → Retrieved 20 candidates.\n",
      "   → Step 4: Re-ranking all 20 candidates...\n",
      "   → Re-ranking 20 text chunks with Cross-Encoder...\n",
      "   → Re-ranked. Final 5 sources selected.\n",
      "   → Step 5: Formatting final context for LLM...\n",
      "     → All 5 chunks are text.\n",
      "   → Step 6: Sending context to LLM...\n",
      "   → Step 7: Formatting output sources...\n",
      "\n",
      "======================================================================\n",
      "ANSWER\n",
      "======================================================================\n",
      "\n",
      "According to the provided context:\n",
      "\n",
      "For Apple (AAPL):\n",
      "- The main risk factors are not detailed in the provided context. However, it is mentioned that there have been no material changes to the company's risk factors since the 2024 Form 10-K, as stated in \"Item 1A. Risk Factors.\"\n",
      "\n",
      "For AMD:\n",
      "- The main risk factors include:\n",
      "  - Intel Corporation’s dominance of the microprocessor market and its aggressive business practices, which may limit AMD's ability to compete effectively (Source 2).\n",
      "  - The highly competitive and rapidly evolving markets in which AMD's products are sold (Source 4 and Source 5).\n",
      "  - The cyclical nature of the semiconductor industry, which has experienced severe downturns (Source 4 and Source 5).\n",
      "\n",
      "======================================================================\n",
      "SOURCES (5)\n",
      "======================================================================\n",
      "\n",
      "1. Apple Inc. (AAPL) - ITEM 1A\n",
      "   Filing Date: 2025-01-31\n",
      "   Re-Rank Score: 4.0525\n",
      "   Vector Score: 0.6650\n",
      "\n",
      "2. ADVANCED MICRO DEVICES INC (AMD) - ITEM 1A\n",
      "   Filing Date: 2024-10-30\n",
      "   Re-Rank Score: 3.5313\n",
      "   Vector Score: 0.6457\n",
      "\n",
      "3. ADVANCED MICRO DEVICES INC (AMD) - ITEM 1A\n",
      "   Filing Date: 2025-05-07\n",
      "   Re-Rank Score: 3.5313\n",
      "   Vector Score: 0.6457\n",
      "\n",
      "4. ADVANCED MICRO DEVICES INC (AMD) - ITEM 1A\n",
      "   Filing Date: 2025-11-05\n",
      "   Re-Rank Score: 2.6218\n",
      "   Vector Score: 0.6942\n",
      "\n",
      "5. ADVANCED MICRO DEVICES INC (AMD) - ITEM 1A\n",
      "   Filing Date: 2025-08-06\n",
      "   Re-Rank Score: 2.6218\n",
      "   Vector Score: 0.6942\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\\n--- Running Query: What risks did Apple disclose in their latest 10-Q? ---\n",
      "\n",
      " Processing query with Full Advanced Engine: 'What risks did Apple disclose in their latest 10-Q?'\n",
      "   → Step 1: Processing query (NER, Intent, Embedding)...\n",
      "   → Step 2: Generating filters...\n",
      "   → NER auto-detected filter: {'ticker': ['AAPL']}\n",
      "   → Step 3: Performing retrieval...\n",
      "   → Retrieved 20 candidates.\n",
      "   → Step 4: Re-ranking all 20 candidates...\n",
      "   → Re-ranking 20 text chunks with Cross-Encoder...\n",
      "   → Re-ranked. Final 5 sources selected.\n",
      "   → Step 5: Formatting final context for LLM...\n",
      "     → All 5 chunks are text.\n",
      "   → Step 6: Sending context to LLM...\n",
      "   → Step 7: Formatting output sources...\n",
      "\n",
      "======================================================================\n",
      "ANSWER\n",
      "======================================================================\n",
      "\n",
      "According to the provided context, Apple did not disclose any new material changes to their risk factors in their latest 10-Q. Specifically, \"there have been no material changes to the Company’s risk factors since the 2023 Form 10-K\" (Source 1, AAPL - ITEM 1A). Therefore, the risks disclosed remain consistent with those previously reported in their 2023 Form 10-K.\n",
      "\n",
      "======================================================================\n",
      "SOURCES (5)\n",
      "======================================================================\n",
      "\n",
      "1. Apple Inc. (AAPL) - ITEM 1A\n",
      "   Filing Date: 2024-08-02\n",
      "   Re-Rank Score: 3.0329\n",
      "   Vector Score: 0.7313\n",
      "\n",
      "2. Apple Inc. (AAPL) - ITEM 2\n",
      "   Filing Date: 2025-05-02\n",
      "   Re-Rank Score: 2.4537\n",
      "   Vector Score: 0.6190\n",
      "\n",
      "3. Apple Inc. (AAPL) - ITEM 2\n",
      "   Filing Date: 2025-05-02\n",
      "   Re-Rank Score: 2.0474\n",
      "   Vector Score: 0.5701\n",
      "\n",
      "4. Apple Inc. (AAPL) - ITEM 2\n",
      "   Filing Date: 2024-08-02\n",
      "   Re-Rank Score: 2.0157\n",
      "   Vector Score: 0.5700\n",
      "\n",
      "5. Apple Inc. (AAPL) - ITEM 1A\n",
      "   Filing Date: 2025-05-02\n",
      "   Re-Rank Score: 0.8328\n",
      "   Vector Score: 0.6020\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\\n--- Running Query: Compare the revenue trends of NVIDIA and AMD ---\n",
      "\n",
      " Processing query with Full Advanced Engine: 'Compare the revenue trends of NVIDIA and AMD'\n",
      "   → Step 1: Processing query (NER, Intent, Embedding)...\n",
      "   → Step 2: Generating filters...\n",
      "   → NER auto-detected filter: {'ticker': ['AMD', 'NVDA']}\n",
      "   → Step 3: Performing retrieval...\n",
      "   → Retrieved 20 candidates.\n",
      "   → Step 4: Re-ranking all 20 candidates...\n",
      "   → Re-ranking 20 text chunks with Cross-Encoder...\n",
      "   → Re-ranked. Final 5 sources selected.\n",
      "   → Step 5: Formatting final context for LLM...\n",
      "     → All 5 chunks are text.\n",
      "   → Step 6: Sending context to LLM...\n",
      "   → Step 7: Formatting output sources...\n",
      "\n",
      "======================================================================\n",
      "ANSWER\n",
      "======================================================================\n",
      "\n",
      "Based on the provided context, NVIDIA and AMD both experienced revenue growth, but the specific trends and drivers differ for each company.\n",
      "\n",
      "For NVIDIA:\n",
      "- One customer represented a significant portion of revenue, accounting for 17% and 13% of total revenue for the second quarter and first half of fiscal year 2024, respectively, as noted in Source 1.\n",
      "- Revenue from sales to customers outside of the United States accounted for 53% and 48% of total revenue for the first quarter of fiscal years 2026 and 2025, respectively, as mentioned in Source 3.\n",
      "\n",
      "For AMD:\n",
      "- Net revenue for the three months ended September 27, 2025, was $9.2 billion, marking a 36% increase compared to the prior year period. This increase was driven by strong demand in the Client and Gaming segments, particularly for AMD Ryzen™ processors, semi-custom game console SoCs, and Radeon™ gaming GPUs, as well as an increase in Data Center segment revenue due to demand for AMD EPYC™ processors and AMD Instinct™ MI350 Series GPUs, as detailed in Source 4.\n",
      "\n",
      "In summary, both companies experienced revenue growth, but AMD's growth was notably driven by increased demand in specific product segments, while NVIDIA's revenue was significantly influenced by sales to a major customer and international sales.\n",
      "\n",
      "======================================================================\n",
      "SOURCES (5)\n",
      "======================================================================\n",
      "\n",
      "1. NVIDIA CORP (NVDA) - ITEM 1\n",
      "   Filing Date: 2024-08-28\n",
      "   Re-Rank Score: 1.2985\n",
      "   Vector Score: 0.6962\n",
      "\n",
      "2. NVIDIA CORP (NVDA) - ITEM 1\n",
      "   Filing Date: 2025-08-27\n",
      "   Re-Rank Score: 0.8773\n",
      "   Vector Score: 0.5922\n",
      "\n",
      "3. NVIDIA CORP (NVDA) - ITEM 1\n",
      "   Filing Date: 2025-05-28\n",
      "   Re-Rank Score: 0.6547\n",
      "   Vector Score: 0.5925\n",
      "\n",
      "4. ADVANCED MICRO DEVICES INC (AMD) - ITEM 2\n",
      "   Filing Date: 2025-11-05\n",
      "   Re-Rank Score: 0.6101\n",
      "   Vector Score: 0.6258\n",
      "\n",
      "5. NVIDIA CORP (NVDA) - ITEM 1\n",
      "   Filing Date: 2024-08-28\n",
      "   Re-Rank Score: 0.0571\n",
      "   Vector Score: 0.6392\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\\n--- Running Query: What was Tesla's R&D spending in the latest quarter? ---\n",
      "\n",
      " Processing query with Full Advanced Engine: 'What was Tesla's R&D spending in the latest quarter?'\n",
      "   → Step 1: Processing query (NER, Intent, Embedding)...\n",
      "   → Step 2: Generating filters...\n",
      "   → NER auto-detected filter: {'ticker': ['TSLA']}\n",
      "   → Step 3: Performing retrieval...\n",
      "   → Retrieved 20 candidates.\n",
      "   → Step 4: Re-ranking all 20 candidates...\n",
      "   → Re-ranking 20 text chunks with Cross-Encoder...\n",
      "   → Re-ranked. Final 5 sources selected.\n",
      "   → Step 5: Formatting final context for LLM...\n",
      "     → All 5 chunks are text.\n",
      "   → Step 6: Sending context to LLM...\n",
      "   → Step 7: Formatting output sources...\n",
      "\n",
      "======================================================================\n",
      "ANSWER\n",
      "======================================================================\n",
      "\n",
      "The specific amount of Tesla's R&D spending in the latest quarter is not provided in the context. However, according to Source 1 (TSLA - ITEM 2), R&D expenses increased by $591 million, or 57%, in the three months ended September 30, 2025, compared to the same period in 2024.\n",
      "\n",
      "======================================================================\n",
      "SOURCES (5)\n",
      "======================================================================\n",
      "\n",
      "1. Tesla, Inc. (TSLA) - ITEM 2\n",
      "   Filing Date: 2025-10-23\n",
      "   Re-Rank Score: -0.5320\n",
      "   Vector Score: 0.5687\n",
      "\n",
      "2. Tesla, Inc. (TSLA) - ITEM 2\n",
      "   Filing Date: 2025-07-24\n",
      "   Re-Rank Score: -0.9581\n",
      "   Vector Score: 0.5555\n",
      "\n",
      "3. Tesla, Inc. (TSLA) - ITEM 2\n",
      "   Filing Date: 2024-10-24\n",
      "   Re-Rank Score: -1.2376\n",
      "   Vector Score: 0.5818\n",
      "\n",
      "4. Tesla, Inc. (TSLA) - ITEM 2\n",
      "   Filing Date: 2024-10-24\n",
      "   Re-Rank Score: -1.2654\n",
      "   Vector Score: 0.5433\n",
      "\n",
      "5. Tesla, Inc. (TSLA) - ITEM 2\n",
      "   Filing Date: 2025-04-23\n",
      "   Re-Rank Score: -2.0217\n",
      "   Vector Score: 0.5306\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\\n--- Running Query: How has Microsoft's operating income changed over the last year? ---\n",
      "\n",
      " Processing query with Full Advanced Engine: 'How has Microsoft's operating income changed over the last year?'\n",
      "   → Step 1: Processing query (NER, Intent, Embedding)...\n",
      "   → Step 2: Generating filters...\n",
      "   → NER auto-detected filter: {'ticker': ['MSFT']}\n",
      "   → Step 3: Performing retrieval...\n",
      "   → Retrieved 20 candidates.\n",
      "   → Step 4: Re-ranking all 20 candidates...\n",
      "   → Re-ranking 20 text chunks with Cross-Encoder...\n",
      "   → Re-ranked. Final 5 sources selected.\n",
      "   → Step 5: Formatting final context for LLM...\n",
      "     → All 5 chunks are text.\n",
      "   → Step 6: Sending context to LLM...\n",
      "   → Step 7: Formatting output sources...\n",
      "\n",
      "======================================================================\n",
      "ANSWER\n",
      "======================================================================\n",
      "\n",
      "The provided context does not include specific information about changes in Microsoft's operating income over the last year. The context only contains reviews of interim financial information, confirming that no material modifications are needed for conformity with accounting principles, but it does not provide details on operating income figures or changes.\n",
      "\n",
      "======================================================================\n",
      "SOURCES (5)\n",
      "======================================================================\n",
      "\n",
      "1. MICROSOFT CORP (MSFT) - ITEM 1\n",
      "   Filing Date: 2025-04-30\n",
      "   Re-Rank Score: -2.2425\n",
      "   Vector Score: 0.4947\n",
      "\n",
      "2. MICROSOFT CORP (MSFT) - ITEM 1\n",
      "   Filing Date: 2025-01-29\n",
      "   Re-Rank Score: -2.4004\n",
      "   Vector Score: 0.4972\n",
      "\n",
      "3. MICROSOFT CORP (MSFT) - ITEM 1\n",
      "   Filing Date: 2025-10-29\n",
      "   Re-Rank Score: -3.8218\n",
      "   Vector Score: 0.4465\n",
      "\n",
      "4. MICROSOFT CORP (MSFT) - ITEM 1\n",
      "   Filing Date: 2024-10-30\n",
      "   Re-Rank Score: -3.8560\n",
      "   Vector Score: 0.4469\n",
      "\n",
      "5. MICROSOFT CORP (MSFT) - ITEM 1\n",
      "   Filing Date: 2025-01-29\n",
      "   Re-Rank Score: -8.6739\n",
      "   Vector Score: 0.3949\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\\n--- Running Query: What was the gross profit margin for all companies? ---\n",
      "\n",
      " Processing query with Full Advanced Engine: 'What was the gross profit margin for all companies?'\n",
      "   → Step 1: Processing query (NER, Intent, Embedding)...\n",
      "   → Step 2: Generating filters...\n",
      "   → Step 3: Performing retrieval...\n",
      "   → Retrieved 20 candidates.\n",
      "   → Step 4: Re-ranking all 20 candidates...\n",
      "   → Re-ranking 20 text chunks with Cross-Encoder...\n",
      "   → Re-ranked. Final 5 sources selected.\n",
      "   → Step 5: Formatting final context for LLM...\n",
      "     → All 5 chunks are text.\n",
      "   → Step 6: Sending context to LLM...\n",
      "   → Step 7: Formatting output sources...\n",
      "\n",
      "======================================================================\n",
      "ANSWER\n",
      "======================================================================\n",
      "\n",
      "According to the provided context:\n",
      "\n",
      "- For NVIDIA (NVDA):\n",
      "  - The gross margin was 75.1% for the second quarter and 76.6% for the first half of fiscal year 2025 (Source 1).\n",
      "  - The gross margin was 74.6% for the third quarter and 75.8% for the first nine months of fiscal year 2025 (Source 2).\n",
      "\n",
      "- For Tesla (TSLA):\n",
      "  - The gross margin was 19.8% and 17.9% for the periods mentioned in Source 3.\n",
      "  - The gross margin was 17.2% and 18.0% for the periods mentioned in Source 4.\n",
      "  - The gross margin was 18.0% and 19.8% for the periods mentioned in Source 5.\n",
      "\n",
      "These are the gross profit margins for the respective companies as per the provided context.\n",
      "\n",
      "======================================================================\n",
      "SOURCES (5)\n",
      "======================================================================\n",
      "\n",
      "1. NVIDIA CORP (NVDA) - ITEM 2\n",
      "   Filing Date: 2024-08-28\n",
      "   Re-Rank Score: 4.2103\n",
      "   Vector Score: 0.6454\n",
      "\n",
      "2. NVIDIA CORP (NVDA) - ITEM 2\n",
      "   Filing Date: 2024-11-20\n",
      "   Re-Rank Score: 3.3715\n",
      "   Vector Score: 0.5876\n",
      "\n",
      "3. Tesla, Inc. (TSLA) - ITEM 2\n",
      "   Filing Date: 2024-10-24\n",
      "   Re-Rank Score: 2.1722\n",
      "   Vector Score: 0.6659\n",
      "\n",
      "4. Tesla, Inc. (TSLA) - ITEM 2\n",
      "   Filing Date: 2025-07-24\n",
      "   Re-Rank Score: 1.9313\n",
      "   Vector Score: 0.6528\n",
      "\n",
      "5. Tesla, Inc. (TSLA) - ITEM 2\n",
      "   Filing Date: 2025-10-23\n",
      "   Re-Rank Score: 1.8279\n",
      "   Vector Score: 0.6704\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\\n--- All queries complete. Saving all results to: data\\results_model_5_full_pipeline.json ---\n",
      "✓ Successfully saved all answers.\n",
      "\\nPipeline run finished.\n"
     ]
    }
   ],
   "source": [
    "# ===========================================================================\n",
    "# PART 8: USAGE EXAMPLE (Standardized)\n",
    "# ===========================================================================\n",
    "import os\n",
    "import json\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pipeline = SECFilingRAGPipeline()\n",
    "    \n",
    "    print(f\"Checking if Golden Database '{Config.COLLECTION_NAME}' is available...\")\n",
    "    pipeline.load_and_index_filings(num_filings_per_ticker=4)\n",
    "    print(\"Database check complete. Proceeding to queries.\")\n",
    "\n",
    "    DATA_FOLDER = \"data\"\n",
    "    if not os.path.exists(DATA_FOLDER):\n",
    "        os.makedirs(DATA_FOLDER); print(f\"Created data folder: {DATA_FOLDER}\")\n",
    "\n",
    "    # --- Standardized Query Set ---\n",
    "    queries_to_run = [\n",
    "        {\"question\": \"What are the main risk factors mentioned by each companies?\", \"ticker_filter\": None},\n",
    "        {\"question\": \"What risks did Apple disclose in their latest 10-Q?\", \"ticker_filter\": None},\n",
    "        {\"question\": \"Compare the revenue trends of NVIDIA and AMD\", \"ticker_filter\": None},\n",
    "        {\"question\": \"What was Tesla's R&D spending in the latest quarter?\", \"ticker_filter\": None},\n",
    "        {\"question\": \"How has Microsoft's operating income changed over the last year?\", \"ticker_filter\": None},\n",
    "        {\"question\": \"What was the gross profit margin for all companies?\", \"ticker_filter\": None},\n",
    "    ]\n",
    "\n",
    "    print(f\"\\\\nRunning {len(queries_to_run)} queries with (Model 5) Full Advanced RAG...\")\n",
    "    all_results = []\n",
    "    \n",
    "    for query in queries_to_run:\n",
    "        print(f\"\\\\n--- Running Query: {query['question']} ---\")\n",
    "        result = pipeline.query(question=query[\"question\"], ticker_filter=query[\"ticker_filter\"])\n",
    "        all_results.append({\"question\": query[\"question\"], \"ticker_filter_manual\": query[\"ticker_filter\"], \"response\": result})\n",
    "\n",
    "    # --- Standardized Save Path ---\n",
    "    save_path = os.path.join(DATA_FOLDER, \"results_model_5_full_pipeline.json\")\n",
    "    print(f\"\\\\n--- All queries complete. Saving all results to: {save_path} ---\")\n",
    "    try:\n",
    "        with open(save_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(all_results, f, indent=4)\n",
    "        print(f\"✓ Successfully saved all answers.\")\n",
    "    except Exception as e: print(f\"✗ Failed to save results: {e}\")\n",
    "    print(\"\\\\nPipeline run finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
